#| include: false
options(scipen = 999)
library(tidyverse)
library(tidycensus)
library(broom)
library(scales)
census_api_key("20068788c6e79d5716fbceb0dcd562ab23f74ca1")
#| echo: false
#| eval: false
library(tidyverse)
# Generate data following Y = f(X) + ε
set.seed(789)
n <- 50
x <- seq(0, 10, length.out = n)
# True function f(X) - let's make it slightly curved
f_x <- 5 + 2*x - 0.1*x^2
# Add random error ε
epsilon <- rnorm(n, 0, 2)
y <- f_x + epsilon
# Create data frame
data <- data.frame(x = x, y = y, f_x = f_x, epsilon = epsilon)
# Highlight a few specific points to show the decomposition
highlight_points <- c(10, 25, 40)
data$highlight <- 1:n %in% highlight_points
# Create the visualization
ggplot(data, aes(x = x)) +
geom_line(aes(y = f_x), color = "#2C3E50", linewidth = 1.5) +
geom_point(aes(y = y, size = highlight, alpha = highlight),
color = "#E74C3C", show.legend = FALSE) +
geom_segment(data = data[data$highlight, ],
aes(x = x, xend = x, y = f_x, yend = y),
color = "#9B59B6", linewidth = 1,
arrow = arrow(length = unit(0.2, "cm"), ends = "both")) +
annotate("text", x = 2, y = 22, label = "Y = f(X) + ε",
size = 6, fontface = "bold", hjust = 0) +
annotate("text", x = 8, y = 18, label = "f(X)\n(systematic)",
size = 5, color = "#2C3E50", hjust = 0) +
annotate("text", x = 5.5, y = 10, label = "ε\n(random error)",
size = 4, color = "#9B59B6") +
annotate("point", x = 1, y = 8, size = 4, color = "#E74C3C") +
annotate("text", x = 1.5, y = 8, label = "Observed Y",
size = 4, color = "#E74C3C", hjust = 0) +
scale_size_manual(values = c(2, 4)) +
scale_alpha_manual(values = c(0.6, 1)) +
labs(x = "X (predictors)", y = "Y (outcome)",
title = "The Statistical Learning Framework",
subtitle = "Our goal: Estimate f(X) from observed data") +
theme_minimal(base_size = 14) +
theme(plot.title = element_text(face = "bold", size = 16),
panel.grid.minor = element_blank())
ggsave("images/statistical_learning_framework.png",
width = 10, height = 6, dpi = 300)
#| echo: false
#| eval: true
# Fetch PA county data directly from Census API
pa_data <- get_acs(
geography = "county",
state = "PA",
variables = c(
total_pop = "B01003_001",
median_income = "B19013_001"
),
year = 2022,
output = "wide"
)
# Visualize the relationship
ggplot(pa_data, aes(x = total_popE, y = median_incomeE)) +
geom_point(alpha = 0.6, size = 3) +
geom_smooth(method = "lm", se = TRUE, color = "steelblue") +
labs(
title = "Population vs Median Income in PA Counties",
x = "Total Population",
y = "Median Household Income"
) +
scale_x_continuous(labels = comma) +
scale_y_continuous(labels = dollar) +
theme_minimal()
#| echo: true
#| eval: true
model1 <- lm(median_incomeE ~ total_popE, data = pa_data)
summary(model1)
#| echo: false
#| eval: true
#| fig-width: 10
#| fig-height: 4
# Create example data with clear pattern
set.seed(123)
x <- seq(0, 10, 0.5)
y <- 2 + 0.5*x + rnorm(length(x), 0, 1)
example_data <- data.frame(x = x, y = y)
# Three models
p1 <- ggplot(example_data, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ 1, se = FALSE) +
labs(title = "Underfitting", subtitle = "Ignores relationship") +
theme_minimal()
p2 <- ggplot(example_data, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Good Fit", subtitle = "Captures true pattern") +
theme_minimal()
p3 <- ggplot(example_data, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ poly(x, 10), se = FALSE) +
labs(title = "Overfitting", subtitle = "Follows noise") +
theme_minimal()
library(patchwork)
p1 | p2 | p3
#| echo: true
#| eval: true
set.seed(123)
n <- nrow(pa_data)
# 70% training, 30% testing
train_indices <- sample(1:n, size = 0.7 * n)
train_data <- pa_data[train_indices, ]
test_data <- pa_data[-train_indices, ]
# Fit on training data only
model_train <- lm(median_incomeE ~ total_popE, data = train_data)
# Predict on test data
test_predictions <- predict(model_train, newdata = test_data)
#| echo: true
#| eval: true
# Calculate prediction error (RMSE)
rmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))
rmse_train <- summary(model_train)$sigma
cat("Training RMSE:", round(rmse_train, 0), "\n")
cat("Test RMSE:", round(rmse_test, 0), "\n")
#| echo: true
#| eval: true
library(caret)
# 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
cv_model <- train(median_incomeE ~ total_popE,
data = pa_data,
method = "lm",
trControl = train_control)
cv_model$results
#| echo: true
#| eval: true
pa_data$residuals <- residuals(model1)
pa_data$fitted <- fitted(model1)
ggplot(pa_data, aes(x = fitted, y = residuals)) +
geom_point() +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot", x = "Fitted Values", y = "Residuals") +
theme_minimal()
#| echo: true
#| eval: true
library(lmtest)
install.packages("lmtest")
#| echo: true
#| eval: true
library(lmtest)
bptest(model1)
#| echo: false
#| eval: true
#| fig-width: 5
#| fig-height: 4
q <- ggplot(pa_data, aes(sample = residuals)) +
stat_qq() +
stat_qq_line(color = "red") +
labs(title = "Q-Q Plot of Residuals",
x = "Theoretical Quantiles",
y = "Sample Quantiles") +
theme_minimal()
print(q)
#| echo: true
#| eval: false
library(car)
vif(model1)  # Variance Inflation Factor
install.packages("car")
install.packages("car")
#| echo: true
#| eval: false
library(car)
vif(model1)  # Variance Inflation Factor
#| echo: true
#| eval: false
model1 <- lm(Y ~ X1 + X2, data = my_data)
View(model1)
#| echo: false
#| eval: true
#| fig-width: 5
#| fig-height: 4
# Add diagnostic measures
pa_data <- pa_data %>%
mutate(
cooks_d = cooks.distance(model1),
leverage = hatvalues(model1),
is_influential = cooks_d > 4/nrow(pa_data)
)
View(model1)
