[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nA RANDOM PERSON\n\n\n\n\nEmail: cui12@upenn.edu\nGitHub: @ChristineCui12"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "A RANDOM PERSON"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: cui12@upenn.edu\nGitHub: @ChristineCui12"
  },
  {
    "objectID": "midterm/Slides/CuiXinyuan_Presentation.html#the-problem",
    "href": "midterm/Slides/CuiXinyuan_Presentation.html#the-problem",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "The Problem",
    "text": "The Problem\nEvery year, thousands of Philadelphians buy or sell homes, and every transaction tells a story about how people value location, convenience, and opportunity.\nYet the city‚Äôs current Automated Valuation Model doesn‚Äôt always capture these stories. Some neighborhoods are undervalued, while others bear unfairly high assessments.\n\n\n\n\n\n\nWhat we are trying to explore?\n\n\n\nPrice Gap: Why do two homes with similar size and design sell for very different prices in Philadelphia?\nSpatial Drivers: Which neighborhood, accessibility, and socioeconomic factors drive these differences?\nFair Valuation: How can understanding them help the city create a fairer, smarter system for property tax assessment?"
  },
  {
    "objectID": "midterm/Slides/CuiXinyuan_Presentation.html#data-sources",
    "href": "midterm/Slides/CuiXinyuan_Presentation.html#data-sources",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Data Sources",
    "text": "Data Sources\n\n\nüè†Property sales\n(n = 34,559 2023-2024)\n\nLivable Area\nBedrooms Counts\nBathrooms Counts\nHouse Age\nInterior Condition\nQuality Grade\nFireplaces\nGarage Spaces\nCentral Air\nMarket Value\nSale Price\n\n\nüî¢Census ACS\n(n = 2023)\n\nMedian Income\nEducation Level\nUnemployment Rate\n\nüèõOpenDataPhilly\n\nTransit\nParks and Recreation center\nHospitals\nCrime(2023)\nPoint of Interest"
  },
  {
    "objectID": "midterm/Slides/CuiXinyuan_Presentation.html#where-are-expensive-homes",
    "href": "midterm/Slides/CuiXinyuan_Presentation.html#where-are-expensive-homes",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Where Are Expensive Homes?",
    "text": "Where Are Expensive Homes?\n\n\n\n\n\n\nResidential sale prices exhibited clear signs of spatial clustering.\nhighest-priced properties: Center City core, University City.\nMiddle & Lower-Price tiers : North Philadelphia, West Philadelphia (outside the university sphere), and South Philadelphia.\nAccount for the specific ‚Äúmicro-market‚Äù in which a property is situated."
  },
  {
    "objectID": "midterm/Slides/CuiXinyuan_Presentation.html#why-are-they-expensive",
    "href": "midterm/Slides/CuiXinyuan_Presentation.html#why-are-they-expensive",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Why Are They Expensive?",
    "text": "Why Are They Expensive?"
  },
  {
    "objectID": "midterm/Slides/CuiXinyuan_Presentation.html#highlight",
    "href": "midterm/Slides/CuiXinyuan_Presentation.html#highlight",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Highlight",
    "text": "Highlight\n\nOur adjustment follows established mass appraisal practices, where implausible sale prices are identified and adjusted based on their ratio to assessed market value (Deaf Smith CAD, Mass Appraisal and Ratio Study Manual, 2023).\nPrincipleÔºöRetain all information that the data tells us (including outliers) rather than deleting it.\nObjectiveÔºöEnsure equity.\nRelationship Formula: Sale Price = -9189.29 + 1.03*Market value\nApproach: Unreliable sale price(low weight)~ Reliable sale price(high weight)"
  },
  {
    "objectID": "midterm/Slides/CuiXinyuan_Presentation.html#model-building-and-validation",
    "href": "midterm/Slides/CuiXinyuan_Presentation.html#model-building-and-validation",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Model Building and Validation",
    "text": "Model Building and Validation\nModel Performance Improves with Each Layer\n\n\n\nModel\nCV RMSE (log)\nR¬≤\nRMSE\n\n\n\n\nStructural Only\n0.5497\n0.5235\n221675.8\n\n\n+ Census\n0.4519\n0.6779\n178383.4\n\n\n+ Spatial\n0.3994\n0.7486\n132547.4\n\n\n+ Interactions/FE\n0.389\n0.7611\n124417.4\n\n\n\n\nR¬≤ = 0.76 =&gt; The model explains 76% of the variation in residential sale prices.\nRMSE = 124,417.4 =&gt; The average error between the model‚Äôs predicted sale price and the actual sale price is approximately \\(\\$124,417.4\\).\n\n\n\n\n\n\n\nImportant\n\n\n\nFor K-fold cross-validation, we use the entire dataset with replaced outliers for training to maintain consistency with the model derivation process. For the final test, however, we use a purified set that excludes all handled data, ensuring a reliable performance evaluation on genuine, trustworthy data."
  },
  {
    "objectID": "midterm/Slides/CuiXinyuan_Presentation.html#key-factors",
    "href": "midterm/Slides/CuiXinyuan_Presentation.html#key-factors",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Key Factors",
    "text": "Key Factors\n\nLivable Area\nComparable Sales\nInterior Condition\nZip Code"
  },
  {
    "objectID": "midterm/Slides/CuiXinyuan_Presentation.html#identifying-the-hardest-to-predict-neighborhoods",
    "href": "midterm/Slides/CuiXinyuan_Presentation.html#identifying-the-hardest-to-predict-neighborhoods",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Identifying the Hardest-to-Predict Neighborhoods",
    "text": "Identifying the Hardest-to-Predict Neighborhoods\n\n\n\n\n\nInterpretation\n\nUnderestimate zip code: 19130, 19123, 19104, 19144‚Ä¶\nOverestimate zip code: 19102, 19107, 19116, 19154‚Ä¶\nMarket is moving faster than the city‚Äôs valuation system can keep up."
  },
  {
    "objectID": "midterm/Slides/CuiXinyuan_Presentation.html#recommendations-and-limitations",
    "href": "midterm/Slides/CuiXinyuan_Presentation.html#recommendations-and-limitations",
    "title": "The Path to Parity: Optimizing Residential Sale Price Prediction for Fair Taxation",
    "section": "Recommendations and Limitations",
    "text": "Recommendations and Limitations\n\nCorrect the assessed values in low-income communities that have been systematically overestimated.\nRoutinely incorporate the effective spatial features‚Äîparticularly surrounding transaction prices and neighborhood fixed effects.\nExtreme high values almost exclusively stem from corporate transactions and require manual review for outliers.\n\n\n\n\n\n\n\nLimitations: Algorithmic Fairness\n\n\n\nSpatial Coverage Bias and Data Quality Bias\nFeature Omission Bias and Spatial Dependence Bias"
  },
  {
    "objectID": "midterm/Appendix/CuiXinyuan_Appendix.html",
    "href": "midterm/Appendix/CuiXinyuan_Appendix.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation:\n\nThe goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality.\nThe red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions.\n\n1. Removing Non-Market Sales (The Sale Price &lt; 0.05 √ó Market Value Rule)\n\nThere is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000).\nThese are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market.\n\n2. Removing Anomalous High Sales (The Sale Price &gt; 5  Market Value Rule)*\n\nThere are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X).\nThese are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value.\nRetaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price.\n\nTherefore, we can leverage this relationship to estimate the normal market prices for non-market transactions.\nWe record these estimated values as sale_price_predicted.\nBy doing this, we enhance our data!\n\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n\nNeighborhood Safety:\n\nCrime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices.\n\n\nAccessibility and Transportation:\n\nTransit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values.\n\nPoints of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices.\n\nHealthcare Access:\n\nHospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values.\n\n\nSocioeconomic and Demographic Context (from Census data):\n\nTotal population: Indicates population density, which can affect demand for housing.\n\nEducational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability.\n\nMedian income: Directly influences purchasing power and demand for housing in an area.\n\nEmployment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand.\n\nHousing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices.\n\nTogether, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-1-data-preparation",
    "href": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-1-data-preparation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Load necessary libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(nngeo)\nlibrary(car)\nlibrary(knitr)\nlibrary(readr)\nlibrary(patchwork)\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  \n\n\n1.1 Load and clean Philadelphia sales data:\n\n1.1.1 Load data\n\n\n\nCode\nlibrary(tidyverse)\nopa &lt;- read_csv(\"opa_properties_public1.csv\")\n\n\n\n1.1.2 Filter to residential properties, 2023-2024 sales\n\n\n\nCode\n# data in 2022 will be used as predictor, so keep them as well.\nopa_clean &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# record the amount of data we will focus on\nopa_clean2 &lt;- opa %&gt;%\n  mutate(sale_date = as.Date(sale_date)) %&gt;%\n  filter(sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\",\n  category_code == \"1\"  # 1 indicates single family\n  )\n\n# Select relevant variables \nopa_var &lt;- opa_clean %&gt;%\n  dplyr::select(\n    sale_date, sale_price, market_value, building_code_description,\n    total_livable_area, number_of_bedrooms, number_of_bathrooms,\n    number_stories, garage_spaces, central_air, quality_grade,\n    interior_condition, exterior_condition, year_built, \n    off_street_open, zip_code, census_tract, zoning, owner_1,\n    category_code_description, shape, fireplaces\n  )\n\n\n\n1.1.3 Remove obvious errors & Handle missing values\n\n\n\nCode\n# ! check before remove NA value\n\ncat(\"&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\")\n\n\n&lt;small&gt;\nüí° Data Cleaning Notes:&lt;br&gt;\n- Remove NA only if missing count is small.&lt;br&gt;\n- If many are missing, retain them and transform NA into a meaningful category.&lt;br&gt;\n- Always record how missing values were handled.\n&lt;/small&gt;\n\n\nCode\n#remove errors and drop rows with small NA counts in specific columns\nopa_var &lt;- opa_var %&gt;% \n  distinct() %&gt;%  #Remove duplicate lines\n  filter(\n    !is.na(total_livable_area) & total_livable_area &gt; 0,\n    !is.na(year_built) & year_built &gt; 0 & year_built &lt; 2025,\n    !is.na(number_of_bathrooms),\n    !is.na(fireplaces),\n    !is.na(interior_condition),\n    garage_spaces&lt;30\n  )   \n\n\n\n1.1.4 Other cleaning decisions\n\n\n\nCode\n#numeric quality_grade\nvalid_grades &lt;- c(\"A+\", \"A\", \"A-\", \n                  \"B+\", \"B\", \"B-\", \n                  \"C+\", \"C\", \"C-\", \n                  \"D+\", \"D\", \"D-\", \n                  \"E+\", \"E\", \"E-\")\n\nopa_var &lt;- opa_var %&gt;%\n  filter(quality_grade %in% valid_grades) %&gt;%\n  mutate(\n    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),\n    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]\n  )\n\n#central_air (keep and transform the large NA values)\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    central_air_dummy = case_when(\n      central_air %in% c(1, \"Y\", \"y\") ~ 1,\n      central_air %in% c(0, \"N\", \"n\") ~ 0,\n      TRUE ~ NA_real_\n    ),\n    central_air_missing = if_else(is.na(central_air), 1, 0),\n    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)\n  )\n\n#house age\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    house_age = 2025 - year_built\n  )\n\n\n\n1.1.5 Explore structural data biases and identify non-market transactions\n\n\n\nCode\n# check the relation of Sale Price ~ Market Value\noptions(scipen = 999)\nplot(opa_var$sale_price, opa_var$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value (Predicted)  vs  Sale Price (Actual)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# create a new column to record the identified non-market transactions\nopa_var &lt;- opa_var %&gt;%\n  mutate(\n    non_market = \n      (((sale_price &lt; 0.10*market_value) | sale_price &lt; 2000) | (sale_price&gt; 5*market_value))\n  )\n\n\nInterpretation:\n\nThe goal of this model is to predict typical, market-driven sale prices. The provided scatter plot of Market Value (Predicted) vs.¬†Sale Price (Actual) is an essential diagnostic tool for assessing data quality.\nThe red line on the plot represents a perfect 1-to-1 relationship (Y=X), where the property‚Äôs actual sale price is exactly equal to its predicted market value. While most data points cluster tightly around this line‚Äîindicating the ‚ÄúMarket Value‚Äù is a strong predictor‚Äîthe plot clearly reveals two distinct types of extreme outliers that do not represent typical market transactions.\n\n1. Removing Non-Market Sales (The Sale Price &lt; 0.05 √ó Market Value Rule)\n\nThere is a dense cluster of points stacked vertically along the y-axis, where Sale Price (X) is near zero, but Market Value (Y) is high (e.g., $1M, $2M, even $4M). These points represent transactions where a property was ‚Äúsold‚Äù for a price that is a tiny fraction of its assessed worth (e.g., a $2,000,000 house sold for $50,000).\nThese are non-arm‚Äôs-length transactions, not true market sales. Examples include sales between family members, inheritance transfers, or other legal transactions where the price does not reflect the market.\n\n2. Removing Anomalous High Sales (The Sale Price &gt; 5  Market Value Rule)*\n\nThere are several data points scattered far to the right and bottom of the plot, far below the red Y=X line. These points represent properties where the Sale Price (X) was massively higher than its Market Value (Y). For example, a property with an assessed value of $500,000 (Y) selling for $4,000,000 (X).\nThese are also not typical sales. They could represent (a) data entry errors, (b) sales where the ‚ÄúMarket Value‚Äù figure is obsolete (e.g., a run-down property sold for land value/development potential), or (c) properties that underwent major renovations not yet captured in the assessed value.\nRetaining these two groups of outliers would severely skew the model‚Äôs coefficients and reduce its predictive accuracy for the vast majority of normal, market-based transactions. This filtering rule is a necessary step to clean the data, ensure the model learns from valid transactions, and improve its overall reliability.\n1.1.6 enhance the sales data\n\nWe have 8000+ non market transactions, that is 1/4 of the total sales data! That‚Äôs too big to let go, The model that we want to generate will become much more stable if we can make use of them.\n\n\n\n\nCode\n# filter the REAL MARKET data in the time period we need\nopa_selected &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n  )   \n\n#filter the NON MARKET data in the time period we need\nopa_non_market &lt;- opa_var %&gt;%\n  filter(\n    non_market ==1,\n    sale_date &gt;= \"2023-01-01\" & sale_date &lt;= \"2024-12-31\"\n    )\n\nopa_selected2 &lt;- opa_var\nopa_bonus &lt;- opa_var \n\n\n\n\nCode\n#try to find the relationship between market_value and sale_price\noptions(scipen = 999)\nplot(opa_selected$sale_price, opa_selected$market_value,\n     xlab = \"Sale Price\", ylab = \"Market Price\",\n     main = \" Market Value vs  Sale Price (cleaned)\",\n     pch = 19, col = rgb(0.2,0.4,0.6,0.4))\nabline(0,1,col=\"red\",lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# That's quite linear, let's try to build a simple OLS model\nopa_mdata &lt;- opa_bonus %&gt;%\n  filter(\n    non_market == 0\n    )\n\nmodel_non &lt;- lm(sale_price ~ market_value, data = opa_mdata)\nsummary(model_non)\n\n\n\nCall:\nlm(formula = sale_price ~ market_value, data = opa_mdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1432832   -35207    -1669    30014  1975714 \n\nCoefficients:\n                 Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  -9189.286424   663.771782  -13.84 &lt;0.0000000000000002 ***\nmarket_value     1.027924     0.001773  579.62 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 86310 on 40927 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8914 \nF-statistic: 3.36e+05 on 1 and 40927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\npred &lt;- predict(model_non, newdata = opa_mdata)\nresid &lt;- opa_mdata$sale_price - pred\n\n# RMSE\nrmse_value &lt;- sqrt(mean(resid^2, na.rm = TRUE))\nrmse_value\n\n\n[1] 86311.29\n\n\n\nThe results demonstrate a very strong linear relationship between sale_price and market_value. The market_value variable in the dataset effectively predicts the sale_price.\n\nTherefore, we can leverage this relationship to estimate the normal market prices for non-market transactions.\nWe record these estimated values as sale_price_predicted.\nBy doing this, we enhance our data!\n\n\n\nCode\n#bring data back\nopa_non_market$sale_price_predicted &lt;- predict(model_non, newdata = opa_non_market)\n\n#join back to the main data\nopa_selected &lt;- opa_selected %&gt;%\n  mutate(sale_price_predicted= sale_price)\n\nset.seed(123)\nopa_bind &lt;- bind_rows(opa_selected, opa_non_market) %&gt;%\n  slice_sample(prop = 1)\n\n\n1.2 Load and clean secondary dataset:\n\n1.2.1 Census data (tidycensus):\n\n\n\nCode\n# Transform to sf object \nopa_bind &lt;- st_as_sf(opa_bind, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(4326)\nopa_sf&lt;- opa_bind\n\n\n\n\nCode\n# Load Census data for Philadelphia tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    \n    ba_degree = \"B15003_022\",\n    total_edu = \"B15003_001\",\n    \n    median_income = \"B19013_001\",\n   \n    labor_force = \"B23025_003\",\n    unemployed = \"B23025_005\",\n    \n    total_housing = \"B25002_001\",\n    vacant_housing = \"B25002_003\"\n  ),\n  year = 2023,\n  state = \"PA\",\n  county = \"Philadelphia\",\n  geometry = TRUE\n) %&gt;%\n  dplyr::select(GEOID, variable, estimate, geometry) %&gt;%   \n  tidyr::pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  dplyr::mutate(\n    ba_rate = 100 * ba_degree / total_edu,\n    unemployment_rate = 100 * unemployed / labor_force,\n    vacancy_rate = 100 * vacant_housing / total_housing\n  ) %&gt;%\n  st_transform(st_crs(opa_sf))\n\n# Spatial join of OPA data with Census data\nopa_census &lt;- st_join(opa_sf, philly_census, join = st_within) %&gt;%\n  filter(!is.na(median_income))\n\n\n\n1.2.2 Spatial amenities (OpenDataPhilly)\n\n\n\nCode\n#load crime,poi,transit,hospital\nopa_census &lt;- st_transform(opa_census, 3857)\n\ncrime &lt;- read_csv(\"crime_sel.csv\") %&gt;% \n  filter(!is.na(lat) & !is.na(lng)) \ncrime_sf &lt;- st_as_sf(crime, coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census)) \n\npoi_sf &lt;- st_read(\"data/gis_osm_pois_a_free_1.shp\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census)) \n\nTransit &lt;- read_csv(\"Transit.csv\")\ntransit_sf &lt;- st_as_sf(Transit, coords = c(\"Lon\", \"Lat\"), crs = 4326) %&gt;%\n  st_transform(st_crs(opa_census))\n\nhospital_sf &lt;- st_read(\"hospitals.geojson\", quiet = TRUE) %&gt;%\n  st_transform(st_crs(opa_census))\n\n\n1.3 Summary tables showing before/after dimensions\n\n\nCode\n# Original data dimensions\nopa_dims &lt;- tibble(\n  dataset = \"raw CSV\",\n  rows = nrow(opa),\n  columns = ncol(opa)\n)\n\n# cleaned data dimensions\nopa_filter_dims &lt;- tibble(\n  dataset = \"after fixed criteria\",\n  rows = nrow(opa_clean2),\n  columns = ncol(opa_clean2)\n)\n\nopa_selected_dims &lt;- tibble(\n  dataset = \"after cleaned\",\n  rows = nrow(opa_bind),\n  columns = ncol(opa_bind)\n)\n# data dimensions (within census tracts)\nopa_census_dims &lt;- tibble(\n  dataset = \"after census joined\",\n  rows = nrow(opa_census),\n  columns = ncol(opa_census)\n)\n\n# create summary table\nsummary_table &lt;- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)\nlibrary(knitr)\nsummary_table %&gt;%\n  kable(caption = \"Summary of OPA data before and after cleaning\")\n\n\n\nSummary of OPA data before and after cleaning\n\n\ndataset\nrows\ncolumns\n\n\n\n\nraw CSV\n153267\n79\n\n\nafter fixed criteria\n34559\n79\n\n\nafter cleaned\n31968\n28\n\n\nafter census joined\n31613\n40\n\n\n\n\n\nPrinciples of Data Processing\n\nopa Data\n\nFilter sale_date between 2023-01-01 and 2024-12-31.\n\nKeep only residential properties (category_code == 1).\n\nRemove records with missing values in total_livable_area, sale_price, or number_of_bathrooms.\n\nFilter records year_built &gt; 0 .\n\nCensus data\n\nLoad data including total_pop, ba_degree, total_edu, median_income, labor_force, unemployed, total_housing, vacant_housing.\nTransform to spatial format and remove records with missing values.\n\nSpatial amenities\n\nLoad datasets Transit, crime, POIs, Hospitals.\nTransform to spatial format and remove records with missing values.\n\n\nInterpretation: The selected variables can be grouped into several meaningful categories that are theoretically and empirically linked to housing prices:\n\nNeighborhood Safety:\n\nCrime data: Areas with lower crime rates are generally more desirable, leading to higher property values. Including crime incidents helps capture the effect of public safety on housing prices.\n\n\nAccessibility and Transportation:\n\nTransit points: Proximity to public transportation (e.g., bus stops, subway stations) increases accessibility and convenience, which is often capitalized into higher home values.\n\nPoints of Interest (POIs): Nearby amenities such as shops, restaurants, and parks improve quality of life and can positively influence housing prices.\n\nHealthcare Access:\n\nHospitals: Easy access to healthcare facilities is a valued neighborhood characteristic, especially for families and older residents, and can contribute to higher property values.\n\n\nSocioeconomic and Demographic Context (from Census data):\n\nTotal population: Indicates population density, which can affect demand for housing.\n\nEducational attainment (e.g., % with BA degree): Higher education levels are often correlated with higher income and neighborhood desirability.\n\nMedian income: Directly influences purchasing power and demand for housing in an area.\n\nEmployment status (labor force and unemployment): Reflects economic stability and local job market health, which affect housing demand.\n\nHousing market conditions (total and vacant housing): Vacancy rates can signal neighborhood decline or oversupply, both of which impact prices.\n\nTogether, these variables provide a multidimensional view of a neighborhood‚Äôs appeal, safety, convenience, and economic health‚Äîall key determinants of housing prices."
  },
  {
    "objectID": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-2-feature-engineering",
    "href": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-2-feature-engineering",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 2: Feature Engineering",
    "text": "Phase 2: Feature Engineering\n2.1 Buffer-based features: \n\n2.1.1 Neighborhood avg sale price in the past year\n\n\n\nCode\n#filter the past sales data\n\nopa_census &lt;- opa_census %&gt;%\n  mutate(sale_id = row_number())\n\n\n\nopa_past &lt;- opa_var %&gt;% \n  filter(\n    non_market==0,\n    sale_date &gt;= \"2022-01-01\" & sale_date &lt;= \"2022-12-31\"\n  ) \n\nopa_past &lt;- opa_past %&gt;%\n  mutate(sale_id2 = row_number())\n\nopa_past &lt;- st_as_sf(opa_past, wkt = \"shape\", crs = 2272) %&gt;%\n  st_transform(3857)\n\nopa_census &lt;- st_transform(opa_census, 3857)\nopa_past   &lt;- st_transform(opa_past, 3857)\n\nopa_census_buffer &lt;- st_buffer(opa_census, 300)\ndrop_cols &lt;- names(opa_census_buffer) %in% c(\"sale_price\", \"total_livable_area\",\n                                             \"sale_price.y\", \"total_livable_area.y\")\nopa_census_buffer &lt;- opa_census_buffer[ , !drop_cols, drop = FALSE]\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\n\n\njoin_result &lt;- st_join(\n  opa_census_buffer,\n  opa_past,\n  join = st_intersects,\n  left = TRUE\n)\njoin_dedup &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  distinct(sale_id, sale_id2, .keep_all = TRUE)\n\nopa_summary &lt;- join_dedup %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(\n    past_count = sum(!is.na(sale_id2)),\n    avg_past_price_density = ifelse(\n      sum(!is.na(total_livable_area)) == 0, NA_real_,\n      sum(sale_price, na.rm = TRUE) / sum(total_livable_area, na.rm = TRUE)\n    ),\n    .groups = \"drop\"\n  )\nopa_census &lt;- opa_census %&gt;%\n  left_join(opa_summary, by = \"sale_id\")\n\n\n\n2.1.2 Crime numbers\n\n\n\nCode\nradius_cri &lt;- 250 \n\nopa_census$crime_count &lt;- lengths(st_is_within_distance(opa_census, crime_sf, dist = radius_cri)) \n\n\n\n2.1.3 POI Numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\npoi_sf_m     &lt;- st_transform(poi_sf, 3857)\n\n\nradius_poi &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_poi)\n\njoin_result &lt;- st_join(opa_census_buffer, poi_sf_m, join = st_intersects, left = TRUE)\n\npoi_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(poi_count = sum(!is.na(osm_id)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(poi_summary, by = \"sale_id\")\n\n\n\n2.1.4 Transit Numbers\n\n\n\nCode\nopa_census_m &lt;- st_transform(opa_census, 3857)\ntransit_sf_m &lt;- st_transform(transit_sf, 3857)\n\nradius_ts &lt;- 400\nopa_census_buffer &lt;- st_buffer(opa_census_m, radius_ts)\n\njoin_result &lt;- st_join(opa_census_buffer, transit_sf_m, join = st_intersects, left = TRUE)\n\ntransit_summary &lt;- join_result %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(sale_id) %&gt;%\n  summarise(transit_count = sum(!is.na(FID)))  \n\nopa_census &lt;- opa_census_m %&gt;%\n  left_join(transit_summary, by = \"sale_id\")\n\n\n2.2 k-Nearest Neighbor features: \n\n2.2.1 Hospitals (KNN-3)\n\n\n\nCode\nnearest_hospital_index &lt;- st_nn(\n  opa_census,\n  hospital_sf,\n  k = 3,            \n  returnDist = TRUE \n)\n\nopa_census$nearest_hospital_knn3 &lt;- sapply(nearest_hospital_index$dist, mean)\n\n\n2.3 Weights: \n\n\nCode\n# add different weights to actual and non market transactions\n\nopa_census_all &lt;- opa_census\n\nnon_market_share&lt;- mean(opa_census_all$non_market == 1, na.rm = TRUE)\nnon_market_share\n\n\n[1] 0.261791\n\n\nCode\nopa_census_all &lt;- opa_census %&gt;%\n  mutate(weight_mix = ifelse(non_market == 1, non_market_share, 1))\n\n\n2.4 Transformation: \n\n\nCode\n#standardize houseage and median income\nmean_age_all &lt;- mean(opa_census_all$house_age, na.rm = TRUE)\nmean_income_log &lt;- mean(log(opa_census_all$median_income), na.rm = TRUE)\n\n# centralize\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(\n    house_age_c  = house_age - mean_age_all,\n    house_age_c2 = house_age_c^2,\n    income_log   = log(median_income),\n    income_scaled = income_log - mean_income_log\n  )\n\nopa_census_2&lt;- opa_census_all %&gt;%\n  filter(\n    non_market==0\n  )"
  },
  {
    "objectID": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-3-exploratory-data-analysis",
    "href": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-3-exploratory-data-analysis",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 3: Exploratory Data Analysis",
    "text": "Phase 3: Exploratory Data Analysis\n\n3.1 Distribution of sale prices (histogram)\n\n\nCode\nggplot(opa_census_all, aes(x = sale_price_predicted)) +\n  geom_histogram(\n    bins = 50,\n    fill = \"#6A1B9A\",\n    color = \"white\",\n    alpha = 0.8  \n  ) +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Sale Prices\",\n    x = \"Predicted Sale Price (USD)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(opa_census_all, aes(x = log(sale_price_predicted))) +\n  geom_histogram(\n    bins = 50,\n    fill = \"#6A1B9A\",\n    color = \"white\",\n    alpha = 0.8\n  ) +\n  scale_x_continuous() +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Distribution of  Log(Sale Prices)\",\n    x = \"Log(Predicted Sale Price) \",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings:\n\nHighly Right-Skewed: The bulk of the distribution is concentrated on the left side (low price range), while the right tail is very long, extending towards higher prices. This means that the majority of houses have lower prices, and very expensive houses are very few in number.\nConcentration and Outliers: The count of houses in each bin drops rapidly as the price increases. The number of houses above $2,500,000 is very small, indicating the presence of extreme high-price outliers.\nPreprocessing Requirement: This distribution strongly suggests that before building a house price prediction model, the Sale Price variable will need a transformation, most commonly a log transformation, to make the distribution more closely resemble a normal distribution.\n\n\n\n3.2 Spatial distribution of sale prices (map)\n\n\nCode\nopa_census_all &lt;- opa_census_all %&gt;%\n  mutate(price_quartile = ntile(sale_price_predicted, 4))\n\nggplot() +\n  geom_sf(data = philly_census, fill = \"lightgrey\", color = \"white\") +\n  geom_sf(data = opa_census_all, aes(color = factor(price_quartile)), size = 0.5, alpha = 0.7) +\n  scale_color_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    labels = c(\"0%-25%\", \"25%-50%\", \"50%-75%\", \"75%-100%\")\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Housing Sales Price in Philadelphia (2023‚Äì2024)\",\n    color = \"Sale Price Quartile\"\n  )  +\n  guides(color = guide_legend(override.aes = list(size = 3)))\n\n\n\n\n\n\n\n\n\nKey Findings:\n\nSpatial Concentration of Housing Prices: Highest Prices are clearly concentrated in the Center City/Downtown area of Philadelphia and its immediate surroundings, which indicates that the most expensive property transactions occur in the high-value areas of and around the city center.\nDiscontinuous Price Transitions: Housing prices do not exhibit smooth gradients across the city; instead, they show abrupt changes between different price quartiles, forming distinct spatial clusters. This suggests that price variations are influenced by fixed boundaries such as neighborhoods, infrastructure, or socio-economic factors, rather than continuous spatial diffusion.\nPeripheral Price Patterns: Lower-price quartiles (0%-25% and 25%-50%) are predominantly located in the outer regions of the city, particularly in the northern and southern peripheries, indicating a clear core-periphery divide in housing values.\n\n\n\n3.3 Price vs.¬†structural features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(log_sale_price = log(sale_price_predicted))\n\n# 1Ô∏è‚É£ total_livable_area\np1 &lt;- ggplot(opa_census_plot, aes(x = log(total_livable_area), y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Log(Livable Area)\",\n    x = \"Log(Total Livable Area)\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 2Ô∏è‚É£ number_of_bathrooms\np2 &lt;- ggplot(opa_census_plot, aes(x = number_of_bathrooms, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Bathrooms\",\n    x = \"Number of Bathrooms\",  \n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 3Ô∏è‚É£ interior_condition\np3 &lt;- ggplot(opa_census_plot, aes(x = interior_condition, y = log_sale_price)) +\n  geom_jitter(alpha = 0.5, color = \"#6A1B9A\", width = 0.2, height = 0) +  \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. Interior Condition\",\n    x = \"Interior Condition\",\n    y = \"Log(Sale Price)\"\n  ) +\n  theme_minimal(base_size = 10)\n\n# 4Ô∏è‚É£ house_age\np4 &lt;- ggplot(opa_census_plot, aes(x = house_age^2, y = log_sale_price)) + \n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(\n    title = \"Log(Sale Price) vs. House Age¬≤\",\n    x = \"House Age¬≤ (2025 - Year Built)¬≤\",\n    y = \"\"\n  ) +\n  theme_minimal(base_size = 10)\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nKey Findings:\n\nStrong Positive Correlation with Size and Bathrooms: There is a clear positive linear relationship between the log of sale price and both the log of total livable area and the number of bathrooms. This indicates that larger properties and those with more bathrooms command significantly higher market prices.\nNon-Linear Relationship with Interior Condition: While a general positive trend exists, the relationship between log sale price and interior condition rating is not perfectly linear. The data dispersion suggests that the effect of interior condition on price may be subject to diminishing marginal returns or other non-linear dynamics.\nNegative Correlation with House Age Squared: A significant negative relationship is observed between log sale price and the squared term of house age. This indicates that property values depreciate as homes get older, and this depreciation effect may accelerate over time, reflecting a non-linear aging effect on housing value.\n\n\n\n3.4 Price vs.¬†spatial & Social features (scatter plots)\n\n\nCode\nopa_census_plot &lt;- opa_census_all %&gt;%\n  mutate(\n    log_sale_price = log(sale_price_predicted),\n    sqrt_crime_count = sqrt(crime_count)\n  )\n\np1 &lt;- ggplot(opa_census_plot, aes(x = ba_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. BA Rate\",\n       x = \"Bachelor's Degree Rate\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np2 &lt;- ggplot(opa_census_plot, aes(x = unemployment_rate, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(title = \"Log(Sale Price) vs. Unemployment Rate\",\n       x = \"Unemployment Rate\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np3 &lt;- ggplot(opa_census_plot, aes(x = median_income, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  scale_x_continuous(labels = dollar_format()) +\n  labs(title = \"Log(Sale Price) vs. Median Income\",\n       x = \"Median Household Income (USD)\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np4 &lt;- ggplot(opa_census_plot, aes(x = avg_past_price_density, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Past Price Density\",\n       x = \"Average Past Price Density\", y = \"\") +\n  theme_minimal(base_size = 10)\n\np5 &lt;- ggplot(opa_census_plot, aes(x = sqrt_crime_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. ‚àö(Crime Count)\",\n       x = \"Square Root of Crime Count\", y = \"Log(Sale Price)\") +\n  theme_minimal(base_size = 10)\n\np6 &lt;- ggplot(opa_census_plot, aes(x = transit_count, y = log_sale_price)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  labs(title = \"Log(Sale Price) vs. Transit Count\",\n       x = \"Number of Transit Stops Nearby\", y = \"\") +\n  theme_minimal(base_size = 10)\n\n(p1 | p2 | p3) / (p4 | p5 | p6)\n\n\n\n\n\n\n\n\n\nKey Findings:\n\nStrong Socioeconomic Influence: Housing prices show clear positive correlations with key socioeconomic indicators. Both bachelor‚Äôs degree rate and median household income exhibit strong positive relationships with sale prices, indicating that neighborhoods with higher educational attainment and income levels command substantially higher property values.\nNegative Impact of Crime and Unemployment: There are evident negative relationships between housing prices and both crime levels (measured by square root of crime count) and unemployment rates. This demonstrates that public safety and local economic vitality are significant determinants of property values in Philadelphia.\nPositive Effects of Historical Prices and Transit Access: Sale prices maintain a positive relationship with both historical price density and accessibility to public transportation. This suggests that areas with established high-value characteristics and better transit infrastructure maintain their premium in the housing market, reflecting path dependence in neighborhood valuation and the value of transportation accessibility.\n\n\n\nOther visualization\n\n\nCode\ntract_price &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(avg_past_price_density = mean(avg_past_price_density, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_price, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = avg_past_price_density), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",\n    name = \"Mean Sale Price Per sqft\",\n    labels = scales::dollar_format(),\n    na.value = \"grey60\"\n  ) +\n  scale_alpha(range = c(0.2, 0.8), name = \"Interior Condition\") +\n  \n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Buffered Area Mean Sold Price of 2022\",\n    subtitle = \"clustered in census tracts\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_condition &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_interior_condition = mean(interior_condition, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_condition, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_interior_condition), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"#6A1B9A\", high = \"white\",\n    name = \"Avg Interior Condition\",\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Interior Condition by Census Tract\",\n    subtitle = \"Darker color = better average condition\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntract_area &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_total_livable_area = mean(total_livable_area, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_area, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_total_livable_area), color = \"white\", size = 0.2) +\n  scale_fill_gradient(\n    low = \"white\", high = \"#6A1B9A\",   \n    name = \"Avg Livable Area (sqft)\",\n    labels = scales::comma_format(),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Average Livable Area by Census Tract\",\n    subtitle = \"Darker color indicates larger average livable area\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nKey Findings:\n\nSpatial Variation in Property Values: The average sale price per square foot shows significant geographic clustering across census tracts, with distinct high-value areas concentrated in specific neighborhoods. This indicates strong spatial autocorrelation in housing prices, where adjacent tracts tend to have similar price levels.\nCorrelation Between Property Condition and Location: Better average interior conditions are systematically concentrated in particular geographic areas, suggesting that housing maintenance and quality are not randomly distributed but follow spatial patterns that may correlate with neighborhood characteristics and property values.\nHeterogeneous Distribution of Housing Size: The average livable area varies substantially across census tracts, with larger properties clustered in specific regions. This spatial patterning of housing size complements the price distribution, indicating that both property characteristics and location factors contribute to the overall housing market structure in Philadelphia."
  },
  {
    "objectID": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-4-model-building",
    "href": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-4-model-building",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 4: Model Building",
    "text": "Phase 4: Model Building\n\nBuild models progressively\n4.1 Structural features only:\n\n\nCode\nmodel_1 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +  #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing, \n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_1)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing, data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2930 -0.2144  0.0510  0.2911  2.4585 \n\nCoefficients:\n                            Estimate   Std. Error t value            Pr(&gt;|t|)\n(Intercept)              6.435857858  0.077629460  82.905 &lt;0.0000000000000002\nlog(total_livable_area)  0.744348571  0.010689344  69.635 &lt;0.0000000000000002\nnumber_of_bathrooms      0.051667122  0.005580674   9.258 &lt;0.0000000000000002\nhouse_age_c              0.000002273  0.000118374   0.019               0.985\nhouse_age_c2             0.000046457  0.000001746  26.607 &lt;0.0000000000000002\ninterior_condition      -0.112636239  0.004358818 -25.841 &lt;0.0000000000000002\nquality_grade_num        0.069613279  0.002849720  24.428 &lt;0.0000000000000002\nfireplaces               0.116405937  0.010884143  10.695 &lt;0.0000000000000002\ngarage_spaces            0.140429585  0.006549989  21.440 &lt;0.0000000000000002\ncentral_air_dummy        0.458743112  0.008036173  57.085 &lt;0.0000000000000002\ncentral_air_missing     -0.237637667  0.008809496 -26.975 &lt;0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num       ***\nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4911 on 31602 degrees of freedom\nMultiple R-squared:  0.5212,    Adjusted R-squared:  0.521 \nF-statistic:  3439 on 10 and 31602 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation: \n\nlog(total_livable_area) (0.752): An elasticity coefficient. A 1% increase in livable area is associated with a 0.752% increase in price. This is a strong positive driver.\nnumber_of_bathrooms (0.046): Each additional bathroom is associated with a 4.6% increase in price.\nhouse_age_c (-0.00001): The linear term for house age is statistically insignificant (p=0.929).\nhouse_age_c2 (0.000047): The squared term for age is positive and significant. Combined with the insignificant linear term, this suggests a slight U-shaped relationship, where new homes and very old homes (perhaps with historical value) command a premium over middle-aged homes.\ninterior_condition (-0.114): Assuming a higher value means worse condition, each one-unit worsening in condition is associated with an 11.4% decrease in price.\n\nquality_grade_num (0.070): Each one-unit increase in the quality grade is associated with a 7.0% increase in price.\nfireplaces (0.117): Each additional fireplace is associated with a 11.7% increase in price.\ngarage_spaces (0.143): Each additional garage space is associated with a 14.3% increase in price.\ncentral_air_dummy (0.458): Homes with central air are estimated to be 45.8% more expensive than the baseline (e.g., no AC). This is a very significant amenity premium.\ncentral_air_missing (-0.230): Homes where central air data is missing are 23.0% cheaper than the baseline.\n\n4.2 Census variables:\n\n\nCode\nmodel_2 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census\n                  ba_rate +\n                  unemployment_rate,\n  \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_2)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate, \n    data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-3.2514 -0.1415  0.0546  0.2237  2.0880 \n\nCoefficients:\n                            Estimate   Std. Error t value             Pr(&gt;|t|)\n(Intercept)              7.279501574  0.064443586 112.959 &lt; 0.0000000000000002\nlog(total_livable_area)  0.700431343  0.008755698  79.997 &lt; 0.0000000000000002\nnumber_of_bathrooms      0.057764788  0.004568488  12.644 &lt; 0.0000000000000002\nhouse_age_c             -0.000064246  0.000097377  -0.660                0.509\nhouse_age_c2             0.000009814  0.000001468   6.686    0.000000000023255\ninterior_condition      -0.126482820  0.003572164 -35.408 &lt; 0.0000000000000002\nquality_grade_num        0.001702051  0.002417186   0.704                0.481\nfireplaces               0.064233267  0.008917462   7.203    0.000000000000602\ngarage_spaces            0.168324520  0.005472052  30.761 &lt; 0.0000000000000002\ncentral_air_dummy        0.219136581  0.006851612  31.983 &lt; 0.0000000000000002\ncentral_air_missing     -0.155840316  0.007252663 -21.487 &lt; 0.0000000000000002\nincome_scaled            0.453407655  0.009052596  50.086 &lt; 0.0000000000000002\nba_rate                  0.012813063  0.000358216  35.769 &lt; 0.0000000000000002\nunemployment_rate       -0.006619614  0.000527486 -12.549 &lt; 0.0000000000000002\n                           \n(Intercept)             ***\nlog(total_livable_area) ***\nnumber_of_bathrooms     ***\nhouse_age_c                \nhouse_age_c2            ***\ninterior_condition      ***\nquality_grade_num          \nfireplaces              ***\ngarage_spaces           ***\ncentral_air_dummy       ***\ncentral_air_missing     ***\nincome_scaled           ***\nba_rate                 ***\nunemployment_rate       ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4017 on 31599 degrees of freedom\nMultiple R-squared:  0.6796,    Adjusted R-squared:  0.6794 \nF-statistic:  5155 on 13 and 31599 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation: \n\nCoefficient Evolution (vs.¬†Model 1):\n\nlog(total_livable_area) (0.710 vs 0.752): The elasticity of area decreased. This suggests Model 1 overestimated the impact of area. Why? Because larger homes are often located in wealthier neighborhoods. Model 1 incorrectly attributed some of the ‚Äúwealthy neighborhood‚Äù premium to ‚Äúlarge area.‚Äù\nquality_grade_num (0.0015 vs 0.070): The coefficient for quality grade became statistically insignificant (p=0.520). This is a key finding: home quality is highly correlated with neighborhood income. Once we directly control for income (income_scaled), the independent effect of quality grade disappears.\ncentral_air_dummy (0.219 vs 0.458): The premium for central air was halved. This also indicates that central air is more common in affluent areas, and Model 1 suffered from significant Omitted Variable Bias (OVB).\n\nNew Variable (Census) Interpretation:\n\nincome_scaled (0.455): A one-unit increase in standardized census tract income is associated with a 45.5% increase in price. A very strong positive effect.\nba_rate (0.0129): A 1 percentage point increase in the neighborhood‚Äôs bachelor‚Äôs degree attainment rate is associated with a 1.3% price increase.\nunemployment_rate (-0.0066): A 1 percentage point increase in the unemployment rate is associated with a 0.66% decrease in price.\n\n\n4.3 Spatial features:\n\n\nCode\nmodel_3 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_3)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-3.03217 -0.09905  0.05666  0.18464  2.17114 \n\nCoefficients:\n                               Estimate   Std. Error t value\n(Intercept)                 6.562309985  0.084718417  77.460\nlog(total_livable_area)     0.742766329  0.007908779  93.917\nnumber_of_bathrooms         0.057510788  0.004057815  14.173\nhouse_age_c                -0.000093444  0.000087876  -1.063\nhouse_age_c2               -0.000005113  0.000001322  -3.866\ninterior_condition         -0.161139310  0.003179231 -50.685\nquality_grade_num          -0.029091664  0.002254432 -12.904\nfireplaces                  0.031025554  0.008023515   3.867\ngarage_spaces               0.096993528  0.005071631  19.125\ncentral_air_dummy           0.099045830  0.006176862  16.035\ncentral_air_missing        -0.164351875  0.006393736 -25.705\nincome_scaled               0.166039882  0.008635180  19.228\nba_rate                     0.003250996  0.000349765   9.295\nunemployment_rate          -0.002568034  0.000466955  -5.500\ntransit_count               0.000311944  0.000171205   1.822\navg_past_price_density      0.003018698  0.000039395  76.626\nsqrt(crime_count)          -0.049042531  0.001408152 -34.828\nlog(nearest_hospital_knn3)  0.081937118  0.006616703  12.383\n                                       Pr(&gt;|t|)    \n(Intercept)                &lt; 0.0000000000000002 ***\nlog(total_livable_area)    &lt; 0.0000000000000002 ***\nnumber_of_bathrooms        &lt; 0.0000000000000002 ***\nhouse_age_c                            0.287626    \nhouse_age_c2                           0.000111 ***\ninterior_condition         &lt; 0.0000000000000002 ***\nquality_grade_num          &lt; 0.0000000000000002 ***\nfireplaces                             0.000110 ***\ngarage_spaces              &lt; 0.0000000000000002 ***\ncentral_air_dummy          &lt; 0.0000000000000002 ***\ncentral_air_missing        &lt; 0.0000000000000002 ***\nincome_scaled              &lt; 0.0000000000000002 ***\nba_rate                    &lt; 0.0000000000000002 ***\nunemployment_rate                  0.0000000384 ***\ntransit_count                          0.068458 .  \navg_past_price_density     &lt; 0.0000000000000002 ***\nsqrt(crime_count)          &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3) &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3536 on 31481 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7505,    Adjusted R-squared:  0.7504 \nF-statistic:  5571 on 17 and 31481 DF,  p-value: &lt; 0.00000000000000022\n\n\nCoefficient Interpretation: \n\nCoefficient Evolution (vs.¬†Model 2):\n\nincome_scaled (0.146 vs 0.455): The effect of income dropped sharply (by ~2/3). This again reveals OVB in Model 2. The large ‚Äúincome‚Äù effect in Model 2 was confounded with ‚Äúspatial amenities‚Äù‚Äîhigh-income individuals tend to live in low-crime, accessible areas.\nba_rate (0.0027 vs 0.0129): The education premium also dropped significantly for the same reason.\ngarage_spaces (0.095 vs 0.170): The garage premium decreased, likely because spatial variables (like density or transit access) have captured related information.\n\nNew Variable (Spatial) Interpretation:\n\ntransit_count (0.00029): Each additional nearby public transit stop is associated with a 0.029% increase in price.\navg_past_price_density (0.0032): As a proxy for local market heat or locational value, each unit increase is associated with a 0.32% price increase.\nsqrt(crime_count) (-0.040): A one-unit increase in the square root of the crime count is associated with a 4.0% decrease in price.\nlog(nearest_hospital_knn3) (0.087): A 1% increase in the distance from the nearest hospital is associated with a 0.087% increase in price. This suggests people prefer to live further away from hospitals (perhaps to avoid noise, traffic, or sirens), not closer.\n\n\n4.4 Interactions and fixed effects:\n\n\nCode\nmodel_4 &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  +   #Structural\n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                \n                  income_scaled +              #Census \n                  ba_rate +\n                  unemployment_rate +\n                \n                  transit_count+\n                  avg_past_price_density+      #Spatial \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3)+\n                \n                  (interior_condition * income_scaled)+  #FE & Interaction\n                  factor(zip_code),\n              \n              data = opa_census_all,\n              weight=weight_mix\n               )\nsummary(model_4)\n\n\n\nCall:\nlm(formula = log(sale_price_predicted) ~ log(total_livable_area) + \n    number_of_bathrooms + house_age_c + house_age_c2 + interior_condition + \n    quality_grade_num + fireplaces + garage_spaces + central_air_dummy + \n    central_air_missing + income_scaled + ba_rate + unemployment_rate + \n    transit_count + avg_past_price_density + sqrt(crime_count) + \n    log(nearest_hospital_knn3) + (interior_condition * income_scaled) + \n    factor(zip_code), data = opa_census_all, weights = weight_mix)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-2.86007 -0.09067  0.05500  0.16974  2.17553 \n\nCoefficients:\n                                     Estimate   Std. Error t value\n(Intercept)                       7.147174262  0.125631514  56.890\nlog(total_livable_area)           0.762702187  0.007951214  95.923\nnumber_of_bathrooms               0.062545809  0.003951966  15.827\nhouse_age_c                       0.000081673  0.000091303   0.895\nhouse_age_c2                      0.000002396  0.000001333   1.797\ninterior_condition               -0.167693153  0.003180473 -52.726\nquality_grade_num                -0.020614687  0.002369293  -8.701\nfireplaces                        0.040919276  0.008065426   5.073\ngarage_spaces                     0.062755291  0.005224896  12.011\ncentral_air_dummy                 0.088693723  0.006224102  14.250\ncentral_air_missing              -0.145545244  0.006529396 -22.291\nincome_scaled                    -0.308308801  0.022099101 -13.951\nba_rate                           0.004727434  0.000434125  10.890\nunemployment_rate                -0.002009575  0.000512327  -3.922\ntransit_count                     0.000210309  0.000188166   1.118\navg_past_price_density            0.002676947  0.000053851  49.710\nsqrt(crime_count)                -0.040746561  0.001633811 -24.940\nlog(nearest_hospital_knn3)       -0.007368586  0.012879355  -0.572\nfactor(zip_code)19103            -0.191888874  0.038623356  -4.968\nfactor(zip_code)19104             0.043168643  0.044596804   0.968\nfactor(zip_code)19106            -0.072911591  0.039431744  -1.849\nfactor(zip_code)19107             0.002870411  0.041113568   0.070\nfactor(zip_code)19111             0.092033842  0.042730139   2.154\nfactor(zip_code)19114             0.046991631  0.045784241   1.026\nfactor(zip_code)19115             0.036786258  0.045577032   0.807\nfactor(zip_code)19116             0.074693711  0.047046924   1.588\nfactor(zip_code)19118             0.042277408  0.050964443   0.830\nfactor(zip_code)19119            -0.005571634  0.045415173  -0.123\nfactor(zip_code)19120            -0.001979921  0.042451411  -0.047\nfactor(zip_code)19121            -0.079268931  0.042967693  -1.845\nfactor(zip_code)19122            -0.046067555  0.043677075  -1.055\nfactor(zip_code)19123            -0.124474312  0.042021722  -2.962\nfactor(zip_code)19124            -0.042922550  0.042779720  -1.003\nfactor(zip_code)19125            -0.053924641  0.040136398  -1.344\nfactor(zip_code)19126            -0.096134211  0.050099576  -1.919\nfactor(zip_code)19127            -0.054620383  0.046526938  -1.174\nfactor(zip_code)19128            -0.063379705  0.041111172  -1.542\nfactor(zip_code)19129            -0.081840032  0.044983229  -1.819\nfactor(zip_code)19130             0.004465219  0.038447508   0.116\nfactor(zip_code)19131            -0.124963371  0.044224170  -2.826\nfactor(zip_code)19132            -0.353731640  0.042632020  -8.297\nfactor(zip_code)19133            -0.360578685  0.045760207  -7.880\nfactor(zip_code)19134            -0.167019067  0.042150583  -3.962\nfactor(zip_code)19135             0.066237684  0.045406809   1.459\nfactor(zip_code)19136             0.093091097  0.045399194   2.051\nfactor(zip_code)19137             0.003922355  0.050119037   0.078\nfactor(zip_code)19138            -0.061547285  0.045851962  -1.342\nfactor(zip_code)19139            -0.125514020  0.043923079  -2.858\nfactor(zip_code)19140            -0.243638620  0.042070333  -5.791\nfactor(zip_code)19141            -0.104140516  0.045656220  -2.281\nfactor(zip_code)19142            -0.117894811  0.045192100  -2.609\nfactor(zip_code)19143            -0.121037876  0.042647300  -2.838\nfactor(zip_code)19144            -0.119218035  0.044443147  -2.682\nfactor(zip_code)19145             0.000416732  0.040490607   0.010\nfactor(zip_code)19146            -0.065296494  0.038364344  -1.702\nfactor(zip_code)19147            -0.022134932  0.038396424  -0.576\nfactor(zip_code)19148             0.034927326  0.039935049   0.875\nfactor(zip_code)19149             0.163264326  0.043056311   3.792\nfactor(zip_code)19150            -0.044481749  0.048200789  -0.923\nfactor(zip_code)19151            -0.087776350  0.045219855  -1.941\nfactor(zip_code)19152             0.091653850  0.044520396   2.059\nfactor(zip_code)19153            -0.033554531  0.052491824  -0.639\nfactor(zip_code)19154             0.056967215  0.044374083   1.284\ninterior_condition:income_scaled  0.119746969  0.005588581  21.427\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nlog(total_livable_area)          &lt; 0.0000000000000002 ***\nnumber_of_bathrooms              &lt; 0.0000000000000002 ***\nhouse_age_c                                   0.37105    \nhouse_age_c2                                  0.07241 .  \ninterior_condition               &lt; 0.0000000000000002 ***\nquality_grade_num                &lt; 0.0000000000000002 ***\nfireplaces                        0.00000039295484010 ***\ngarage_spaces                    &lt; 0.0000000000000002 ***\ncentral_air_dummy                &lt; 0.0000000000000002 ***\ncentral_air_missing              &lt; 0.0000000000000002 ***\nincome_scaled                    &lt; 0.0000000000000002 ***\nba_rate                          &lt; 0.0000000000000002 ***\nunemployment_rate                 0.00008784000945812 ***\ntransit_count                                 0.26371    \navg_past_price_density           &lt; 0.0000000000000002 ***\nsqrt(crime_count)                &lt; 0.0000000000000002 ***\nlog(nearest_hospital_knn3)                    0.56724    \nfactor(zip_code)19103             0.00000067928689067 ***\nfactor(zip_code)19104                         0.33306    \nfactor(zip_code)19106                         0.06446 .  \nfactor(zip_code)19107                         0.94434    \nfactor(zip_code)19111                         0.03126 *  \nfactor(zip_code)19114                         0.30472    \nfactor(zip_code)19115                         0.41960    \nfactor(zip_code)19116                         0.11238    \nfactor(zip_code)19118                         0.40680    \nfactor(zip_code)19119                         0.90236    \nfactor(zip_code)19120                         0.96280    \nfactor(zip_code)19121                         0.06507 .  \nfactor(zip_code)19122                         0.29156    \nfactor(zip_code)19123                         0.00306 ** \nfactor(zip_code)19124                         0.31571    \nfactor(zip_code)19125                         0.17911    \nfactor(zip_code)19126                         0.05501 .  \nfactor(zip_code)19127                         0.24042    \nfactor(zip_code)19128                         0.12316    \nfactor(zip_code)19129                         0.06887 .  \nfactor(zip_code)19130                         0.90754    \nfactor(zip_code)19131                         0.00472 ** \nfactor(zip_code)19132            &lt; 0.0000000000000002 ***\nfactor(zip_code)19133             0.00000000000000339 ***\nfactor(zip_code)19134             0.00007435204084637 ***\nfactor(zip_code)19135                         0.14464    \nfactor(zip_code)19136                         0.04032 *  \nfactor(zip_code)19137                         0.93762    \nfactor(zip_code)19138                         0.17951    \nfactor(zip_code)19139                         0.00427 ** \nfactor(zip_code)19140             0.00000000705409283 ***\nfactor(zip_code)19141                         0.02256 *  \nfactor(zip_code)19142                         0.00909 ** \nfactor(zip_code)19143                         0.00454 ** \nfactor(zip_code)19144                         0.00731 ** \nfactor(zip_code)19145                         0.99179    \nfactor(zip_code)19146                         0.08876 .  \nfactor(zip_code)19147                         0.56429    \nfactor(zip_code)19148                         0.38180    \nfactor(zip_code)19149                         0.00015 ***\nfactor(zip_code)19150                         0.35610    \nfactor(zip_code)19151                         0.05225 .  \nfactor(zip_code)19152                         0.03953 *  \nfactor(zip_code)19153                         0.52268    \nfactor(zip_code)19154                         0.19922    \ninterior_condition:income_scaled &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3423 on 31435 degrees of freedom\n  (114 observations deleted due to missingness)\nMultiple R-squared:  0.7665,    Adjusted R-squared:  0.7661 \nF-statistic:  1638 on 63 and 31435 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\nvif(model_4)\n\n\n                                       GVIF Df GVIF^(1/(2*Df))\nlog(total_livable_area)            1.523908  1        1.234467\nnumber_of_bathrooms                1.611921  1        1.269614\nhouse_age_c                        1.580710  1        1.257263\nhouse_age_c2                       1.470007  1        1.212439\ninterior_condition                 1.533402  1        1.238306\nquality_grade_num                  1.779716  1        1.334060\nfireplaces                         1.295084  1        1.138018\ngarage_spaces                      1.586272  1        1.259473\ncentral_air_dummy                  2.084180  1        1.443669\ncentral_air_missing                1.453277  1        1.205519\nincome_scaled                     24.491928  1        4.948932\nba_rate                            6.178507  1        2.485660\nunemployment_rate                  2.016927  1        1.420186\ntransit_count                      1.715169  1        1.309645\navg_past_price_density             7.837830  1        2.799612\nsqrt(crime_count)                  2.815547  1        1.677959\nlog(nearest_hospital_knn3)         8.102261  1        2.846447\nfactor(zip_code)                 565.126609 45        1.072950\ninterior_condition:income_scaled  20.090358  1        4.482227\n\n\nCoefficient Interpretation: \n\nFixed Effects Interpretation:\n\nThese coefficients represent the price difference for each zip code relative to the ‚Äúreference zip code‚Äù (which is omitted from the list, e.g.¬†19102).\nExample: factor(zip_code)19106 (-0.107): A home in zip code 19106 is, on average, 10.7% less expensive than a home in the reference zip code, holding all other variables constant.\nExample: factor(zip_code)19149 (0.183): A home in zip code 19149 is, on average, 18.3% more expensive.\n\ninterior_condition: income_scaled (0.117) (Interaction Term):\n\nThis is one of the most interesting findings. It shows that the impact of interior_condition depends on income_scaled.\nThe total marginal effect of interior_condition is:\\[= -0.1695 + 0.1165 \\times \\text{income\\_scaled}\\]\nAt the baseline income level (income_scaled = 0), each one-unit worsening in condition is associated with a 17.0% price decrease (-0.1695). However, this penalty is mitigated (lessened) in higher-income areas. For each one-unit increase in income_scaled, the negative penalty of poor condition is reduced by 11.7 percentage points. This may imply that in high-income neighborhoods, ‚Äúfixer-uppers‚Äù (homes in poor condition) are seen as investment opportunities with high renovation potential. Therefore, the market penalty for ‚Äúpoor condition‚Äù is smaller."
  },
  {
    "objectID": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-5-model-validation",
    "href": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-5-model-validation",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 5: Model Validation",
    "text": "Phase 5: Model Validation\n\n10-fold cross-validation\n5.1 Compare all 4 models:\n\n5.1.1 Create predicted vs.¬†actual plot\n\n\n\nCode\nopa_census_2_clean &lt;- opa_census_2\n\nmodels &lt;- list(model_1, model_2, model_3, model_4)\nmodel_names &lt;- c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\")\n\n\noptions(scipen = 999)\n\nall_pred_usd &lt;- c()\nfor (m in models) {\n  pred_log_tmp &lt;- predict(m, newdata = opa_census_2_clean)\n  smearing_tmp &lt;- mean(exp(residuals(m)), na.rm = TRUE)\n  pred_usd_tmp &lt;- exp(pred_log_tmp) * smearing_tmp\n  all_pred_usd &lt;- c(all_pred_usd, pred_usd_tmp)\n}\nactual_usd &lt;- opa_census_2_clean$sale_price_predicted\n\nactual_k &lt;- actual_usd / 1000\npred_all_k &lt;- all_pred_usd / 1000\n\nx_min &lt;- min(actual_k, pred_all_k, na.rm = TRUE)\nx_max &lt;- 8000               # manually fix max x at 8000K\ny_min &lt;- min(actual_k, pred_all_k, na.rm = TRUE)\ny_max &lt;- max(actual_k, pred_all_k, na.rm = TRUE)\n\nx_ticks &lt;- pretty(c(x_min, x_max), n = 6)\ny_ticks &lt;- pretty(c(y_min, y_max), n = 6)\n\n# Loop\nfor (i in seq_along(models)) {\n  model &lt;- models[[i]]\n  model_name &lt;- model_names[i]\n  \n  #  Predict on the same validation dataset \n  pred_log &lt;- predict(model, newdata = opa_census_2_clean)\n  \n  # Smearing correction (to restore to USD scale) \n  smearing_factor &lt;- mean(exp(residuals(model)), na.rm = TRUE)\n  pred_usd &lt;- exp(pred_log) * smearing_factor\n  pred_k &lt;- pred_usd / 1000   # Convert to thousand dollars\n  \n\n  rmse &lt;- sqrt(mean((pred_usd - opa_census_2_clean$sale_price_predicted)^2, na.rm = TRUE))\n  rmse_norm &lt;- rmse / mean(opa_census_2_clean$sale_price_predicted, na.rm = TRUE)\n  \n\n  cat(\"\\n============================\\n\")\n  cat(model_name, \"\\n\")\n  cat(\"RMSE (USD):\", round(rmse, 2), \"\\n\")\n  cat(\"Normalized RMSE:\", round(rmse_norm, 4), \"\\n\")\n  cat(\"============================\\n\")\n  \n  \n  plot(\n    actual_k, pred_k,\n    xlab = \"Actual Price ($K)\",\n    ylab = \"Predicted Price ($K)\",\n    main = paste(model_name, \"- Predicted vs Actual Sale Price\"),\n    pch = 19,\n    col = rgb(0.2, 0.4, 0.6, 0.4),\n    xlim = c(x_min, x_max),\n    ylim = c(y_min, y_max),\n    axes = FALSE\n  )\n\n  axis(1, at = x_ticks, labels = paste0(x_ticks, \"K\"))\n  axis(2, at = y_ticks, labels = paste0(y_ticks, \"K\"), las = 1)\n  box()\n  \n  # Add 45-degree line\n  abline(0, 1, col = \"red\", lwd = 2)\n  \n  #Save as PNG with same scale\n  png_filename &lt;- paste0(\"pred_actual_\", gsub(\" \", \"_\", tolower(model_name)), \".png\")\n  png(png_filename, width = 900, height = 800)\n  par(mar = c(5, 5, 4, 2))\n  \n  plot(\n    actual_k, pred_k,\n    xlab = \"Actual Price ($K)\",\n    ylab = \"Predicted Price ($K)\",\n    main = paste(model_name, \"- Predicted vs Actual Sale Price\"),\n    pch = 19,\n    col = rgb(0.2, 0.4, 0.6, 0.4),\n    xlim = c(x_min, x_max),\n    ylim = c(y_min, y_max),\n    axes = FALSE\n  )\n  axis(1, at = x_ticks, labels = paste0(x_ticks),cex.axis = 0.8)\n  axis(2, at = y_ticks, labels = paste0(y_ticks), las = 1,cex.axis = 0.8)\n  box()\n  abline(0, 1, col = \"red\", lwd = 2)\n  dev.off()\n}\n\n\n\n============================\nModel 1 \nRMSE (USD): 223781.1 \nNormalized RMSE: 0.7696 \n============================\n\n\n\n\n\n\n\n\n\n\n============================\nModel 2 \nRMSE (USD): 178923.4 \nNormalized RMSE: 0.6153 \n============================\n\n\n\n\n\n\n\n\n\n\n============================\nModel 3 \nRMSE (USD): 136874.5 \nNormalized RMSE: 0.4707 \n============================\n\n\n\n\n\n\n\n\n\n\n============================\nModel 4 \nRMSE (USD): 124511.4 \nNormalized RMSE: 0.4282 \n============================\n\n\n\n\n\n\n\n\n\n\n5.1.2 Report and Compare RMSE, MAE, R¬≤\n\n\n\nCode\n#Since data have different weights, we need to define a new 10-fold cross-validation model.\n\nset.seed(123)   \nk &lt;- 10\n\n# Shuffle dataset rows\nopa_census_all &lt;- opa_census_all %&gt;%\n  slice_sample(prop = 1) %&gt;%  # randomly reorder all rows\n  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs\n\n#Initialize result vectors\nrmse_usd_vec &lt;- numeric(k)\nr2_log_vec &lt;- numeric(k)\nrmse_log_vec &lt;- numeric(k)\nmae_usd_vec &lt;- numeric(k)\n\n#Perform k-fold cross-validation\nfor (i in 1:k) {\n  \n  # Split training / validation sets\n  train &lt;- opa_census_all %&gt;% filter(fold_id != i)\n  test_raw &lt;- opa_census_all %&gt;% filter(fold_id == i)\n  \n  # ‚úÖ Very important! Ensure the 1/10 test data only include real market transactions\n\n  test &lt;- test_raw %&gt;% filter(non_market != 1)  \n  \n  if (nrow(test) &lt; 10) {\n    cat(\"Skipping fold\", i, \": too few validation samples after cleaning.\\n\")\n    next\n  }\n  \n  # Weighted linear regression\n  model_i &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  + \n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing, \n    data = train,\n    weights = train$weight_mix\n  )\n  \n  # Predict in log scale\n  test$pred_log &lt;- predict(model_i, newdata = test)\n  \n  # Compute R¬≤ \n  actual_log &lt;- log(test$sale_price_predicted)\n  ss_res &lt;- sum((actual_log - test$pred_log)^2, na.rm = TRUE)\n  ss_tot &lt;- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)\n  r2_log_vec[i] &lt;- 1 - ss_res / ss_tot\n  \n  # RMSE (log)\n  rmse_log_vec[i] &lt;- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))\n  \n  # Compute RMSE in USD (Duan smearing correction)\n  smearing_factor &lt;- mean(exp(residuals(model_i)), na.rm = TRUE)\n  test$pred_usd &lt;- exp(test$pred_log) * smearing_factor\n  rmse_usd_vec[i] &lt;- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))\n  \n  mae_usd_vec[i] &lt;- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)\n}\n\n\n\n\n\n====================================\n\n\nMODEL_1\n\n\nüíµ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n\n\nAverage RMSE (log): 0.5497 \n\n\nAverage RMSE (USD): 221011.4 \n\n\nRMSE Std. Dev (USD): 44239.67 \n\n\nAverage R¬≤: 0.5236 \n\n\nAverage MAE (USD): 109376.6 \n\n\n====================================\n\n\n   Fold RMSE RMSE_USD     R2  MAE_USD\n1     1 0.54 177098.9 0.5276 103944.6\n2     2 0.54 231058.2 0.5441 111621.1\n3     3 0.56 213497.9 0.5008 110701.4\n4     4 0.54 278760.5 0.5430 111846.0\n5     5 0.56 206552.1 0.5222 110424.2\n6     6 0.57 175373.5 0.5164 107466.6\n7     7 0.54 215275.2 0.5437 108946.3\n8     8 0.54 247041.8 0.5249 111203.1\n9     9 0.54 299499.2 0.5218 113716.5\n10   10 0.57 165957.1 0.4912 103896.0\n\n\n\n\nCode\n#Since data have different weights, we need to define a new 10-fold cross-validation model.\n\nset.seed(234)   \nk &lt;- 10\n\n# Shuffle dataset rows\nopa_census_all &lt;- opa_census_all %&gt;%\n  slice_sample(prop = 1) %&gt;%  # randomly reorder all rows\n  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs\n\n#Initialize result vectors\nrmse_usd_vec &lt;- numeric(k)\nr2_log_vec &lt;- numeric(k)\nrmse_log_vec &lt;- numeric(k)\nmae_usd_vec &lt;- numeric(k)\n\n#Perform k-fold cross-validation\nfor (i in 1:k) {\n  \n  # Split training / validation sets\n  train &lt;- opa_census_all %&gt;% filter(fold_id != i)\n  test_raw &lt;- opa_census_all %&gt;% filter(fold_id == i)\n  \n  # ‚úÖ Very important! Ensure the 1/10 test data only include real market transactions\n\n  test &lt;- test_raw %&gt;% filter(non_market != 1)  \n  \n  if (nrow(test) &lt; 10) {\n    cat(\"Skipping fold\", i, \": too few validation samples after cleaning.\\n\")\n    next\n  }\n  \n  # Weighted linear regression\n  model_i &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  + \n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                  \n                  income_scaled +             \n                  ba_rate +\n                  unemployment_rate,\n    data = train,\n    weights = train$weight_mix\n  )\n  \n  # Predict in log scale\n  test$pred_log &lt;- predict(model_i, newdata = test)\n  \n  # Compute R¬≤ \n  actual_log &lt;- log(test$sale_price_predicted)\n  ss_res &lt;- sum((actual_log - test$pred_log)^2, na.rm = TRUE)\n  ss_tot &lt;- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)\n  r2_log_vec[i] &lt;- 1 - ss_res / ss_tot\n  \n  # RMSE (log)\n  rmse_log_vec[i] &lt;- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))\n  \n  # Compute RMSE in USD (Duan smearing correction)\n  smearing_factor &lt;- mean(exp(residuals(model_i)), na.rm = TRUE)\n  test$pred_usd &lt;- exp(test$pred_log) * smearing_factor\n  rmse_usd_vec[i] &lt;- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))\n  \n  mae_usd_vec[i] &lt;- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)\n}\n\n\n\n\n\n====================================\n\n\nMODEL_2\n\n\nüíµ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n\n\nAverage RMSE (log): 0.4517 \n\n\nAverage RMSE (USD): 178032.4 \n\n\nRMSE Std. Dev (USD): 22996.36 \n\n\nAverage R¬≤: 0.6779 \n\n\nAverage MAE (USD): 88273.9 \n\n\n====================================\n\n\n   Fold RMSE RMSE_USD     R2  MAE_USD\n1     1 0.44 175948.9 0.6971 91146.41\n2     2 0.45 186551.3 0.6724 87506.03\n3     3 0.46 167991.3 0.6794 88387.24\n4     4 0.46 154476.2 0.6711 85382.81\n5     5 0.44 173918.7 0.6781 85451.28\n6     6 0.45 204933.4 0.6848 92647.75\n7     7 0.45 196285.1 0.6799 95280.97\n8     8 0.43 142827.1 0.6712 80911.13\n9     9 0.47 161435.2 0.6588 84010.81\n10   10 0.46 215956.6 0.6863 92014.57\n\n\n\n\nCode\n#Since data have different weights, we need to define a new 10-fold cross-validation model.\n\nset.seed(345)   \nk &lt;- 10\n\n# Shuffle dataset rows\nopa_census_all &lt;- opa_census_all %&gt;%\n  slice_sample(prop = 1) %&gt;%  # randomly reorder all rows\n  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs\n\n#Initialize result vectors\nrmse_usd_vec &lt;- numeric(k)\nr2_log_vec &lt;- numeric(k)\nrmse_log_vec &lt;- numeric(k)\nmae_usd_vec &lt;- numeric(k)\n\n#Perform k-fold cross-validation\nfor (i in 1:k) {\n  \n  # Split training / validation sets\n  train &lt;- opa_census_all %&gt;% filter(fold_id != i)\n  test_raw &lt;- opa_census_all %&gt;% filter(fold_id == i)\n  \n  # ‚úÖ Very important! Ensure the 1/10 test data only include real market transactions\n\n  test &lt;- test_raw %&gt;% filter(non_market != 1)  \n  \n  if (nrow(test) &lt; 10) {\n    cat(\"Skipping fold\", i, \": too few validation samples after cleaning.\\n\")\n    next\n  }\n  \n  # Weighted linear regression\n  model_i &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  + \n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                  \n                  income_scaled +             \n                  ba_rate +\n                  unemployment_rate+\n                  \n                  transit_count+\n                  avg_past_price_density+ \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3),\n    data = train,\n    weights = train$weight_mix\n  )\n  \n  # Predict in log scale\n  test$pred_log &lt;- predict(model_i, newdata = test)\n  \n  # Compute R¬≤ \n  actual_log &lt;- log(test$sale_price_predicted)\n  ss_res &lt;- sum((actual_log - test$pred_log)^2, na.rm = TRUE)\n  ss_tot &lt;- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)\n  r2_log_vec[i] &lt;- 1 - ss_res / ss_tot\n  \n  # RMSE (log)\n  rmse_log_vec[i] &lt;- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))\n  \n  # Compute RMSE in USD (Duan smearing correction)\n  smearing_factor &lt;- mean(exp(residuals(model_i)), na.rm = TRUE)\n  test$pred_usd &lt;- exp(test$pred_log) * smearing_factor\n  rmse_usd_vec[i] &lt;- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))\n  \n  mae_usd_vec[i] &lt;- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)\n}\n\n\n\n\n\n====================================\n\n\nMODEL_3\n\n\nüíµ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n\n\nAverage RMSE (log): 0.3987 \n\n\nAverage RMSE (USD): 136597.9 \n\n\nRMSE Std. Dev (USD): 13503.92 \n\n\nAverage R¬≤: 0.7502 \n\n\nAverage MAE (USD): 68985.46 \n\n\n====================================\n\n\n   Fold RMSE RMSE_USD     R2  MAE_USD\n1     1 0.39 129277.7 0.7376 67230.91\n2     2 0.39 120960.8 0.7575 68329.57\n3     3 0.39 136325.4 0.7564 68010.22\n4     4 0.42 139659.6 0.7344 70052.96\n5     5 0.38 129831.5 0.7641 66646.98\n6     6 0.40 147847.2 0.7414 69674.19\n7     7 0.40 137101.1 0.7536 69573.63\n8     8 0.39 142079.7 0.7625 70609.14\n9     9 0.41 164728.4 0.7354 71700.95\n10   10 0.40 118167.1 0.7593 68026.06\n\n\n\n\nCode\n#Since data have different weights, we need to define a new 10-fold cross-validation model.\n\nset.seed(456)   \nk &lt;- 10\n\n# Shuffle dataset rows\nopa_census_all &lt;- opa_census_all %&gt;%\n  slice_sample(prop = 1) %&gt;%  # randomly reorder all rows\n  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs\n\n#Initialize result vectors\nrmse_usd_vec &lt;- numeric(k)\nr2_log_vec &lt;- numeric(k)\nrmse_log_vec &lt;- numeric(k)\nmae_usd_vec &lt;- numeric(k)\n\n#Perform k-fold cross-validation\nfor (i in 1:k) {\n  \n  # Split training / validation sets\n  train &lt;- opa_census_all %&gt;% filter(fold_id != i)\n  test_raw &lt;- opa_census_all %&gt;% filter(fold_id == i)\n  \n  # ‚úÖ Very important! Ensure the 1/10 test data only include real market transactions\n\n  test &lt;- test_raw %&gt;% filter(non_market != 1)  \n  \n  if (nrow(test) &lt; 10) {\n    cat(\"Skipping fold\", i, \": too few validation samples after cleaning.\\n\")\n    next\n  }\n  \n  # Weighted linear regression\n  model_i &lt;- lm(log(sale_price_predicted) ~ \n                  log(total_livable_area)  + \n                  number_of_bathrooms + \n                  house_age_c +\n                  house_age_c2 +\n                  interior_condition +\n                  quality_grade_num + \n                  fireplaces +\n                  garage_spaces +\n                  central_air_dummy + \n                  central_air_missing+\n                  \n                  income_scaled +             \n                  ba_rate +\n                  unemployment_rate+\n                  \n                  transit_count+\n                  avg_past_price_density+ \n                  sqrt(crime_count) +\n                  log(nearest_hospital_knn3)+\n                  \n                  (interior_condition * income_scaled)+\n                  factor(zip_code),\n    data = train,\n    weights = train$weight_mix\n  )\n  \n  # Predict in log scale\n  test$pred_log &lt;- predict(model_i, newdata = test)\n  \n  # Compute R¬≤ \n  actual_log &lt;- log(test$sale_price_predicted)\n  ss_res &lt;- sum((actual_log - test$pred_log)^2, na.rm = TRUE)\n  ss_tot &lt;- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)\n  r2_log_vec[i] &lt;- 1 - ss_res / ss_tot\n  # RMSE (log)\n  rmse_log_vec[i] &lt;- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))\n  \n  # Compute RMSE in USD (Duan smearing correction)\n  smearing_factor &lt;- mean(exp(residuals(model_i)), na.rm = TRUE)\n  test$pred_usd &lt;- exp(test$pred_log) * smearing_factor\n  rmse_usd_vec[i] &lt;- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))\n  \n  mae_usd_vec[i] &lt;- mean(abs(test$sale_price_predicted - test$pred_usd), na.rm = TRUE)\n}\n\n\n\n\n\n====================================\n\n\nMODEL_4\n\n\nüíµ Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n\n\nAverage RMSE (log): 0.3864 \n\n\nAverage RMSE (USD): 124437.8 \n\n\nRMSE Std. Dev (USD): 13635.38 \n\n\nAverage R¬≤: 0.7654 \n\n\nAverage MAE (USD): 63941.9 \n\n\n====================================\n\n\n   Fold RMSE  RMSE_USD     R2  MAE_USD\n1     1 0.37  90244.89 0.7756 58403.27\n2     2 0.38 121731.32 0.7667 62732.30\n3     3 0.40 126818.56 0.7501 63681.22\n4     4 0.40 128966.47 0.7510 64401.43\n5     5 0.39 115300.73 0.7561 63659.17\n6     6 0.39 133936.40 0.7566 68322.77\n7     7 0.39 130009.08 0.7684 60725.06\n8     8 0.38 126520.18 0.7753 66694.08\n9     9 0.38 138597.91 0.7715 63561.74\n10   10 0.38 132252.04 0.7824 67237.96\n\n\nDiscussion of the most mattered features:\n\nLivable Area ‚Äì The Fundamental Structural Driver:\n\nAmong all traditional structural housing characteristics, the total livable area demonstrates the strongest and most consistent positive relationship with sale price. This finding robustly confirms that the physical scale of a property remains a primary determinant of its market value. A larger livable space directly provides more utility and functionality, which homebuyers are consistently willing to pay a premium for. Its strength as a predictor underscores that, despite the importance of location and market trends, the core structure and size of a house still fundamentally matter.\n\nComparable Sales ‚Äì Capturing Appraiser Logic and Micro-Market Dynamics:\n\nThe historical sale prices of nearby properties (modeled as average past price density) serve as a powerful predictive feature. This variable effectively allows the model to ‚Äúthink like a real appraiser‚Äù by incorporating the most relevant and recent market transactions as references. It captures hyperlocal, block-by-block market momentum and the value influence that comparable homes exert on a subject property. By leveraging this spatial and temporal proximity, the feature helps to correct for the inherent time lag in other census-based data and anchors the price prediction in real-time, micro-level market activity.\n\nZip Code Fixed Effects ‚Äì The Ultimate Location Premium:\n\nThe inclusion of Zip Code Fixed Effects proves to be the most consequential ‚Äúgame-changer‚Äù in the model. While other variables capture specific, quantifiable aspects of a property, this feature holistically encapsulates the unobserved, intangible value associated with a specific location. It quantifies the true ‚Äúlocation value‚Äù premium, capturing a wide array of factors that are difficult to measure directly but are capitalized into housing prices, such as school district quality, neighborhood prestige, long-term desirability, and overall ‚Äúmarket mood.‚Äù Its dominant role confirms that in the Philadelphia housing market, the answer to ‚ÄúWhere is it?‚Äù often carries more weight than ‚ÄúWhat is it?‚Äù, as the zip code effectively bundles the net effect of all locational advantages and community characteristics."
  },
  {
    "objectID": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-6-model-diagnostics",
    "href": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-6-model-diagnostics",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 6: Model Diagnostics",
    "text": "Phase 6: Model Diagnostics\n\nCheck assumptions for best model:\n6.1 Residual plot:\n\n\nCode\nmodel_data &lt;- data.frame(\n  Fitted = fitted(model_4),\n  Residuals = resid(model_4)\n)\n\np_resid_fitted &lt;- ggplot(model_data, aes(x = Fitted, y = Residuals)) +\n  geom_point(alpha = 0.5, color = \"#6A1B9A\", size = 2) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", color = \"black\", se = FALSE, linewidth = 0.8) +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    subtitle = \"Checking linearity and homoscedasticity for Model 4\",\n    x = \"Fitted Values (Log(Sale Price))\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_resid_fitted\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nresid_full &lt;- rep(NA, nrow(opa_census_all))\nresid_full[-as.numeric(model_4$na.action)] &lt;- resid(model_4)\n\nopa_census_all$residuals &lt;- resid_full\n\ntract_resid &lt;- opa_census_all %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(mean_residual = mean(residuals, na.rm = TRUE))\n\ntract_map &lt;- philly_census %&gt;%\n  left_join(tract_resid, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = tract_map, aes(fill = mean_residual), color = \"white\", size = 0.2) +\n  scale_fill_gradient2(\n    low = \"#6A1B9A\", mid = \"white\", high = \"#FFB300\",\n    midpoint = 0,\n    limits = c(-0.5, 0.5),\n    name = \"Mean Log Residual\",\n    breaks = c(-0.3, 0, 0.3),\n    labels = c(\"Overestimated\", \"Accurate\", \"Underestimated\"),\n    na.value = \"grey60\"\n  ) +\n  theme_minimal(base_size = 16) +\n  labs(\n    title = \"Hardest to Predict Neighborhoods in Philadelphia\",\n    subtitle = \"Yellow = underestimation | Purple = overestimation\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nCentral Tendency of Residuals: The residuals are generally centered around the zero line, showing no systematic deviation. This indicates that the model captures the overall linear relationship between predictors and the response variable effectively.\nHomoscedasticity: The residuals show greater variability and dispersion in the lower fitted value range (approximately 10‚Äì12), while they appear more stable at higher fitted values. This suggests that the model performs less effectively for observations with lower predicted values.\nModel Assumption Assessment: Overall, the assumptions of linearity and homoscedasticity are largely satisfied, indicating a sound model fit, with only slight deviations to monitor at higher fitted values.\n\n6.2 Q-Q plot:\n\n\nCode\np_qq &lt;- ggplot(model_data, aes(sample = Residuals)) +\n  stat_qq(color = \"#6A1B9A\", size = 2, alpha = 0.6) +\n  stat_qq_line(color = \"red\",linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Normal Q-Q Plot\",\n    subtitle = \"Checking normality of residuals for Model 4\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_qq\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nOverall Shape of the Plot: The residual points generally follow the diagonal line, indicating that the overall distribution of residuals is roughly consistent with a normal distribution.\nGood Fit in the Central Range: In the central range (around -1 to 1 quantiles), the sample quantiles align closely with the theoretical quantiles, suggesting that most residuals conform well to the assumption of normality.\nDeviation in the Tails: At both tails‚Äîespecially the upper quantiles‚Äîthe points deviate noticeably from the red dashed line, indicating that the residuals have heavier tails than a normal distribution, suggesting slight non-normality.\nAssessment of Normality Assumption: Although deviations appear in the tails, the overall alignment with the reference line is strong, indicating that the normality assumption largely holds, with only minor deviations for extreme residuals.\n\n6.3 Cook‚Äôs distance:\n\n\nCode\ncooks_d &lt;- cooks.distance(model_4)\nmodel_data &lt;- data.frame(\n  Index = 1:length(cooks_d),\n  CooksD = cooks_d\n)\nthreshold &lt;- 4 / nrow(model_4$model)\n\np_cook &lt;- ggplot(model_data, aes(x = Index, y = CooksD)) +\n  geom_segment(aes(xend = Index, yend = 0), color = \"#6A1B9A\", alpha = 0.7) +  # vertical lines\n  geom_point(color = \"#6A1B9A\", size = 0.15) +\n  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Cook's Distance\",\n    subtitle = \"Identifying influential observations for Model 4\",\n    x = \"Observation Index\",\n    y = \"Cook's Distance\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 13, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\np_cook\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nOverall Distribution Pattern: Most observations have Cook‚Äôs Distance values close to zero, indicating that the dataset‚Äôs overall influence on the model is balanced, with no widespread undue impact.\nPresence of Influential Points: A few vertical spikes rise noticeably above the rest, indicating the presence of some influential observations that may affect the estimated model coefficients.\nModel Robustness Conclusion: Overall, the model appears robust, with no single observation exerting excessive influence."
  },
  {
    "objectID": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-7-conclusions-recommendations",
    "href": "midterm/Appendix/CuiXinyuan_Appendix.html#phase-7-conclusions-recommendations",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Phase 7: Conclusions & Recommendations",
    "text": "Phase 7: Conclusions & Recommendations\n\nConclusion:\n\nOur final model‚Äôs R¬≤ is 0.84, indicating that the model explains 84% of the variance in log sale prices. The RMSE is 124437.8USD, showing that the model‚Äôs predictions are reasonably close to the observed values.\n\nLivable Area of the House matters most for Philadelphia prices.\n\n\n\nRecommendations:\n\nEquity concerns\n\nWhich neighborhoods are hardest to predict?\n\nThe largest prediction errors occur primarily in central Philadelphia, where both overestimation and underestimation coexist within close proximity. This pattern indicates that the model struggles most in areas with high housing heterogeneity- neighborhoods that contain a mix of old row houses, newly renovated apartments, and varying property types within short distances.\n\nIn contrast, outer neighborhoods such as those in the northwest and northeast tend to have more consistent housing characteristics, leading to smaller residuals. The central tracts‚Äô larger residuals suggest that cultural or historical features have introduced variability that the model‚Äôs current features (mainly physical characteristics) cannot fully explain.\n\nAny data bias?\n\nThe observed spatial pattern of prediction errors reflects inherent data bias. The dataset likely overrepresents mid-range housing conditions and underrepresents both luxury and low-income housing. As shown in the livable area and interior condition maps, data coverage in wealthier areas (especially the northwest) is limited, and these tracts often contain more unique, high-value properties that the model cannot generalize well.\n\nWealthier tracts are more likely to be well-documented, while poorer areas lack records, creating spatial imbalance in model performance.\n\nHigh-priced homes typically have larger negotiation margins, meaning their final sale prices are often lower than the listed prices. In contrast, low-priced homes sell closer to their listing prices. As a result, model tends to overestimate expensive homes and underestimate affordable ones, introducing systematic bias.\n\n\nRecommendations to government\n\nImmediate System Calibration: We recommend utilizing our model‚Äôs findings to immediately adjust property assessments in systematically overvalued low-income communities. This action will ensure a fair distribution of the tax burden and address current inequities.\n\n\n\nIntegrate Advanced Spatial Features: We advise the Office of Property Assessment (OPA) to permanently integrate the effective spatial characteristics identified by our study‚Äîspecifically Comparable Sales Proxies (surrounding transaction prices) and Neighborhood Fixed Effects‚Äîinto the next-generation AVM. This will significantly enhance the model‚Äôs responsiveness to rapidly changing market dynamics.\n\n\n\nExtreme high values almost exclusively stem from corporate transactions and require manual review for outliers.\n\nLimitations and next steps\n\nLimitations: Inherent Data Biases\n\nOur predictive accuracy is constrained by inherent data biases which affect equity. We find a dual challenge in data coverage: in affluent areas, high-value, unique properties suffer from data sparsity, making generalization difficult. Conversely, lower-income areas often show data incompleteness, leading to less reliable predictions and higher residual errors. Critically, we observed a price-tier bias: high-priced homes tend to sell below their list price, while low-priced homes transact closer to it. This systemic pattern means the model is prone to over-assessing expensive properties and under-assessing affordable ones, creating a structural risk for vertical inequity in the tax system.\n\n\nNext Steps: Enhancing Data Quality and Fairness\n\nTo address these limitations, our next steps focus on data enrichment and equitable optimization. The City should partner with us to integrate non-public data, such as detailed appraisal records and permit data, which can account for unobserved renovation quality and close the data gap. Furthermore, to combat the systematic price-tier bias, we recommend integrating fairness metrics directly into the AVM‚Äôs optimization process. This will shift the model‚Äôs objective beyond simple average accuracy (RMSE) to ensure that prediction errors are uniformly low across all price tiers and all Philadelphia neighborhoods, securing both accuracy and equity."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9 Notes - Critical Perspectives on Predictive Policing",
    "section": "",
    "text": "Part 1: Where We Are\n\nReview\n\nWeeks 1-3: Data foundations\n\nCensus data, tidycensus, spatial data basics\nVisualization and exploratory analysis\n\nWeek 5: Linear regression fundamentals\n\nY = f(X) + Œµ framework\nTrain/test splits, cross-validation\nChecking assumptions\n\nWeek 6: Expanding the toolkit\n\nCategorical variables and interactions\nSpatial features (buffers, kNN, distance)\nNeighborhood fixed effects\n\n\nThe Regression Workflow\n\nBuilding the model:\n\nVisualize relationships\nEngineer features\nFit the model\nEvaluate performance (RMSE, R¬≤)\nCheck assumptions\n\nSpatial diagnostics:\n\nAre errors random or clustered?\nDo we predict better in some areas?\nIs there remaining spatial structure?\n\nIf errors cluster spatially, it suggests:\n\nMissing spatial variables\nMisspecified relationships\nNon-stationarity (relationships vary across space) ÈùûÂπ≥Á®≥ÊÄß\n\n\n\nPart 2: Understanding Spatial Patterns in Errors\n\nVisualizing Error PatternsÔºàËØØÂ∑ÆÊúâËÅöÈõÜÔºå‰∏çÈöèÊú∫Ôºå‰∏çÂ•ΩÔºâ‚ÜíÊÄé‰πàÊîπËøõÔºöfixed\n\nPart 3: Moran‚Äôs I\n\nMoran‚Äôs I measures spatial autocorrelation\nRange: -1(Perfect negative correlation (dispersion)) to +1(Perfect positive correlation (clustering)),0 = Random spatial pattern\nwij = spatial weight between locations i and j (0 or 1)"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-09-notes.html#key-concepts-learned",
    "title": "Week 9 Notes - Critical Perspectives on Predictive Policing",
    "section": "",
    "text": "Part 1: Where We Are\n\nReview\n\nWeeks 1-3: Data foundations\n\nCensus data, tidycensus, spatial data basics\nVisualization and exploratory analysis\n\nWeek 5: Linear regression fundamentals\n\nY = f(X) + Œµ framework\nTrain/test splits, cross-validation\nChecking assumptions\n\nWeek 6: Expanding the toolkit\n\nCategorical variables and interactions\nSpatial features (buffers, kNN, distance)\nNeighborhood fixed effects\n\n\nThe Regression Workflow\n\nBuilding the model:\n\nVisualize relationships\nEngineer features\nFit the model\nEvaluate performance (RMSE, R¬≤)\nCheck assumptions\n\nSpatial diagnostics:\n\nAre errors random or clustered?\nDo we predict better in some areas?\nIs there remaining spatial structure?\n\nIf errors cluster spatially, it suggests:\n\nMissing spatial variables\nMisspecified relationships\nNon-stationarity (relationships vary across space) ÈùûÂπ≥Á®≥ÊÄß\n\n\n\nPart 2: Understanding Spatial Patterns in Errors\n\nVisualizing Error PatternsÔºàËØØÂ∑ÆÊúâËÅöÈõÜÔºå‰∏çÈöèÊú∫Ôºå‰∏çÂ•ΩÔºâ‚ÜíÊÄé‰πàÊîπËøõÔºöfixed\n\nPart 3: Moran‚Äôs I\n\nMoran‚Äôs I measures spatial autocorrelation\nRange: -1(Perfect negative correlation (dispersion)) to +1(Perfect positive correlation (clustering)),0 = Random spatial pattern\nwij = spatial weight between locations i and j (0 or 1)"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#coding-techniques",
    "href": "weekly-notes/week-09-notes.html#coding-techniques",
    "title": "Week 9 Notes - Critical Perspectives on Predictive Policing",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#questions-challenges",
    "href": "weekly-notes/week-09-notes.html#questions-challenges",
    "title": "Week 9 Notes - Critical Perspectives on Predictive Policing",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhat I didn‚Äôt fully understand\n\nthe basic workflow: pull-commit-push\n\nAreas needing more practice\n\nremember the essential dplyr functions"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#connections-to-policy",
    "href": "weekly-notes/week-09-notes.html#connections-to-policy",
    "title": "Week 9 Notes - Critical Perspectives on Predictive Policing",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week‚Äôs content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#reflection",
    "href": "weekly-notes/week-09-notes.html#reflection",
    "title": "Week 9 Notes - Critical Perspectives on Predictive Policing",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I‚Äôll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Part 1: Where We Are\n\nReview\n\nWeeks 1-3: Data foundations\n\nCensus data, tidycensus, spatial data basics\nVisualization and exploratory analysis\n\nWeek 5: Linear regression fundamentals\n\nY = f(X) + Œµ framework\nTrain/test splits, cross-validation\nChecking assumptions\n\nWeek 6: Expanding the toolkit\n\nCategorical variables and interactions\nSpatial features (buffers, kNN, distance)\nNeighborhood fixed effects\n\n\nThe Regression Workflow\n\nBuilding the model:\n\nVisualize relationships\nEngineer features\nFit the model\nEvaluate performance (RMSE, R¬≤)\nCheck assumptions\n\nSpatial diagnostics:\n\nAre errors random or clustered?\nDo we predict better in some areas?\nIs there remaining spatial structure?\n\nIf errors cluster spatially, it suggests:\n\nMissing spatial variables\nMisspecified relationships\nNon-stationarity (relationships vary across space) ÈùûÂπ≥Á®≥ÊÄß\n\n\n\nPart 2: Understanding Spatial Patterns in Errors\n\nVisualizing Error PatternsÔºàËØØÂ∑ÆÊúâËÅöÈõÜÔºå‰∏çÈöèÊú∫Ôºå‰∏çÂ•ΩÔºâ‚ÜíÊÄé‰πàÊîπËøõÔºöfixed\n\nPart 3: Moran‚Äôs I\n\nMoran‚Äôs I measures spatial autocorrelation\nRange: -1(Perfect negative correlation (dispersion)) to +1(Perfect positive correlation (clustering)),0 = Random spatial pattern\nwij = spatial weight between locations i and j (0 or 1)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Part 1: Where We Are\n\nReview\n\nWeeks 1-3: Data foundations\n\nCensus data, tidycensus, spatial data basics\nVisualization and exploratory analysis\n\nWeek 5: Linear regression fundamentals\n\nY = f(X) + Œµ framework\nTrain/test splits, cross-validation\nChecking assumptions\n\nWeek 6: Expanding the toolkit\n\nCategorical variables and interactions\nSpatial features (buffers, kNN, distance)\nNeighborhood fixed effects\n\n\nThe Regression Workflow\n\nBuilding the model:\n\nVisualize relationships\nEngineer features\nFit the model\nEvaluate performance (RMSE, R¬≤)\nCheck assumptions\n\nSpatial diagnostics:\n\nAre errors random or clustered?\nDo we predict better in some areas?\nIs there remaining spatial structure?\n\nIf errors cluster spatially, it suggests:\n\nMissing spatial variables\nMisspecified relationships\nNon-stationarity (relationships vary across space) ÈùûÂπ≥Á®≥ÊÄß\n\n\n\nPart 2: Understanding Spatial Patterns in Errors\n\nVisualizing Error PatternsÔºàËØØÂ∑ÆÊúâËÅöÈõÜÔºå‰∏çÈöèÊú∫Ôºå‰∏çÂ•ΩÔºâ‚ÜíÊÄé‰πàÊîπËøõÔºöfixed\n\nPart 3: Moran‚Äôs I\n\nMoran‚Äôs I measures spatial autocorrelation\nRange: -1(Perfect negative correlation (dispersion)) to +1(Perfect positive correlation (clustering)),0 = Random spatial pattern\nwij = spatial weight between locations i and j (0 or 1)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#coding-techniques",
    "href": "weekly-notes/week-07-notes.html#coding-techniques",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#questions-challenges",
    "href": "weekly-notes/week-07-notes.html#questions-challenges",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhat I didn‚Äôt fully understand\n\nthe basic workflow: pull-commit-push\n\nAreas needing more practice\n\nremember the essential dplyr functions"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#connections-to-policy",
    "href": "weekly-notes/week-07-notes.html#connections-to-policy",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week‚Äôs content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#reflection",
    "href": "weekly-notes/week-07-notes.html#reflection",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I‚Äôll apply this knowledge]"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#quick-clarification",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#quick-clarification",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Quick Clarification",
    "text": "Quick Clarification\nSome midterm teams suggested:\n\n‚ÄúTo improve predictions, implement a spatial lag model (SAR) or spatial error model (SEM)‚Äù\n\nLet‚Äôs revisit why this doesn‚Äôt work for prediction\n\n(And what you should recommend instead)"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#first-what-is-a-spatial-lag-model",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#first-what-is-a-spatial-lag-model",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "First: What IS a Spatial Lag Model?",
    "text": "First: What IS a Spatial Lag Model?\nStandard regression: \\[\\text{Price}_i = \\beta_0 + \\beta_1(\\text{Sqft}) + \\beta_2(\\text{Beds}) + \\varepsilon\\]\nSpatial lag regression: \\[\\text{Price}_i = \\beta_0 + \\rho \\times \\color{red}{\\text{Avg(Neighbor Prices)}} + \\beta_1(\\text{Sqft}) + \\beta_2(\\text{Beds}) + \\varepsilon\\]\n\nKey difference: Your price depends on your neighbors‚Äô actual prices\nQuestion: ‚ÄúDo nearby house prices affect each other?‚Äù (spillover effects)\nUsed for: Understanding spatial processes, causal inference about neighborhood effects"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#the-problem-for-prediction",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#the-problem-for-prediction",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "The Problem for Prediction",
    "text": "The Problem for Prediction\nLet‚Äôs work through two concrete scenarios where this breaks down:\n\nTemporal: Training on 2024, predicting 2025\nTransfer: Philadelphia model ‚Üí Orlando"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#step-1-estimate-model-on-2024-data",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#step-1-estimate-model-on-2024-data",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Step 1: Estimate Model on 2024 Data",
    "text": "Step 1: Estimate Model on 2024 Data\nYou fit this spatial lag model on 2024 Philadelphia sales:\n# Your estimated model from 2024\nmodel_2024 &lt;- spatialreg::lagsarlm(\n  log(price) ~ sqft + bedrooms + bathrooms,\n  data = sales_2024,\n  listw = neighbors_weights\n)\nResults:\nSpatial lag coefficient (œÅ) = 0.65\nŒ≤_sqft = 0.00015\nŒ≤_beds = 0.12\n\nInterpretation: A 1% increase in neighbors‚Äô prices ‚Üí 0.65% increase in my price\nThis works for 2024 because all prices are known!"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#step-2-three-houses-list-in-january-2025",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#step-2-three-houses-list-in-january-2025",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Step 2: Three Houses List in January 2025",
    "text": "Step 2: Three Houses List in January 2025\nYour prediction task:\n\n\n\nHouse\nSqft\nBeds\nBaths\n5 Nearest Neighbors\n\n\n\n\nA\n1,500\n3\n2\nB, C, D, E, F\n\n\nB\n1,800\n3\n2\nA, C, G, H, I\n\n\nC\n2,000\n4\n3\nA, B, J, K, L\n\n\n\n\nWhat you know:\n\n‚úì Sqft, beds, baths for A, B, C\n‚úì Locations of A, B, C\n‚úó What A, B, C will actually sell for"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#step-3-try-to-predict-house-a",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#step-3-try-to-predict-house-a",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Step 3: Try to Predict House A",
    "text": "Step 3: Try to Predict House A\nYour model equation: \\[\\text{Price}_A = \\beta_0 + 0.65 \\times \\color{red}{\\text{Avg}(\\text{Price}_B, \\text{Price}_C, ...)} + 0.00015 \\times 1500 + 0.12 \\times 3\\]\n\nProblem: You need PriceB and PriceC to predict PriceA\n\n\nBut wait‚Ä¶ PriceB and PriceC haven‚Äôt happened yet! They‚Äôre listed, not sold."
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#step-4-realize-the-circular-dependency",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#step-4-realize-the-circular-dependency",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Step 4: Realize the Circular Dependency",
    "text": "Step 4: Realize the Circular Dependency\nTry to predict House B: \\[\\text{Price}_B = \\beta_0 + 0.65 \\times \\color{red}{\\text{Avg}(\\text{Price}_A, \\text{Price}_C, ...)} + ...\\]\nTry to predict House C: \\[\\text{Price}_C = \\beta_0 + 0.65 \\times \\color{red}{\\text{Avg}(\\text{Price}_A, \\text{Price}_B, ...)} + ...\\]\n\nPrice_A needs Price_B and Price_C\nPrice_B needs Price_A and Price_C  \nPrice_C needs Price_A and Price_B\nYou‚Äôre stuck in a circular dependency!"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#visual-the-circular-dependency",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#visual-the-circular-dependency",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Visual: The Circular Dependency",
    "text": "Visual: The Circular Dependency\n\n\n\n\n\ngraph TD\n    A[House A&lt;br/&gt;Need to predict] --&gt;|needs price of| B[House B&lt;br/&gt;Need to predict]\n    B --&gt;|needs price of| C[House C&lt;br/&gt;Need to predict]\n    C --&gt;|needs price of| A\n    \n    style A fill:#e74c3c,stroke:#c0392b,color:#fff\n    style B fill:#e74c3c,stroke:#c0392b,color:#fff\n    style C fill:#e74c3c,stroke:#c0392b,color:#fff\n\n\n\n\n\n\nAll the unknowns depend on each other!"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#but-cant-i-use-recent-sales",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#but-cant-i-use-recent-sales",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "‚ÄúBut Can‚Äôt I Use Recent Sales?‚Äù",
    "text": "‚ÄúBut Can‚Äôt I Use Recent Sales?‚Äù\nYou might think: ‚ÄúI‚Äôll use recent sales from December 2024 as the spatial lag‚Äù\n\nProblem 1: House A‚Äôs neighbors (B, C) haven‚Äôt sold - they‚Äôre ALSO new listings\nProblem 2: If you use OLD sales (from months ago), you‚Äôre predicting based on stale prices in a changing market\nProblem 3: What if it‚Äôs a new development? No recent sales exist nearby\nProblem 4: Your spatial lag coefficient (œÅ = 0.65) was estimated assuming SIMULTANEOUS prices, not lagged prices\n\n\nBottom line: Spatial lag models assume all observations exist simultaneously. Prediction is inherently sequential."
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#your-sales-pitch-to-orlando",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#your-sales-pitch-to-orlando",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Your Sales Pitch to Orlando",
    "text": "Your Sales Pitch to Orlando\nYou: ‚ÄúWe built an amazing spatial lag model for Philadelphia! R¬≤ = 0.85!‚Äù\nOrlando Chief Data Officer: ‚ÄúGreat! We have 5,000 active listings. Can you predict their prices?‚Äù\n\nYou: ‚ÄúSure! Just send me the data‚Ä¶‚Äù\n(You open the file)\norlando_listings &lt;- read_csv(\"orlando_new_listings.csv\")\n# Variables: address, sqft, beds, baths, lat, lon\n# Missing: SALE_PRICE (that's what we're predicting!)\n\n\nYou realize: ‚ÄúWait‚Ä¶ I need neighbors‚Äô prices to predict each price‚Ä¶‚Äù\nOrlando: ‚ÄúThat‚Äôs why we hired you - these HAVEN‚ÄôT sold yet!‚Äù"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#the-parameter-transfer-problem",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#the-parameter-transfer-problem",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "The Parameter Transfer Problem",
    "text": "The Parameter Transfer Problem\nEven if you had some recent Orlando sales to use:\nYour Philadelphia model: \\[\\text{Price}_i = \\beta_0 + \\color{red}{0.65} \\times \\text{Avg}(\\text{Neighbor Prices}) + \\beta_1(\\text{Sqft}) + ...\\]\n\nQuestions:\n\nWas œÅ = 0.65 estimated on Philadelphia‚Äôs price distribution (avg $350k)\nOrlando‚Äôs average is $280k - different scale\nOrlando is sprawling suburbs vs.¬†Philadelphia‚Äôs dense rowhouses\nDoes œÅ = 0.65 even mean the same thing in Orlando?\n\n\n\nAnswer: No! You‚Äôd have to re-estimate the entire model on Orlando data.\nSo your ‚Äúmodel‚Äù isn‚Äôt actually transferable."
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#contrast-spatial-features-transfer",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#contrast-spatial-features-transfer",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Contrast: Spatial Features Transfer",
    "text": "Contrast: Spatial Features Transfer\nIf you had built with spatial FEATURES:\n# Philadelphia model\nmodel &lt;- lm(log(price) ~ sqft + bedrooms + \n              dist_to_transit + parks_500ft + \n              median_income_tract,\n            data = philly)\n\nTo use in Orlando:\n# Get Orlando's spatial context (all observable!)\norlando$dist_to_transit &lt;- get_transit_distance(orlando)\norlando$parks_500ft &lt;- count_parks_buffer(orlando, 500)\norlando$median_income_tract &lt;- get_census_data(orlando)\n\n# Apply Philadelphia coefficients\norlando$predicted_price &lt;- predict(model, newdata = orlando)\nThis works because features are CONTEXT, not outcomes!"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#visual-comparison",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#visual-comparison",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Visual Comparison",
    "text": "Visual Comparison\n\n\nSpatial Lag (Fails)\n\n\n\n\n\ngraph TD\n    A[Predict&lt;br/&gt;House A] --&gt;|needs| B[Price of B&lt;br/&gt;UNKNOWN]\n    A --&gt;|needs| C[Price of C&lt;br/&gt;UNKNOWN]\n    \n    style A fill:#e74c3c\n    style B fill:#e74c3c\n    style C fill:#e74c3c\n\n\n\n\n\n\nCircular dependency\n\nSpatial Features (Works)\n\n\n\n\n\ngraph TD\n    T[Transit: 0.3mi&lt;br/&gt;KNOWN] --&gt; A[Predict&lt;br/&gt;House A]\n    P[Parks: 2&lt;br/&gt;KNOWN] --&gt; A\n    I[Income: 65k&lt;br/&gt;KNOWN] --&gt; A\n    \n    style A fill:#27ae60\n    style T fill:#3498db\n    style P fill:#3498db\n    style I fill:#3498db\n\n\n\n\n\n\nAll inputs observable"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#so-when-are-spatial-lag-models-useful",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#so-when-are-spatial-lag-models-useful",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "So When ARE Spatial Lag Models Useful?",
    "text": "So When ARE Spatial Lag Models Useful?\nSpatial lag models are GREAT for:\n\nUnderstanding spillover effects: ‚ÄúDoes gentrification in one neighborhood cause price increases in adjacent neighborhoods?‚Äù\nCausal inference: ‚ÄúDo nearby foreclosures depress my home value?‚Äù\nPolicy simulation: ‚ÄúIf we build a park here, how will it affect the surrounding area?‚Äù\nCross-sectional analysis: Looking at ONE point in time where all prices exist\n\n\nBut NOT for:\n\nPredicting future sales\nTransferring models between cities\nReal-time valuation systems\nOut-of-sample forecasting"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#what-high-morans-i-actually-tells-you",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#what-high-morans-i-actually-tells-you",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "What High Moran‚Äôs I Actually Tells You",
    "text": "What High Moran‚Äôs I Actually Tells You\nIf your model errors have Moran‚Äôs I = 0.58 (high spatial clustering):\n\n‚ùå Wrong response: ‚ÄúSwitch to spatial lag model‚Äù\n‚úì Right response: ‚ÄúAdd better spatial features!‚Äù"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#instead-of-implement-a-spatial-lag-model",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#instead-of-implement-a-spatial-lag-model",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Instead of:** ‚ÄúImplement a spatial lag model‚Äù",
    "text": "Instead of:** ‚ÄúImplement a spatial lag model‚Äù\nSay this:\n\n‚ÄúThe high Moran‚Äôs I (0.58) indicates spatial clustering in errors, suggesting our model is missing important location-based predictors. Recommendations:\n\nVary buffer distances - Currently using 500ft; try 250ft, 1000ft, 1500ft\nAdd more amenities - Coffee shops, grocery stores, restaurants, crime incidents\nRicher census data - Use block group instead of tract; add commute time variables\nNeighborhood interactions - sqft √ó neighborhood, age √ó distance_downtown\nTime-varying features - Recent building permits, development activity\nMore granular fixed effects - Census block group FE instead of neighborhood FE‚Äù"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#the-key-distinction",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#the-key-distinction",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "The Key Distinction",
    "text": "The Key Distinction\n\n\nSPATIAL FEATURES (What you observe about a location)\n\nDistance to transit\nParks within buffer\nMedian income\nCrime rate\nSchool quality\nWalkability score\n\nFor prediction: ‚úì Always observable\n\nSPATIAL LAG (What neighbors‚Äô outcomes are)\n\nAverage neighbor price\nNeighbor sale date\nNeighbor appreciation rate\n\nFor prediction: ‚úó Creates circular dependency\n\n\nFix spatial autocorrelation by improving FEATURES, not by changing model structure"
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#machine-learning-doesnt-fix-this",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#machine-learning-doesnt-fix-this",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Machine Learning Doesn‚Äôt Fix This",
    "text": "Machine Learning Doesn‚Äôt Fix This\nSome might think: ‚ÄúI‚Äôll use a Random Forest with neighbors‚Äô prices as a feature!‚Äù\n\n# This STILL doesn't work for prediction\nrf_model &lt;- randomForest(\n  price ~ sqft + bedrooms + avg_neighbor_price, # ‚ö†Ô∏è \n  data = train_2024\n)\n\n# Try to predict 2025\npredictions_2025 &lt;- predict(rf_model, newdata = new_listings)\n#                                      ‚Üë\n#                               avg_neighbor_price is MISSING!\n\n\nThe problem isn‚Äôt the model TYPE, it‚Äôs the LOGIC:\nIf a feature requires knowing other predictions first, it‚Äôs not a valid predictor."
  },
  {
    "objectID": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#final-thought",
    "href": "weekly-notes/week-09/lecture/First_spatial_lag_prediction_problem.html#final-thought",
    "title": "Why Spatial Lag Models Don‚Äôt Work for Prediction",
    "section": "Final Thought",
    "text": "Final Thought\nAlways understand what a method is designed for before recommending it."
  },
  {
    "objectID": "labs/lab_4/assignment4.html",
    "href": "labs/lab_4/assignment4.html",
    "title": "Predictive Policing - Technical Implementation",
    "section": "",
    "text": "In this exercise, you will build a spatial predictive model for burglaries using count regression and spatial features."
  },
  {
    "objectID": "labs/lab_4/assignment4.html#about-this-exercise",
    "href": "labs/lab_4/assignment4.html#about-this-exercise",
    "title": "Predictive Policing - Technical Implementation",
    "section": "",
    "text": "In this exercise, you will build a spatial predictive model for burglaries using count regression and spatial features."
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-1.1-load-chicago-spatial-data",
    "href": "labs/lab_4/assignment4.html#exercise-1.1-load-chicago-spatial-data",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 1.1: Load Chicago Spatial Data",
    "text": "Exercise 1.1: Load Chicago Spatial Data\n\n\nCode\n# Load police districts (used for spatial cross-validation)\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 25 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load police beats (smaller administrative units)\npoliceBeats &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(Beat = beat_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 277 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nReading layer `chicagoBoundary' from data source \n  `https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -87.8367 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84\n\n\nCode\ncat(\"‚úì Loaded spatial boundaries\\n\")\n\n\n‚úì Loaded spatial boundaries\n\n\nCode\ncat(\"  - Police districts:\", nrow(policeDistricts), \"\\n\")\n\n\n  - Police districts: 25 \n\n\nCode\ncat(\"  - Police beats:\", nrow(policeBeats), \"\\n\")\n\n\n  - Police beats: 277 \n\n\n\n\n\n\n\n\nNoteCoordinate Reference System\n\n\n\nWe‚Äôre using ESRI:102271 (Illinois State Plane East, NAD83, US Feet). This is appropriate for Chicago because:\n\nIt minimizes distortion in this region\nUses feet (common in US planning)\nAllows accurate distance calculations"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-1.2-load-burglary-data",
    "href": "labs/lab_4/assignment4.html#exercise-1.2-load-burglary-data",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 1.2: Load Burglary Data",
    "text": "Exercise 1.2: Load Burglary Data\n\n\nCode\n# Load from provided data file (downloaded from Chicago open data portal)\nburglaries &lt;- st_read(\"data/burglaries.shp\") %&gt;% \n  st_transform('ESRI:102271')\n\n\nReading layer `burglaries' from data source \n  `D:\\MUSA5080PPA\\portfolio-setup-ChristineCui12\\labs\\lab_4\\data\\burglaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7482 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 340492 ymin: 552959.6 xmax: 367153.5 ymax: 594815.1\nProjected CRS: NAD83(HARN) / Illinois East\n\n\nCode\n# Check the data\ncat(\"\\n‚úì Loaded burglary data\\n\")\n\n\n\n‚úì Loaded burglary data\n\n\nCode\ncat(\"  - Number of burglaries:\", nrow(burglaries), \"\\n\")\n\n\n  - Number of burglaries: 7482 \n\n\nCode\ncat(\"  - CRS:\", st_crs(burglaries)$input, \"\\n\")\n\n\n  - CRS: ESRI:102271 \n\n\nCode\ncat(\"  - Date range:\", min(burglaries$Date, na.rm = TRUE), \"to\", \n    max(burglaries$Date, na.rm = TRUE), \"\\n\")\n\n\n  - Date range: 17167 to 17532 \n\n\nQuestion 1.1: How many burglaries are in the dataset? What time period does this cover? Why does the coordinate reference system matter for our spatial analysis?\nYour answer here: - Number of burglaries: 7482 - Time period: 2017.1.1-2018.1.1 - Coordinate reference system: ?????\n\n\n\n\n\n\nWarningCritical Pause #1: Data Provenance\n\n\n\nBefore proceeding, consider where this data came from:\nWho recorded this data? Chicago Police Department officers and detectives\nWhat might be missing?\n\nUnreported burglaries (victims didn‚Äôt call police)\nIncidents police chose not to record\nDowngraded offenses (burglary recorded as trespassing)\nSpatial bias (more patrol = more recorded crime)\n\nThink About Was there a Department of Justice investigation of CPD during this period? What did they find about data practices?"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-1.3-visualize-point-data",
    "href": "labs/lab_4/assignment4.html#exercise-1.3-visualize-point-data",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 1.3: Visualize Point Data",
    "text": "Exercise 1.3: Visualize Point Data\n\n\nCode\n# Simple point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = burglaries, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Burglary Locations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(burglaries))\n  )\n\n# Density surface using modern syntax\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(burglaries)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"  # Modern ggplot2 syntax (not guide = FALSE)\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel density estimation\"\n  )\n\n# Combine plots using patchwork (modern approach)\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Burglaries in Chicago\",\n    tag_levels = 'A'\n  )\n\n\n\n\n\n\n\n\n\nQuestion 1.2: What spatial patterns do you observe? Are burglaries evenly distributed across Chicago? Where are the highest concentrations? What might explain these patterns?\nYour answer here: - Spatial patterns: - Evenly distributed: No, it‚Äôs dispersed obviously and it‚Äôs concentrated in 2 parts-north and south. - Highest concentrations: The highest concentration is in the south side. - Explaination:"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-2.1-understanding-the-fishnet",
    "href": "labs/lab_4/assignment4.html#exercise-2.1-understanding-the-fishnet",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 2.1: Understanding the Fishnet",
    "text": "Exercise 2.1: Understanding the Fishnet\nA fishnet grid converts irregular point data into a regular grid of cells where we can:\n\nAggregate counts\nCalculate spatial features\nApply regression models\n\nThink of it as overlaying graph paper on a map.\n\n\nCode\n# Create 500m x 500m grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,  # 500 meters per cell\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells that intersect Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\n# View basic info\ncat(\"‚úì Created fishnet grid\\n\")\n\n\n‚úì Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size:\", 500, \"x\", 500, \"meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nCode\ncat(\"  - Cell area:\", round(st_area(fishnet[1,])), \"square meters\\n\")\n\n\n  - Cell area: 250000 square meters\n\n\nQuestion 2.1: Why do we use a regular grid instead of existing boundaries like neighborhoods or census tracts? What are the advantages and disadvantages of this approach?\nYour answer here: - Reasons for using a regular grid: Existing boundaries like neighborhoods or census tractsÂ∞∫Â∫¶‰∏ç‰∏ÄÊ†∑Ôºårbitrary, unequal sizes, Modifiable Areal Unit Problem. - Pros and Cons: Fishnet grid has Consistent size, no boundary bias. We use fishnet because: Standard approach in predictive policing, Easier spatial operations, Consistent unit of analysis. But it has Arbitrary, may split ‚Äúnatural‚Äù areas."
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-2.2-aggregate-burglaries-to-grid",
    "href": "labs/lab_4/assignment4.html#exercise-2.2-aggregate-burglaries-to-grid",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 2.2: Aggregate Burglaries to Grid",
    "text": "Exercise 2.2: Aggregate Burglaries to Grid\n\n\nCode\n# Spatial join: which cell contains each burglary?\nburglaries_fishnet &lt;- st_join(burglaries, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countBurglaries = n())\n\n# Join back to fishnet (cells with 0 burglaries will be NA)\nfishnet &lt;- fishnet %&gt;%\n  left_join(burglaries_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countBurglaries = replace_na(countBurglaries, 0))\n\n# Summary statistics\ncat(\"\\nBurglary count distribution:\\n\")\n\n\n\nBurglary count distribution:\n\n\nCode\nsummary(fishnet$countBurglaries)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   3.042   5.000  40.000 \n\n\nCode\ncat(\"\\nCells with zero burglaries:\", \n    sum(fishnet$countBurglaries == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero burglaries: 781 / 2458 ( 31.8 %)\n\n\n\n\nCode\n# Visualize aggregated counts\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Burglaries\",\n    option = \"plasma\",\n    trans = \"sqrt\",  # Square root for better visualization of skewed data\n    breaks = c(0, 1, 5, 10, 20, 40)\n  ) +\n  labs(\n    title = \"Burglary Counts by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\nQuestion 2.2: What is the distribution of burglary counts across cells? Why do so many cells have zero burglaries? Is this distribution suitable for count regression? (Hint: look up overdispersion)\nYour answer here: - Distribution of burglary counts: - Reasons for cells have zero burglaries: - Suitability for count regression: No, ÊúâÂ§™Â§ö0ÂÄºÔºå‰ª£Ë°®Êï∞ÊçÆÈùûÂ∏∏ÂàÜÊï£ÔºåÊñπÂ∑ÆÂ∫îËØ•Â§ß‰∫éÂπ≥ÂùáÂÄº„ÄÇÊ≥äÊùæÂàÜÂ∏Écannot handle overdispersion and will underestimated if overdispersed. Poisson assumption: Variance = Mean, but Reality with crime data: Variance &gt; Mean ,ÊâÄ‰ª•‰∏çÁ¨¶ÂêàÊ≥äÊùæÂàÜÂ∏ÉÁöÑÂÅáËÆæÔºåÊâÄ‰ª•Ë¶ÅÁî®Ë¥ü‰∫åÂêëÂºèÂõûÂΩí„ÄÇ"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-4.1-load-311-abandoned-vehicle-calls",
    "href": "labs/lab_4/assignment4.html#exercise-4.1-load-311-abandoned-vehicle-calls",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.1: Load 311 Abandoned Vehicle Calls",
    "text": "Exercise 4.1: Load 311 Abandoned Vehicle Calls\n\n\nCode\nabandoned_cars &lt;- read_csv(\"data/abandoned_cars_2017.csv\")%&gt;%\n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271')\n\ncat(\"‚úì Loaded abandoned vehicle calls\\n\")\n\n\n‚úì Loaded abandoned vehicle calls\n\n\nCode\ncat(\"  - Number of calls:\", nrow(abandoned_cars), \"\\n\")\n\n\n  - Number of calls: 31390 \n\n\n\n\n\n\n\n\nNoteData Loading Note\n\n\n\nThe data was downloaded from Chicago‚Äôs Open Data Portal. You can now request an api from the Chicago portal and tap into the data there.\nConsider: How might the 311 reporting system itself be biased? Who calls 311? What neighborhoods have better 311 awareness?"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-4.2-count-of-abandoned-cars-per-cell",
    "href": "labs/lab_4/assignment4.html#exercise-4.2-count-of-abandoned-cars-per-cell",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.2: Count of Abandoned Cars per Cell",
    "text": "Exercise 4.2: Count of Abandoned Cars per Cell\n\n\nCode\n# Aggregate abandoned car calls to fishnet\nabandoned_fishnet &lt;- st_join(abandoned_cars, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(abandoned_cars = n())\n\n# Join to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(abandoned_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(abandoned_cars = replace_na(abandoned_cars, 0))\n\ncat(\"Abandoned car distribution:\\n\")\n\n\nAbandoned car distribution:\n\n\nCode\nsummary(fishnet$abandoned_cars)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    2.00    9.00   12.74   19.00  123.00 \n\n\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abandoned_cars), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"magma\") +\n  labs(title = \"Abandoned Vehicle 311 Calls\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Burglaries\") +\n  theme_crime()\n\np1 + p2 +\n  plot_annotation(title = \"Are abandoned cars and burglaries correlated?\")\n\n\n\n\n\n\n\n\n\nQuestion 4.1: Do you see a visual relationship between abandoned cars and burglaries? What does this suggest?"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#your-answer-here",
    "href": "labs/lab_4/assignment4.html#your-answer-here",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Your answer here:",
    "text": "Your answer here:"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-4.3-nearest-neighbor-features",
    "href": "labs/lab_4/assignment4.html#exercise-4.3-nearest-neighbor-features",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.3: Nearest Neighbor Features",
    "text": "Exercise 4.3: Nearest Neighbor Features\nCount in a cell is one measure. Distance to the nearest 3 abandoned cars captures local context.\n\n\nCode\n# Calculate mean distance to 3 nearest abandoned cars\n# (Do this OUTSIDE of mutate to avoid sf conflicts)\n\n# Get coordinates\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nabandoned_coords &lt;- st_coordinates(abandoned_cars)\n\n# Calculate k nearest neighbors and distances\nnn_result &lt;- get.knnx(abandoned_coords, fishnet_coords, k = 3)\n\n# Add to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    abandoned_cars.nn = rowMeans(nn_result$nn.dist)\n  )\n\ncat(\"‚úì Calculated nearest neighbor distances\\n\")\n\n\n‚úì Calculated nearest neighbor distances\n\n\nCode\nsummary(fishnet$abandoned_cars.nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   4.386   88.247  143.293  246.946  271.283 2195.753 \n\n\nQuestion 4.2: What does a low value of abandoned_cars.nn mean? A high value? Why might this be informative?\nYour answer here:"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-4.4-distance-to-hot-spots",
    "href": "labs/lab_4/assignment4.html#exercise-4.4-distance-to-hot-spots",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.4: Distance to Hot Spots",
    "text": "Exercise 4.4: Distance to Hot Spots\nLet‚Äôs identify clusters of abandoned cars using Local Moran‚Äôs I, then calculate distance to these hot spots.\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  \n  # Create spatial weights\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  # Calculate Local Moran's I\n  local_moran &lt;- localmoran(data[[variable]], weights)\n  \n  # Classify clusters\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      \n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to abandoned cars\nfishnet &lt;- calculate_local_morans(fishnet, \"abandoned_cars\", k = 5)\n\n\n\n\nCode\n# Visualize hot spots\nggplot() +\n  geom_sf(\n    data = fishnet, \n    aes(fill = moran_class), \n    color = NA\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Abandoned Car Clusters\",\n    subtitle = \"High-High = Hot spots of disorder\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get centroids of \"High-High\" cells (hot spots)\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance from each cell to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  \n  cat(\"‚úì Calculated distance to abandoned car hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(dist_to_hotspot = 0)\n  cat(\"‚ö† No significant hot spots found\\n\")\n}\n\n\n‚úì Calculated distance to abandoned car hot spots\n  - Number of hot spot cells: 275 \n\n\nQuestion 4.3: Why might distance to a cluster of abandoned cars be more informative than distance to a single abandoned car? What does Local Moran‚Äôs I tell us?\nYour answer here:\n\n\n\n\n\n\nNote\n\n\n\nLocal Moran‚Äôs I identifies:\n\nHigh-High: Hot spots (high values surrounded by high values)\nLow-Low: Cold spots (low values surrounded by low values)\nHigh-Low / Low-High: Spatial outliers\n\nThis helps us understand spatial clustering patterns."
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-6.1-poisson-regression",
    "href": "labs/lab_4/assignment4.html#exercise-6.1-poisson-regression",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 6.1: Poisson Regression",
    "text": "Exercise 6.1: Poisson Regression\nBurglary counts are count data (0, 1, 2, 3‚Ä¶). We‚Äôll use Poisson regression.\n\n\nCode\n# Create clean modeling dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countBurglaries,\n    abandoned_cars,\n    abandoned_cars.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()  # Remove any remaining NAs\n\ncat(\"‚úì Prepared modeling data\\n\")\n\n\n‚úì Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708 \n\n\nCode\ncat(\"  - Variables:\", ncol(fishnet_model), \"\\n\")\n\n\n  - Variables: 6 \n\n\n\n\nCode\n# Fit Poisson regression\nmodel_poisson &lt;- glm(\n  countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\n# Summary\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                      Estimate   Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        1.976262369  0.042512701  46.486 &lt;0.0000000000000002 ***\nabandoned_cars    -0.001360741  0.001089805  -1.249               0.212    \nabandoned_cars.nn -0.004965200  0.000198914 -24.962 &lt;0.0000000000000002 ***\ndist_to_hotspot    0.000002874  0.000006206   0.463               0.643    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6710.3  on 1707  degrees of freedom\nResidual deviance: 5070.6  on 1704  degrees of freedom\nAIC: 9138.9\n\nNumber of Fisher Scoring iterations: 6\n\n\nQuestion 6.1: Interpret the coefficients. Which variables are significant? What do the signs (positive/negative) tell you?\nYour answer here:"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-6.2-check-for-overdispersion",
    "href": "labs/lab_4/assignment4.html#exercise-6.2-check-for-overdispersion",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 6.2: Check for Overdispersion",
    "text": "Exercise 6.2: Check for Overdispersion\nPoisson regression assumes mean = variance. Real count data often violates this (overdispersion).\n\n\nCode\n# Calculate dispersion parameter\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 3.38 \n\n\nCode\ncat(\"Rule of thumb: &gt;1.5 suggests overdispersion\\n\")\n\n\nRule of thumb: &gt;1.5 suggests overdispersion\n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"‚ö† Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"‚úì Dispersion looks okay for Poisson model.\\n\")\n}\n\n\n‚ö† Overdispersion detected! Consider Negative Binomial model."
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-6.3-negative-binomial-regression",
    "href": "labs/lab_4/assignment4.html#exercise-6.3-negative-binomial-regression",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 6.3: Negative Binomial Regression",
    "text": "Exercise 6.3: Negative Binomial Regression\nIf overdispersed, use Negative Binomial regression (more flexible).\n\n\nCode\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Summary\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 1.603099596, \n    link = log)\n\nCoefficients:\n                      Estimate   Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        2.092907737  0.077469423  27.016 &lt;0.0000000000000002 ***\nabandoned_cars    -0.002006352  0.002091851  -0.959               0.337    \nabandoned_cars.nn -0.005844829  0.000321389 -18.186 &lt;0.0000000000000002 ***\ndist_to_hotspot    0.000006861  0.000011049   0.621               0.535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.6031) family taken to be 1)\n\n    Null deviance: 2534.0  on 1707  degrees of freedom\nResidual deviance: 1796.6  on 1704  degrees of freedom\nAIC: 7522.6\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.6031 \n          Std. Err.:  0.0888 \n\n 2 x log-likelihood:  -7512.5850 \n\n\nCode\n# Compare AIC (lower is better)\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 9138.9 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 7522.6 \n\n\nQuestion 6.2: Which model fits better (lower AIC)? What does this tell you about the data?\nYour answer here:"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-8.1-generate-final-predictions",
    "href": "labs/lab_4/assignment4.html#exercise-8.1-generate-final-predictions",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 8.1: Generate Final Predictions",
    "text": "Exercise 8.1: Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Also add KDE predictions (normalize to same scale as counts)\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countBurglaries, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_kde = (kde_value / kde_sum) * count_sum\n  )"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "href": "labs/lab_4/assignment4.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 8.2: Compare Model vs.¬†KDE Baseline",
    "text": "Exercise 8.2: Compare Model vs.¬†KDE Baseline\n\n\nCode\n# Create three maps\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Burglaries\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions (Neg. Binomial)\") +\n  theme_crime()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline Predictions\") +\n  theme_crime()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Actual vs. Predicted Burglaries\",\n    subtitle = \"Does our complex model outperform simple KDE?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate performance metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countBurglaries - prediction_nb)),\n    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),\n    kde_mae = mean(abs(countBurglaries - prediction_kde)),\n    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model Performance Comparison\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel Performance Comparison\n\n\napproach\nmae\nrmse\n\n\n\n\nmodel\n2.48\n3.59\n\n\nkde\n2.06\n2.95\n\n\n\n\n\nQuestion 8.1: Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?\nYour answer here:"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-9.3-where-does-the-model-work-well",
    "href": "labs/lab_4/assignment4.html#exercise-9.3-where-does-the-model-work-well",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 9.3: Where Does the Model Work Well?",
    "text": "Exercise 9.3: Where Does the Model Work Well?\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countBurglaries - prediction_nb,\n    error_kde = countBurglaries - prediction_kde,\n    abs_error_nb = abs(error_nb),\n    abs_error_kde = abs(error_kde)\n  )\n\n# Map errors\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0,\n    limits = c(-10, 10)\n  ) +\n  labs(title = \"Model Errors (Actual - Predicted)\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs. Error\", option = \"magma\") +\n  labs(title = \"Absolute Model Errors\") +\n  theme_crime()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nQuestion 9.2: Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?\nYour answer here:"
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-10.1-model-summary-table",
    "href": "labs/lab_4/assignment4.html#exercise-10.1-model-summary-table",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 10.1: Model Summary Table",
    "text": "Exercise 10.1: Model Summary Table\n\n\nCode\n# Create nice summary table\nmodel_summary &lt;- broom::tidy(final_model, exponentiate = TRUE) %&gt;%\n  mutate(\n    across(where(is.numeric), ~round(., 3))\n  )\n\nmodel_summary %&gt;%\n  kable(\n    caption = \"Final Negative Binomial Model Coefficients (Exponentiated)\",\n    col.names = c(\"Variable\", \"Rate Ratio\", \"Std. Error\", \"Z\", \"P-Value\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = \"Rate ratios &gt; 1 indicate positive association with burglary counts.\"\n  )\n\n\n\nFinal Negative Binomial Model Coefficients (Exponentiated)\n\n\nVariable\nRate Ratio\nStd. Error\nZ\nP-Value\n\n\n\n\n(Intercept)\n8.108\n0.077\n27.016\n0.000\n\n\nabandoned_cars\n0.998\n0.002\n-0.959\n0.337\n\n\nabandoned_cars.nn\n0.994\n0.000\n-18.186\n0.000\n\n\ndist_to_hotspot\n1.000\n0.000\n0.621\n0.535\n\n\n\nNote: \n\n\n\n\n\n\n Rate ratios &gt; 1 indicate positive association with burglary counts."
  },
  {
    "objectID": "labs/lab_4/assignment4.html#exercise-10.2-key-findings-summary",
    "href": "labs/lab_4/assignment4.html#exercise-10.2-key-findings-summary",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 10.2: Key Findings Summary",
    "text": "Exercise 10.2: Key Findings Summary\nBased on your analysis, complete this summary:\nTechnical Performance:\n\nCross-validation MAE: 2.7\nModel vs.¬†KDE: [Which performed better?]\nMost predictive variable: [Which had largest effect?]\n\nSpatial Patterns:\n\nBurglaries are [evenly distributed / clustered]\nHot spots are located in [describe]\nModel errors show [random / systematic] patterns\n\nModel Limitations:\n\nOverdispersion: [Yes/No]\nSpatial autocorrelation in residuals: [Test this!]\nCells with zero counts: [What % of data?]"
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "You are policy analysts hired by the Georgia Department of Corrections. They are considering deploying a recidivism prediction model to inform parole decisions. Your team must analyze the model and make a GO/NO-GO recommendation to the Commissioner.\n\n\n\n\n\n\nRun the provided R script (week10_exercise.R) and note:\n\nWhat‚Äôs the model‚Äôs AUC?\n\n\n0.732\n\n\nAt threshold 0.50, what‚Äôs the sensitivity and specificity?\n\n\nSensitivity: 0.8167219\nSpecificity: 0.4923896\n\n\nWhich racial group has the highest false positive rate?\n\n\nBlack, cos the FPR is 0.562 compared with 0.425 in white racial group.\n\n\nWhich group has the highest false negative rate?\n\n\nWhite, cos the FNR is 0.227 compared with 0.154 in black.\n\n\nWhat happens if we change the threshold to 0.30 or 0.70?\n\n\nThe FPR in both groups fall down dramatically, but the rate in black racial group is still higher than that in white racial group.\nIn reverse, FNR shows different pattern if we change the threshold to 0.30 or 0.70, which is an increase trend. But still, FNR in white racial group is higher than FNR in black group.\n\n\n\n\nAs a table or half table team, discuss your findings and complete the template below. Prepare to present your recommendation.\n\n\n\nPresent your recommendation to the ‚ÄúCommissioner‚Äù (instructor)."
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#your-role",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#your-role",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "You are policy analysts hired by the Georgia Department of Corrections. They are considering deploying a recidivism prediction model to inform parole decisions. Your team must analyze the model and make a GO/NO-GO recommendation to the Commissioner."
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#instructions",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#instructions",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "Run the provided R script (week10_exercise.R) and note:\n\nWhat‚Äôs the model‚Äôs AUC?\n\n\n0.732\n\n\nAt threshold 0.50, what‚Äôs the sensitivity and specificity?\n\n\nSensitivity: 0.8167219\nSpecificity: 0.4923896\n\n\nWhich racial group has the highest false positive rate?\n\n\nBlack, cos the FPR is 0.562 compared with 0.425 in white racial group.\n\n\nWhich group has the highest false negative rate?\n\n\nWhite, cos the FNR is 0.227 compared with 0.154 in black.\n\n\nWhat happens if we change the threshold to 0.30 or 0.70?\n\n\nThe FPR in both groups fall down dramatically, but the rate in black racial group is still higher than that in white racial group.\nIn reverse, FNR shows different pattern if we change the threshold to 0.30 or 0.70, which is an increase trend. But still, FNR in white racial group is higher than FNR in black group.\n\n\n\n\nAs a table or half table team, discuss your findings and complete the template below. Prepare to present your recommendation.\n\n\n\nPresent your recommendation to the ‚ÄúCommissioner‚Äù (instructor)."
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#consulting-team-information",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#consulting-team-information",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "Consulting Team Information",
    "text": "Consulting Team Information\nClever Team Name: _____________\nTeam Members:\n\nChristine_____________________\nDemi_____________________\nJinyang_____________________"
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#technical-assessment",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#technical-assessment",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "1. TECHNICAL ASSESSMENT",
    "text": "1. TECHNICAL ASSESSMENT\n\nModel Performance Metrics\nAUC (Area Under ROC Curve): 0.732____\nAt threshold = 0.50:\n\nSensitivity (True Positive Rate): 0.817________\nSpecificity (True Negative Rate): 0.492______\nPrecision (Positive Predictive Value): 0.706______\nOverall Accuracy: 0.686______\n\n\n\nTechnical Quality Rating\nSelect one:\n\nExcellent (AUC &gt; 0.90)\nGood (AUC 0.80-0.90)\n‚òê‚àö Acceptable (AUC 0.70-0.80)\nPoor (AUC &lt; 0.70)\n\n\n\nBrief Technical Summary (2-3 sentences)\nIs the model accurate enough for high-stakes decision-making?\nThe model demonstrates acceptable but limited discriminatory power (AUC = 0.732). While it has good sensitivity (81.7%), its low specificity (49.2%) results in a high rate of false positives. Therefore, it is not accurate enough for high-stakes decision-making without careful consideration of the costs associated with false alarms.______________________________________________________________________"
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#equity-analysis",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#equity-analysis",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "2. EQUITY ANALYSIS",
    "text": "2. EQUITY ANALYSIS\n\nFalse Positive Rates by Race (at threshold 0.50)\n\n\n\nRacial Group\nFalse Positive Rate\nSample Size\n\n\n\n\nGroup 1: Black\n0.562\n3931\n\n\nGroup 2: White\n0.425\n2620\n\n\nGroup 3:\n\n\n\n\nGroup 4:\n\n\n\n\n\n\n\nFalse Negative Rates by Race (at threshold 0.50)\n\n\n\nRacial Group\nFalse Negative Rate\nSample Size\n\n\n\n\nGroup 1: Black\n0.154\n3931\n\n\nGroup 2: White\n0.227\n2620\n\n\nGroup 3:\n\n\n\n\nGroup 4:\n\n\n\n\n\n\n\nDisparity Analysis\nLargest disparity identified:\nGroup _____Black________ has 13.7_% higher false positive rate than Group White______\nOR\nGroup ______White_______ has 7.3% higher false negative rate than Group ____Black_________\n\n\nEquity Concerns Summary (3-4 sentences)\nWhat are the implications of these disparities? Who is harmed?\nThe model exhibits significant racial disparity, with the Black population experiencing a 13.7% higher False Positive Rate than the White population. This means that Black individuals are disproportionately harmed by being incorrectly flagged as ‚Äúhigh-risk‚Äù when they are not. Such a disparity could lead to unjust outcomes, including denied opportunities or increased scrutiny, thereby perpetuating and automating existing biases. The model in its current state raises serious equity concerns and is not suitable for deployment without algorithmic fairness interventions."
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#threshold-recommendation",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#threshold-recommendation",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "3. THRESHOLD RECOMMENDATION",
    "text": "3. THRESHOLD RECOMMENDATION\n\nIf we deploy this model, we recommend:\nSelect one:\n\nThreshold = 0.30 (Aggressive - prioritize catching recidivists)\nThreshold = 0.50 (Balanced - default)\n‚òê‚àö Threshold = 0.70 (Conservative - minimize false accusations)\nOther: ________\n\n\n\nRationale for Threshold Choice (3-4 sentences)\nWhy this threshold? What does it optimize for? What are the trade-offs?\n\nWe recommend the conservative threshold of 0.70 to prioritize minimizing false positives, thereby reducing the risk of unjustly accusing individuals who would not reoffend. This choice is critically informed by the equity analysis, which revealed that false positives disproportionately harm the Black population. The trade-off is a deliberate acceptance of more false negatives, meaning some actual recidivists may be missed, in order to prevent the more severe societal harm of systematic false accusations against a protected group. This approach optimizes for fairness and mitigates the model‚Äôs potential to amplify existing biases.\n\n\n\nThis threshold prioritizes:\nSelect one:\n\nHigh Sensitivity - Catch more people who will reoffend (accept more false positives)\n‚òê‚àö High Specificity - Avoid false accusations (accept more false negatives)\nBalance - Try to minimize both types of errors"
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#deployment-recommendation",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#deployment-recommendation",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "4. DEPLOYMENT RECOMMENDATION",
    "text": "4. DEPLOYMENT RECOMMENDATION\n\nOur recommendation to Georgia DOC:\nSelect one:\n\nDEPLOY - Use this model to inform parole decisions\nDO NOT DEPLOY - Do not use this model\n‚òê‚àö CONDITIONAL DEPLOY - Deploy only with specific safeguards in place\n\n\n\nKey Reasons for Our Recommendation\nProvide 3-5 bullet points supporting your decision:\n\n\n\n\n\n\n\n\n\nWhat about the equity concerns?\nHow do you justify your recommendation given the disparate impact you identified?"
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#safeguards-or-alternatives",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#safeguards-or-alternatives",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "5. SAFEGUARDS OR ALTERNATIVES",
    "text": "5. SAFEGUARDS OR ALTERNATIVES\n\nIf DEPLOY - Required Safeguards\nWhat protections must be in place before deployment?\n\n\n\n\n\n\nOR\n\n\nIf DO NOT DEPLOY - Alternative Approaches\nWhat should Georgia DOC do instead?"
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#limitations-uncertainties",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#limitations-uncertainties",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "6. LIMITATIONS & UNCERTAINTIES",
    "text": "6. LIMITATIONS & UNCERTAINTIES\n\nWhat we don‚Äôt know (but wish we did)\nWhat additional information would strengthen your recommendation?\n\n\n\n\n\nWeaknesses in our recommendation\nWhat‚Äôs the strongest argument AGAINST your recommendation?"
  },
  {
    "objectID": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#bottom-line",
    "href": "weekly-notes/week-10/script/In_Class_Exercise_Instruction.html#bottom-line",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "7. BOTTOM LINE",
    "text": "7. BOTTOM LINE\n\nOne-Sentence Recommendation\nIf the Commissioner only reads one thing, what should it be?"
  }
]