data = bus_plot_data,
aes(x = X, y = Y, fill = ..level.., weight = Ridership),
geom = "polygon",
alpha = 0.75
) +
# 配色：蓝色系 (代表正常活动)
scale_fill_distiller(palette = "Blues", direction = 1, guide = "none") +
labs(
title = "Weekday Ridership Hotspots",
subtitle = "High Transit Activity Zones"
) +
mapTheme
# 右图：犯罪热力图 (Crime Density)
p_crime_map <- ggplot() +
# 底图
geom_sf(data = philly_boundary, fill = "#f5f5f5", color = "grey80") +
# 热力层 (修改部分)
stat_density_2d(
data = crime_plot_data,
aes(x = X, y = Y, fill = ..level..),
geom = "polygon",
alpha = 0.4,       # 1. 调低透明度，减少"硬边"的感觉
bins = 30,         # 2. 增加层数，让过渡更平滑 (原来默认很少)
adjust = 0.5       # 3. 调低带宽 (0.5), 让热力点收缩，更聚焦局部热点
) +
# 配色
scale_fill_distiller(palette = "Reds", direction = 1, guide = "none") +
labs(
title = "Crime Hotspots",
subtitle = "High Incident Zones"
) +
mapTheme
# --- 3. 组合展示 (Combine Side-by-Side) ---
# 使用 patchwork 进行拼接
combined_map <- p_ridership + p_crime_map +
plot_annotation(
title = "Spatial Mismatch Analysis: Eyes on the Street vs. Targets?",
subtitle = "Left: Where people are (Ridership) | Right: Where crimes happen",
theme = theme(
plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
plot.subtitle = element_text(size = 12, color = "grey40", hjust = 0.5)
)
)
# 输出图形
combined_map
# Crime vs. Ridership (Interaction Plot)
# 这是一个非常关键的图，用于验证你的核心假设：周末的客流影响是否不同？
ggplot(final_data, aes(x = Log_Ridership, y = Crime_Daily_Rate, color = is_weekend_factor)) +
geom_point(alpha = 0.1, size = 1) +
geom_smooth(method = "glm", method.args = list(family = "quasipoisson"), se = TRUE) +
scale_color_manual(values = c("Weekday" = "#3182bd", "Weekend" = "#de2d26")) +
labs(
title = "Does Ridership impact Crime differently on Weekends?",
subtitle = "Interaction Effect (Normalized by Number of Days)",
x = "Log(Daily Ridership)",
y = "Average Daily Crime Count (per 400m)", # 修改 Y 轴标签
color = "Time Period"
) +
plotTheme
library(patchwork)
library(scales)
# 为了可视化清晰，我们先创建一个带有 log(crime) 的临时绘图数据
plot_data <- final_data %>%
mutate(
log_crime = log(Crime_Daily_Rate + 0.01), # +1 避免 log(0)
# 给 Income 取个 log 看看是否更线性，很多经济学变量 log 后效果更好
log_income = log(Avg_Income + 1)
)
# --- 1. POI & Infrastructure (Built Environment) ---
# A. Alcohol Outlets (线性还是指数增加？)
p1 <- ggplot(plot_data, aes(x = Alcohol_Count, y = log_crime)) +
geom_point(alpha = 0.1, color = "#6A1B9A") +
geom_smooth(method = "loess", color = "red", se = FALSE) +
labs(title = "Log(Crime) vs. Alcohol Outlets",
subtitle = "Check for diminishing returns",
x = "Count of Alcohol Outlets", y = "Log(Crime Count)") +
plotTheme
# B. Street Lights (路灯越多越安全？还是路灯只出现在繁华区？)
p2 <- ggplot(plot_data, aes(x = Light_Count, y = log_crime)) +
geom_point(alpha = 0.1, color = "#6A1B9A") +
geom_smooth(method = "loess", color = "red", se = FALSE) +
labs(title = "Log(Crime) vs. Street Lights",
subtitle = "Is the relationship linear?",
x = "Count of Street Lights", y = "") +
plotTheme
# C. Distance to Police (离警局越远越危险？)
p3 <- ggplot(plot_data, aes(x = Dist_Police, y = log_crime)) +
geom_point(alpha = 0.1, color = "#6A1B9A") +
geom_smooth(method = "loess", color = "red", se = FALSE) +
labs(title = "Log(Crime) vs. Dist to Police",
subtitle = "Check for U-shape or threshold",
x = "Distance to Station (Miles)", y = "") +
plotTheme
# --- 2. Demographics (Census) ---
# D. Poverty Rate (贫困率与犯罪)
p4 <- ggplot(plot_data, aes(x = Avg_Poverty, y = log_crime)) +
geom_point(alpha = 0.1, color = "#3182bd") +
geom_smooth(method = "loess", color = "orange", se = FALSE) +
scale_x_continuous(labels = scales::percent) +
labs(title = "Log(Crime) vs. Poverty Rate",
x = "Poverty Rate", y = "Log(Crime Count)") +
plotTheme
# E. Median Income (收入与犯罪 - 这是一个典型的可能需要 log 的变量)
p5 <- ggplot(plot_data, aes(x = Avg_Income, y = log_crime)) +
geom_point(alpha = 0.1, color = "#3182bd") +
geom_smooth(method = "loess", color = "orange", se = FALSE) +
scale_x_continuous(labels = scales::dollar) +
labs(title = "Log(Crime) vs. Median Income",
subtitle = "Does wealth shield against crime?",
x = "Median Household Income", y = "") +
plotTheme
# F. Vacancy Rate (空置率/破窗理论)
p6 <- ggplot(plot_data, aes(x = Avg_Vacancy, y = log_crime)) +
geom_point(alpha = 0.1, color = "#3182bd") +
geom_smooth(method = "loess", color = "orange", se = FALSE) +
scale_x_continuous(labels = scales::percent) +
labs(title = "Log(Crime) vs. Vacancy Rate",
x = "Housing Vacancy Rate", y = "") +
plotTheme
# --- Combine Plots ---
(p1 | p2 | p3) / (p4 | p5 | p6) +
plot_annotation(
title = "Exploratory Analysis: Variable Functional Forms",
subtitle = "Red Line = Loess Smoother (Non-linear trend)",
theme = theme(plot.title = element_text(size = 16, face = "bold"))
)
library(ggcorrplot)
# 1. 选择数值型变量
numeric_vars <- final_data %>%
st_drop_geometry() %>%
dplyr::select(
Crime_Daily_Rate,
Ridership,
Alcohol_Count,
Light_Count,
Dist_Police,
Avg_Poverty,
Avg_Income,
Avg_Unemployment,
Avg_Vacancy
)
# 2. 计算相关系数矩阵
corr_matrix <- cor(numeric_vars, use = "complete.obs")
# 3. 绘图
ggcorrplot(
corr_matrix,
method = "square",
type = "lower",
lab = TRUE,
lab_size = 3,
colors = c("#6D9EC1", "white", "#E46726"),
title = "Correlation Matrix of Features",
ggtheme = theme_minimal()
)
### 3.5 Weekend Effect Boxplot
ggplot(final_data, aes(x = is_weekend_factor, y = Crime_Daily_Rate, fill = is_weekend_factor)) +
geom_boxplot(alpha = 0.7, outlier.shape = NA) +
# 加上均值点
stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") +
# 限制 Y 轴范围 (注意：现在的单位是“每天”，数值会很小，比如 0.05)
# 使用 quantile 自动截断极端值
coord_cartesian(ylim = c(0, quantile(final_data$Crime_Daily_Rate, 0.95))) +
scale_fill_manual(values = c("Weekday" = "#3182bd", "Weekend" = "#de2d26")) +
labs(
title = "Daily Crime Risk: Weekday vs. Weekend",
subtitle = "Comparison of Average Daily Crime Counts per Stop",
x = "Time Period",
y = "Average Daily Crime Count (per 400m)", # 修改标签
caption = "Note: Values represent daily averages to account for fewer weekend days per year."
) +
plotTheme
model_1 <- glm.nb(Crime_Total_Count ~
Log_Ridership +
offset(log(Exposure_Days)),
data = final_data)
summary(model_1)
model_2 <- glm.nb(Crime_Total_Count ~
Log_Ridership * is_weekend_factor +
offset(log(Exposure_Days)),
data = final_data)
summary(model_2)
model_3 <- glm.nb(Crime_Total_Count ~
Log_Ridership * is_weekend_factor +
log(Alcohol_Count + 1) +        # 诱发因子
poly(Light_Count, 2) +          # 预防因子
Avg_Poverty +          # 社会结构
Avg_Vacancy + I(Avg_Vacancy^2) +          # 破窗效应
Avg_Income + I(Avg_Income^2)  +
Avg_Unemployment +
Dist_Police +
offset(log(Exposure_Days)),
data = final_data)
summary(model_3)
# 加入 'GEOID' (Census Tract) 作为固定效应
# 这相当于为费城每一个普查区都加了一个“基准拦截”，控制了所有观测不到的社区特征
model_4 <- glm.nb(Crime_Total_Count ~
Log_Ridership * is_weekend_factor +
log(Alcohol_Count + 1) +        # 诱发因子
poly(Light_Count, 2) +          # 预防因子
Avg_Poverty +          # 社会结构
Avg_Vacancy + I(Avg_Vacancy^2) +          # 破窗效应
Avg_Income + I(Avg_Income^2)  +
Avg_Unemployment +
Dist_Police +          # 政策变量：警力可达性
factor(PSA_ID) +      # <--- 如果模型跑不动(不收敛)，注释掉这一行
offset(log(Exposure_Days)),
data = final_data,
control = glm.control(maxit = 100)) # 增加迭代次数以防不收敛
summary(model_4)
# 加入警局距离，并加入 'GEOID' (Census Tract) 作为固定效应
# 这相当于为费城每一个普查区都加了一个“基准拦截”，控制了所有观测不到的社区特征
model_5 <- glm.nb(Crime_Total_Count ~
Log_Ridership * is_weekend_factor +
log(Alcohol_Count + 1) +        # 诱发因子
poly(Light_Count, 2) +          # 预防因子
Avg_Poverty +          # 社会结构
Avg_Vacancy + I(Avg_Vacancy^2) +          # 破窗效应
Avg_Income + I(Avg_Income^2)  +
Avg_Unemployment +
Dist_Police +          # 政策变量：警力可达性
factor(PSA_ID) +
log(Crime_Daily_Rate_lag + 0.001) +
offset(log(Exposure_Days)),
data = final_data,
control = glm.control(maxit = 100)) # 增加迭代次数以防不收敛
summary(model_5)
vif(model_5)
model_6 <- glm.nb(Crime_Total_Count ~
Log_Ridership * is_weekend_factor +
log(Alcohol_Count + 1) +
Light_Count +
# --- 核心修改 ---
# 保留贫困率
Avg_Poverty +
# 删除了 Avg_Income 和 I(Avg_Income^2）
# 检查空置率：如果平方项不显著，也可以删掉平方项只留 Avg_Vacancy
Avg_Vacancy + I(Avg_Vacancy^2) +
Avg_Unemployment +
Dist_Police +
factor(PSA_ID) +
log(Crime_Daily_Rate_lag + 0.001) +
offset(log(Exposure_Days)),
data = final_data,
control = glm.control(maxit = 100))
summary(model_6)
# 重新看 VIF
vif(model_6)
# 1. 加载必要的包
library(modelsummary)
library(ggplot2)
library(dplyr)
# 2. 定义变量名的“美化映射” (跟 Stargazer 的 label 对应)
coef_map_list <- c(
"Log_Ridership" = "Ridership (Log)",
"is_weekend_factorWeekend" = "Weekend Effect",
"Log_Ridership:is_weekend_factorWeekend" = "Interaction: Ridership × Weekend",
# --- 环境变量 ---
"log(Alcohol_Count + 1)" = "Alcohol Outlets",
# --- 路灯 (兼容 model_6 的线性项和旧模型的二次项) ---
"Light_Count" = "Street Lights (Linear)",       # Model 6 用
"poly(Light_Count, 2)1" = "Street Lights (Poly 1)", # Model 5 用
"poly(Light_Count, 2)2" = "Street Lights (Poly 2)", # Model 5 用
# --- 社会经济 (model_6 去掉了 Income) ---
"Avg_Poverty" = "Poverty Rate",
"Avg_Vacancy" = "Vacancy Rate",
"I(Avg_Vacancy^2)" = "Vacancy Rate (Squared)",
# --- 政策变量 ---
"Dist_Police" = "Dist to Police Station",
# --- 滞后项 ---
"log(Crime_Daily_Rate_lag + 0.001)" = "Temporal Lag (Prev. Quarter)"
)
# 3. 绘制系数图
p_models <- modelplot(
list(
"1. Naive" = model_1,
# "3. Context" = model_3, # 可以根据需要保留或注释掉旧模型
"5. Robust (PSA)" = model_5,   # 含 Income 和 Poly Lights
"6. Final (Refined)" = model_6 # ⭐ 你的最终模型 (无 Income, 线性路灯)
),
coef_map = coef_map_list,
conf_level = 0.95
) +
# --- 美化代码 ---
geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
labs(
title = "What Drives Crime Risk?",
subtitle = "Model Evolution: Comparing Robust (M5) vs. Final Refined (M6)",
x = "Effect Size (Coefficient Estimate)",
y = "",
caption = "Note: Model 6 removes collinear Income variables and simplifies Street Lights to a linear term."
) +
theme_minimal() +
scale_color_brewer(palette = "Set1") +
theme(
legend.position = "bottom",
plot.title = element_text(face = "bold", size = 14),
axis.text.y = element_text(size = 11, face = "bold")
)
# 展示图形
p_models
# 1. 重新定义五个模型的公式 (确保与 Phase 4 一致)
f1 <- Crime_Total_Count ~ Log_Ridership + offset(log(Exposure_Days))
f2 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + offset(log(Exposure_Days))
f3 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + Avg_Unemployment + Dist_Police + offset(log(Exposure_Days))
f4 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor + log(Alcohol_Count + 1) + poly(Light_Count, 2) + Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) + Avg_Income + I(Avg_Income^2) + Avg_Unemployment + Dist_Police + factor(PSA_ID) + offset(log(Exposure_Days))
f5 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor +
log(Alcohol_Count + 1) + poly(Light_Count, 2) +
Avg_Poverty + Avg_Vacancy + I(Avg_Vacancy^2) +
Avg_Income + I(Avg_Income^2) + Avg_Unemployment +
Dist_Police + factor(PSA_ID) +
log(Crime_Daily_Rate_lag + 0.001) +
offset(log(Exposure_Days))
f6 <- Crime_Total_Count ~ Log_Ridership * is_weekend_factor +
log(Alcohol_Count + 1) +
Light_Count +              # 线性项
Avg_Poverty +              # 仅保留贫困
Avg_Vacancy + I(Avg_Vacancy^2) +
Avg_Unemployment +
Dist_Police +
factor(PSA_ID) +           # PSA 固定效应
log(Crime_Daily_Rate_lag + 0.001) +
offset(log(Exposure_Days))
# 2. 训练五个模型
m1 <- glm.nb(f1, data = final_data)
m2 <- glm.nb(f2, data = final_data)
m3 <- glm.nb(f3, data = final_data)
m4 <- glm.nb(f4, data = final_data)
m5 <- glm.nb(f5, data = final_data)
m6 <- glm.nb(f6, data = final_data)
# 3. 生成预测数据 (Gather Predictions)
# type = "response" 会直接返回预测的犯罪数量 (Count)，不需要再 exp
plot_data <- final_data %>%
dplyr::select(Crime_Total_Count) %>%
mutate(
Model_1_Pred = predict(m1, type = "response"),
Model_2_Pred = predict(m2, type = "response"),
Model_3_Pred = predict(m3, type = "response"),
Model_4_Pred = predict(m4, type = "response"),
Model_5_Pred = predict(m5, type = "response"),
Model_6_Pred = predict(m6, type = "response"),
) %>%
pivot_longer(
cols = starts_with("Model"),
names_to = "Model",
values_to = "Predicted_Count"
)
# 4. 绘图：Facet Wrap 对比6个模型
# 因为犯罪是计数数据，点会重叠，我们使用 alpha 和 45度线
ggplot(plot_data, aes(x = Predicted_Count, y = Crime_Total_Count)) +
geom_point(alpha = 0.2, color = "#3182bd", size = 0.8) +
# 添加完美预测线 (y=x)
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
facet_wrap(~Model, ncol = 3) +
labs(
title = "Predicted vs. Actual Crime Counts",
subtitle = "Comparing Model Fit: Final Model (M6) shows robust predictions",
x = "Predicted Crime Count",
y = "Actual Crime Count"
) +
theme_bw() +
# 限制坐标轴范围以便看清核心区域 (去掉极值干扰)
coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))
library(caret)
# 1. 设置 5-Fold Cross Validation
set.seed(999)
folds <- createFolds(final_data$Crime_Total_Count, k = 5, list = TRUE)
# 定义一个辅助函数：跑 CV 并计算指标
run_cv <- function(formula, data, folds) {
mae_list <- c()
rmse_list <- c()
for (i in 1:length(folds)) {
# Split Data
test_idx <- folds[[i]]
train_set <- data[-test_idx, ]
test_set  <- data[test_idx, ]
# Train Model (tryCatch 防止不收敛报错)
model <- tryCatch({
glm.nb(formula, data = train_set)
}, error = function(e) return(NULL))
if(!is.null(model)) {
# Predict (type = "response" 返回预测的数量)
preds <- predict(model, newdata = test_set, type = "response")
# Calculate Errors
actuals <- test_set$Crime_Total_Count
mae_list <- c(mae_list, mean(abs(actuals - preds)))
rmse_list <- c(rmse_list, sqrt(mean((actuals - preds)^2)))
}
}
return(c(mean(mae_list), mean(rmse_list)))
}
# 2. 对五个模型分别跑 CV
# 这里的公式需要跟上面定义的一致
results_m1 <- run_cv(f1, final_data, folds)
results_m2 <- run_cv(f2, final_data, folds)
results_m3 <- run_cv(f3, final_data, folds)
results_m4 <- run_cv(f4, final_data, folds)
results_m5 <- run_cv(f5, final_data, folds)
results_m6 <- run_cv(f6, final_data, folds)
# 3. 汇总表格
validation_summary <- data.frame(
Model = c("1. Ridership Only", "2. +Interaction", "3. +Env & Demo", "4. +Policy & Dist", "5. +Temporal lag", "6. Refined"),
MAE  = c(results_m1[1], results_m2[1], results_m3[1], results_m4[1], results_m5[1], results_m6[1]),
RMSE = c(results_m1[2], results_m2[2], results_m3[2], results_m4[2], results_m5[2], results_m6[2])
) %>%
mutate(
# 计算相对于 Model 1 的提升百分比
Improvement_MAE = (MAE[1] - MAE) / MAE[1]
)
# 4. 展示漂亮的表格
kable(validation_summary, digits = 3, caption = "5-Fold Cross-Validation Metrics") %>%
kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
column_spec(4, color = "green", bold = TRUE)
# 1. 明确指定 Model 5 为最佳模型
best_model <- model_6
# 2. 提取残差
# Pearson 残差用于统计检验（因为它是标准化的）
# Deviance 残差用于绘图（因为视觉上更能反映拟合优度）
final_data$resid_pearson  <- residuals(best_model, type = "pearson")
final_data$resid_deviance <- residuals(best_model, type = "deviance")
# 3. 构建空间权重矩阵 (Spatial Weights Matrix)
# 由于公交站点是离散点，使用 k-Nearest Neighbors (KNN) 比距离阈值更稳健
# 这里选取 k=8，意味着每个站点对比它最近的 8 个邻居
coords <- st_coordinates(final_data)
neighbor_nb <- knn2nb(knearneigh(coords, k = 8))
spatial_weights <- nb2listw(neighbor_nb, style = "W")
# 4. 运行 Moran's I 检验
moran_result <- moran.test(final_data$resid_pearson, spatial_weights)
# 打印结果
print(moran_result)
# 解释逻辑：
# 如果 p-value > 0.05: 恭喜！残差是随机分布的，模型非常完美，没有遗漏空间变量。
# 如果 p-value < 0.05 但 Moran's I 很小 (如 < 0.1): 模型还可以，只有轻微的空间依赖。
# Spatial Distribution of Residuals
# 1. 将残差添加回空间数据
# type = "deviance" residuals are often better for mapping goodness-of-fit
final_data$spatial_resid <- residuals(best_model, type = "deviance")
# 2. 绘制地图
ggplot() +
# 底图
geom_sf(data = philly_boundary, fill = "grey95", color = NA) +
# 站点残差图
geom_sf(data = final_data,
aes(color = spatial_resid),
size = 0.8, alpha = 0.7) +
# 颜色比例尺
scale_color_gradient2(
low = "blue", mid = "grey90", high = "red",
midpoint = 0,
name = "Deviance\nResidual",
# 限制范围，防止极个别离群值破坏颜色分布
limits = c(-3, 3),
oob = scales::squish
) +
labs(
title = "Map of Model Residuals",
subtitle = "Red = Unexpectedly High Crime (Under-predicted)\nBlue = Unexpectedly Low Crime (Over-predicted)"
) +
mapTheme
best_model <- model_6
# 1. 提取拟合值和残差
# 注意：对于 glm.nb，必须指定 type = "pearson" 来标准化残差
model_data <- data.frame(
Fitted = fitted(best_model),
Residuals = residuals(best_model, type = "pearson")
)
# 2. 绘制残差 vs. 拟合值图
p_resid_fitted <- ggplot(model_data, aes(x = log(Fitted), y = Residuals)) +
geom_point(alpha = 0.3, color = "#6A1B9A", size = 1.5) +
geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
geom_smooth(method = "loess", color = "black", se = FALSE, linewidth = 0.8) +
labs(
title = "Residuals vs Fitted Values",
subtitle = "Checking for systematic bias in the Count Model",
x = "Log(Fitted Values) - Predicted Crime Count",
y = "Pearson Residuals"
) +
plotTheme
p_resid_fitted
p_qq <- ggplot(model_data, aes(sample = Residuals)) +
stat_qq(color = "#6A1B9A", size = 1.5, alpha = 0.5) +
stat_qq_line(color = "red", linetype = "dashed", linewidth = 1) +
labs(
title = "Q-Q Plot of Pearson Residuals",
subtitle = "Checking for extreme outliers in count data",
x = "Theoretical Quantiles",
y = "Sample Quantiles"
) +
plotTheme
p_qq
# 1. 提取预测值 (Predicted Counts)
final_data <- final_data %>%
mutate(Predicted_Crime = predict(best_model, type = "response"))
# 2. 筛选前 50 个高风险站点
top_50_risky <- final_data %>%
arrange(desc(Predicted_Crime)) %>%
slice(1:50)
# 3. 绘制行动地图
ggplot() +
# 背景底图
geom_sf(data = philly_boundary, fill = "grey95", color = "white") +
# 所有站点 (灰色背景)
geom_sf(data = final_data, color = "grey80", size = 0.5, alpha = 0.3) +
# 高风险站点 (红色醒目)
geom_sf(data = top_50_risky, color = "red", size = 2, alpha = 0.9) +
# (可选) 加上文字标签
# geom_sf_text(data = head(top_50_risky, 5), aes(label = Stop), dy = 100, size = 3) +
labs(
title = "Top 50 High-Risk Bus Stops",
subtitle = "Priority Zones for Police Patrol Deployment (Based on Model Prediction)",
caption = "Red dots represent the stops with the highest predicted crime counts."
) +
mapTheme
