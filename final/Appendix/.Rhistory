# 3. 生成预测数据 (Gather Predictions)
# type = "response" 会直接返回预测的犯罪数量 (Count)，不需要再 exp
plot_data <- final_data %>%
dplyr::select(Crime_Total_Count) %>%
mutate(
Model_1_Pred = predict(m1, type = "response"),
Model_2_Pred = predict(m2, type = "response"),
Model_3_Pred = predict(m3, type = "response"),
Model_4_Pred = predict(m4, type = "response"),
Model_5_Pred = predict(m5, type = "response"),
Model_6_Pred = predict(m6, type = "response"),
) %>%
pivot_longer(
cols = starts_with("Model"),
names_to = "Model",
values_to = "Predicted_Count"
)
# 4. 绘图：Facet Wrap 对比6个模型
# 因为犯罪是计数数据，点会重叠，我们使用 alpha 和 45度线
ggplot(plot_data, aes(x = Predicted_Count, y = Crime_Total_Count)) +
geom_point(alpha = 0.2, color = "#3182bd", size = 0.8) +
# 添加完美预测线 (y=x)
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
facet_wrap(~Model, ncol = 3) +
labs(
title = "Predicted vs. Actual Crime Counts",
subtitle = "Comparing Model Fit: Final Model (M6) shows robust predictions",
x = "Predicted Crime Count",
y = "Actual Crime Count"
) +
theme_bw() +
# 限制坐标轴范围以便看清核心区域 (去掉极值干扰)
coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))
library(caret)
# 1. 设置 5-Fold Cross Validation
set.seed(999)
folds <- createFolds(final_data$Crime_Total_Count, k = 5, list = TRUE)
# 定义一个辅助函数：跑 CV 并计算指标
run_cv <- function(formula, data, folds) {
mae_list <- c()
rmse_list <- c()
for (i in 1:length(folds)) {
# Split Data
test_idx <- folds[[i]]
train_set <- data[-test_idx, ]
test_set  <- data[test_idx, ]
# Train Model (tryCatch 防止不收敛报错)
model <- tryCatch({
glm.nb(formula, data = train_set)
}, error = function(e) return(NULL))
if(!is.null(model)) {
# Predict (type = "response" 返回预测的数量)
preds <- predict(model, newdata = test_set, type = "response")
# Calculate Errors
actuals <- test_set$Crime_Total_Count
mae_list <- c(mae_list, mean(abs(actuals - preds)))
rmse_list <- c(rmse_list, sqrt(mean((actuals - preds)^2)))
}
}
return(c(mean(mae_list), mean(rmse_list)))
}
# 2. 对五个模型分别跑 CV
# 这里的公式需要跟上面定义的一致
results_m1 <- run_cv(f1, final_data, folds)
results_m2 <- run_cv(f2, final_data, folds)
results_m3 <- run_cv(f3, final_data, folds)
results_m4 <- run_cv(f4, final_data, folds)
results_m5 <- run_cv(f5, final_data, folds)
results_m6 <- run_cv(f6, final_data, folds)
# 3. 汇总表格
validation_summary <- data.frame(
Model = c("1. Ridership Only", "2. +Interaction", "3. +Env & Demo", "4. +Spatial Fixed", "5. +Temporal lag", "6. Refined"),
MAE  = c(results_m1[1], results_m2[1], results_m3[1], results_m4[1], results_m5[1], results_m6[1]),
RMSE = c(results_m1[2], results_m2[2], results_m3[2], results_m4[2], results_m5[2], results_m6[2])
) %>%
mutate(
# 计算相对于 Model 1 的提升百分比
Improvement_MAE = (MAE[1] - MAE) / MAE[1]
)
# 4. 展示漂亮的表格
kable(validation_summary, digits = 3, caption = "5-Fold Cross-Validation Metrics") %>%
kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
column_spec(4, color = "green", bold = TRUE)
# 1. 明确指定 Model 5 为最佳模型
best_model <- model_6
# 2. 提取残差
# Pearson 残差用于统计检验（因为它是标准化的）
# Deviance 残差用于绘图（因为视觉上更能反映拟合优度）
final_data$resid_pearson  <- residuals(best_model, type = "pearson")
final_data$resid_deviance <- residuals(best_model, type = "deviance")
# 3. 构建空间权重矩阵 (Spatial Weights Matrix)
# 由于公交站点是离散点，使用 k-Nearest Neighbors (KNN) 比距离阈值更稳健
# 这里选取 k=8，意味着每个站点对比它最近的 8 个邻居
coords <- st_coordinates(final_data)
neighbor_nb <- knn2nb(knearneigh(coords, k = 8))
spatial_weights <- nb2listw(neighbor_nb, style = "W")
# 4. 运行 Moran's I 检验
moran_result <- moran.test(final_data$resid_pearson, spatial_weights)
# 打印结果
print(moran_result)
# 解释逻辑：
# 如果 p-value > 0.05: 恭喜！残差是随机分布的，模型非常完美，没有遗漏空间变量。
# 如果 p-value < 0.05 但 Moran's I 很小 (如 < 0.1): 模型还可以，只有轻微的空间依赖。
# Spatial Distribution of Residuals
# 1. 将残差添加回空间数据
# type = "deviance" residuals are often better for mapping goodness-of-fit
final_data$spatial_resid <- residuals(best_model, type = "deviance")
# 2. 绘制地图
ggplot() +
# 底图
geom_sf(data = philly_boundary, fill = "grey95", color = NA) +
# 站点残差图
geom_sf(data = final_data,
aes(color = spatial_resid),
size = 0.8, alpha = 0.7) +
# 颜色比例尺
scale_color_gradient2(
low = "blue", mid = "grey90", high = "red",
midpoint = 0,
name = "Deviance\nResidual",
# 限制范围，防止极个别离群值破坏颜色分布
limits = c(-3, 3),
oob = scales::squish
) +
labs(
title = "Map of Model Residuals",
subtitle = "Red = Unexpectedly High Crime (Under-predicted)\nBlue = Unexpectedly Low Crime (Over-predicted)"
) +
mapTheme
best_model <- model_6
# 1. 提取拟合值和残差
# 注意：对于 glm.nb，必须指定 type = "pearson" 来标准化残差
model_data <- data.frame(
Fitted = fitted(best_model),
Residuals = residuals(best_model, type = "pearson")
)
# 2. 绘制残差 vs. 拟合值图
p_resid_fitted <- ggplot(model_data, aes(x = log(Fitted), y = Residuals)) +
geom_point(alpha = 0.3, color = "#6A1B9A", size = 1.5) +
geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
geom_smooth(method = "loess", color = "black", se = FALSE, linewidth = 0.8) +
labs(
title = "Residuals vs Fitted Values",
subtitle = "Checking for systematic bias in the Count Model",
x = "Log(Fitted Values) - Predicted Crime Count",
y = "Pearson Residuals"
) +
plotTheme
p_resid_fitted
p_qq <- ggplot(model_data, aes(sample = Residuals)) +
stat_qq(color = "#6A1B9A", size = 1.5, alpha = 0.5) +
stat_qq_line(color = "red", linetype = "dashed", linewidth = 1) +
labs(
title = "Q-Q Plot of Pearson Residuals",
subtitle = "Checking for extreme outliers in count data",
x = "Theoretical Quantiles",
y = "Sample Quantiles"
) +
plotTheme
p_qq
## 6.4 Top 10 High-Risk Anomaly Table
library(kableExtra)
# 1. 准备数据
top_10_table <- final_data %>%
mutate(
Predicted = predict(best_model, type = "response"),
Residual = Crime_Total_Count - Predicted
) %>%
# 按残差排序：找出"实际犯罪"比"预测"高得最多的点
arrange(desc(Residual)) %>%
slice(1:10) %>%
# 选择并重命名列，使其适合展示
dplyr::select(
"Bus Stop Name" = Stop,
"Police District (PSA)" = PSA_ID,
"Actual Crime" = Crime_Total_Count,
"Model Predicted" = Predicted,
"Unexplained Excess" = Residual
)
# 2. 输出美化的表格
kbl(top_10_table, digits = 1, caption = "The 'Hit List': Top 10 Stops with Highest Unexplained Crime Risk") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
# 高亮最后一列，强调这是我们需要解决的问题
column_spec(5, bold = TRUE, color = "white", background = "#d73027") %>%
# 加一个脚注解释
footnote(general = " 'Unexplained Excess' = Actual Crime minus Predicted Crime based on local environment. Positive values indicate specific local security failures.")
hypothesis_test <- data.frame(
Scenario = c("Weekday (Commuters)", "Weekend (Non-Routine)"),
Impact_Slope = c(slope_weekday, slope_weekend),
Std_Error = c(se_weekday, se_weekend),
P_Value = c(p_weekday, p_weekend)
) %>%
mutate(
# 自动生成结论列
Interpretation = case_when(
Impact_Slope > 0 & P_Value < 0.05 ~ "Positive & Significant (Target Hypothesis Supported)",
Impact_Slope < 0 & P_Value < 0.05 ~ "Negative & Significant (Eyes on Street Supported)",
TRUE ~ "Not Significant"
),
# 格式化数字
Impact_Slope = round(Impact_Slope, 3),
Std_Error = round(Std_Error, 3),
P_Value = ifelse(P_Value < 0.001, "< 0.001", round(P_Value, 3))
)
library(tidyverse)
library(sf)
library(tidycensus)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")
library(MASS)
library(spdep)
library(dplyr)
library(scales)
library(ggplot2)
library(caret)
library(nngeo)
library(car)
library(knitr)
library(readr)
library(patchwork)
library(kableExtra)
library(lubridate)
options(tigris_use_cache = TRUE)
options(tigris_progress = FALSE)
plotTheme <- theme(
plot.title = element_text(size = 14, face = "bold"),
plot.subtitle = element_text(size = 10),
plot.caption = element_text(size = 8),
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 11, face = "bold"),
panel.background = element_blank(),
panel.grid.major = element_line(colour = "#D0D0D0", size = 0.2),
panel.grid.minor = element_blank(),
axis.ticks = element_blank(),
legend.position = "right"
)
mapTheme <- theme(
plot.title = element_text(size = 14, face = "bold"),
plot.subtitle = element_text(size = 10),
plot.caption = element_text(size = 8),
axis.line = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank(),
panel.background = element_blank(),
panel.border = element_blank(),
panel.grid.major = element_line(colour = 'transparent'),
panel.grid.minor = element_blank(),
legend.position = "right",
plot.margin = margin(1, 1, 1, 1, 'cm'),
legend.key.height = unit(1, "cm"),
legend.key.width = unit(0.2, "cm")
)
palette5 <- c("#eff3ff", "#bdd7e7", "#6baed6", "#3182bd", "#08519c")
# 1. Load Bus Data
bus_raw <- read_csv("data/Summer_2025_Stop_Summary_(Bus).csv")
library(dplyr)
library(ggplot2)
library(knitr)
library(kableExtra)
# --- 1. 提取系数和方差协方差矩阵 ---
# 我们需要手动计算周末的斜率标准误 (Standard Error)，公式是 Var(A+B) = Var(A) + Var(B) + 2Cov(A,B)
# 确保你的模型名字是 model_6
model_final <- model_6
# 获取系数名 (根据你的模型结果，可能需要微调名字)
coef_name_base <- "Log_Ridership"
coef_name_interact <- "Log_Ridership:is_weekend_factorWeekend"
# 提取数值
beta_base <- coef(model_final)[coef_name_base]        # 工作日斜率
beta_interact <- coef(model_final)[coef_name_interact]# 交互项系数
vcov_matrix <- vcov(model_final)                      # 协方差矩阵
# --- 2. 计算两个场景下的真实斜率 (Marginal Effects) ---
# A. 工作日 (Weekday): 也就是基准组 (Base)
slope_weekday <- beta_base
se_weekday <- sqrt(vcov_matrix[coef_name_base, coef_name_base])
p_weekday <- 2 * (1 - pnorm(abs(slope_weekday / se_weekday)))
# B. 周末 (Weekend): 基准 + 交互 (Base + Interaction)
slope_weekend <- beta_base + beta_interact
# 计算组合方差: Var(A+B) = Var(A) + Var(B) + 2*Cov(A,B)
var_weekend <- vcov_matrix[coef_name_base, coef_name_base] +
vcov_matrix[coef_name_interact, coef_name_interact] +
2 * vcov_matrix[coef_name_base, coef_name_interact]
se_weekend <- sqrt(var_weekend)
p_weekend <- 2 * (1 - pnorm(abs(slope_weekend / se_weekend)))
hypothesis_test <- data.frame(
Scenario = c("Weekday (Commuters)", "Weekend (Non-Routine)"),
Impact_Slope = c(slope_weekday, slope_weekend),
Std_Error = c(se_weekday, se_weekend),
P_Value = c(p_weekday, p_weekend)
) %>%
mutate(
# 自动生成结论列
Interpretation = case_when(
Impact_Slope > 0 & P_Value < 0.05 ~ "Positive & Significant (Target Hypothesis Supported)",
Impact_Slope < 0 & P_Value < 0.05 ~ "Negative & Significant (Eyes on Street Supported)",
TRUE ~ "Not Significant"
),
# 格式化数字
Impact_Slope = round(Impact_Slope, 3),
Std_Error = round(Std_Error, 3),
P_Value = ifelse(P_Value < 0.001, "< 0.001", round(P_Value, 3))
)
# 输出美化表格
kbl(hypothesis_test, caption = "The Verdict: Does Ridership Drive Crime in Both Contexts?") %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>%
row_spec(1:2, bold = TRUE, color = "black", background = "#e6f5ff") %>%
footnote(general = "Slopes > 0 indicate that higher ridership is associated with higher crime counts.")
## 7.2.1 Top 50 异常风险站点地图 (Residuals)
# 1. 计算残差 (Actual - Predicted)
# 我们使用 "Raw Residuals" (原始残差)，因为它的业务含义最直观：
# "实际发生的犯罪比模型预测的多出了多少起？"
final_data <- final_data %>%
mutate(
Predicted_Crime = predict(best_model, type = "response"),
# 计算差异：实际 - 预测
# 正值 (Positive) = 实际犯罪 > 预期 (危险异常)
# 负值 (Negative) = 实际犯罪 < 预期 (安全异常)
Resid_Raw = Crime_Total_Count - Predicted_Crime
)
# 2. 筛选前 50 个风险高（人多），但实际没什么事的站点
top_50_anomalies <- final_data %>%
# 按残差从大到小排序 (只看正值最大的)
arrange(desc(Resid_Raw)) %>%
slice(1:50)
# 3. 绘制地图
ggplot() +
# A. 背景底图 (费城)
geom_sf(data = philly_boundary, fill = "grey95", color = "white") +
# B. 所有站点 (作为背景参考，浅灰色)
geom_sf(data = final_data, color = "grey80", size = 0.5, alpha = 0.3) +
# C. 异常站点 (红色醒目)
geom_sf(data = top_50_anomalies,
aes(size = Resid_Raw), # 让残差越大的点越大
color = "steelblue",
alpha = 0.8) +
# E. 设置图例和标题
scale_size_continuous(name = "Excess Crimes\n(Actual - Predicted)") +
labs(
title = "Top 50 Under-policed Stops",
subtitle = "Locations where actual crime significantly exceeds model predictions.",
caption = "Blue dots represent stops performing worse than their environment, suggesting there are more police in need."
) +
mapTheme +
theme(legend.position = "right")
## 7.2.2 Top 50 异常风险站点地图 (Residuals)
# 1. 计算残差 (Actual - Predicted)
# 我们使用 "Raw Residuals" (原始残差)，因为它的业务含义最直观：
# "实际发生的犯罪比模型预测的多出了多少起？"
final_data <- final_data %>%
mutate(
Predicted_Crime = predict(best_model, type = "response"),
# 计算差异：实际 - 预测
# 正值 (Positive) = 实际犯罪 > 预期 (危险异常)
# 负值 (Negative) = 实际犯罪 < 预期 (安全异常)
Resid_Raw =  Predicted_Crime - Crime_Total_Count
)
# 2. 筛选前 50 个风险高（人多），但实际没什么事的站点
top_50_anomalies <- final_data %>%
# 按残差从大到小排序 (只看正值最大的)
arrange(desc(Resid_Raw)) %>%
slice(1:50)
# 3. 绘制地图
ggplot() +
# A. 背景底图 (费城)
geom_sf(data = philly_boundary, fill = "grey95", color = "white") +
# B. 所有站点 (作为背景参考，浅灰色)
geom_sf(data = final_data, color = "grey80", size = 0.5, alpha = 0.3) +
# C. 异常站点 (红色醒目)
geom_sf(data = top_50_anomalies,
aes(size = Resid_Raw), # 让残差越大的点越大
color = "#d73027",
alpha = 0.8) +
# E. 设置图例和标题
scale_size_continuous(name = "Excess Crimes\n(Predicted - Actual)") +
labs(
title = "Top 50 'Over-policed' Stops",
subtitle = "Locations where actual crime significantly under model predictions.",
caption = "Red dots represent stations which are statistically risky, but the actual crime counts are actually small. "
) +
mapTheme +
theme(legend.position = "right")
## 7.2.3 Top 10 High-Risk Anomaly Table
library(kableExtra)
# 1. 准备数据
top_10_table <- final_data %>%
mutate(
Predicted = predict(best_model, type = "response"),
Residual = Crime_Total_Count - Predicted
) %>%
# 按残差排序：找出"实际犯罪"比"预测"高得最多的点
arrange(desc(Residual)) %>%
slice(1:10) %>%
# 选择并重命名列，使其适合展示
dplyr::select(
"Bus Stop Name" = Stop,
"Police District (PSA)" = PSA_ID,
"Actual Crime" = Crime_Total_Count,
"Model Predicted" = Predicted,
"Unexplained Excess" = Residual
)
# 2. 输出美化的表格
kbl(top_10_table, digits = 1, caption = "The 'Hit List': Top 10 Stops with Highest Unexplained Crime Risk") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
# 高亮最后一列，强调这是我们需要解决的问题
column_spec(5, bold = TRUE, color = "white", background = "#d73027") %>%
# 加一个脚注解释
footnote(general = " 'Unexplained Excess' = Actual Crime minus Predicted Crime based on local environment. Positive values indicate specific local security failures.")
# --- 1. 准备绘图数据 (Extract Coordinates) ---
# A. 处理公交数据：只取工作日数据以避免重复，并提取坐标
bus_plot_data <- bus_long %>%
filter(is_weekend == 0) %>%   # 仅使用工作日数据代表典型客流
mutate(
X = st_coordinates(geometry)[,1],
Y = st_coordinates(geometry)[,2]
) %>%
st_drop_geometry() # 移除几何列，方便 stat_density_2d 使用
# B. 处理犯罪数据：提取坐标
crime_plot_data <- crime_sf %>%
mutate(
X = st_coordinates(geometry)[,1],
Y = st_coordinates(geometry)[,2]
) %>%
st_drop_geometry()
# --- 2. 绘制图形 (Create Maps) ---
# 左图：客流热力图 (Ridership Density)
# 注意：aes(weight = Ridership) 是关键，让热力图基于客流量而不是站点数量
p_ridership <- ggplot() +
# 底图：费城轮廓
geom_sf(data = philly_boundary, fill = "#f5f5f5", color = "grey80") +
# 热力层
stat_density_2d(
data = bus_plot_data,
aes(x = X, y = Y, fill = ..level.., weight = Ridership),
geom = "polygon",
alpha = 0.75
) +
# 配色：蓝色系 (代表正常活动)
scale_fill_distiller(palette = "Blues", direction = 1, guide = "none") +
labs(
title = "Weekday Ridership Hotspots",
subtitle = "High Transit Activity Zones"
) +
mapTheme
# 右图：犯罪热力图 (Crime Density)
p_crime_map <- ggplot() +
# 底图
geom_sf(data = philly_boundary, fill = "#f5f5f5", color = "grey80") +
# 热力层 (修改部分)
stat_density_2d(
data = crime_plot_data,
aes(x = X, y = Y, fill = ..level..),
geom = "polygon",
alpha = 0.4,       # 1. 调低透明度，减少"硬边"的感觉
bins = 30,         # 2. 增加层数，让过渡更平滑 (原来默认很少)
adjust = 0.5       # 3. 调低带宽 (0.5), 让热力点收缩，更聚焦局部热点
) +
# 配色
scale_fill_distiller(palette = "Reds", direction = 1, guide = "none") +
labs(
title = "Crime Hotspots",
subtitle = "High Incident Zones"
) +
mapTheme
# --- 3. 组合展示 (Combine Side-by-Side) ---
# 使用 patchwork 进行拼接
combined_map <- p_ridership + p_crime_map +
plot_annotation(
title = "Spatial Mismatch Analysis: Eyes on the Street vs. Targets?",
subtitle = "Left: Where people are (Ridership) | Right: Where crimes happen",
theme = theme(
plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
plot.subtitle = element_text(size = 12, color = "grey40", hjust = 0.5)
)
)
# 输出图形
combined_map
# Load the tidyverse library
library(tidyverse)
library(sf)
# Read the liquor licenses data
Liquor_licenses <- read_csv("D:/CPLN6890cityatnight/assignment_2/PHL_PLCB_geocoded.csv")
# Load the tidyverse library
library(tidyverse)
library(sf)
# Read the liquor licenses data
Liquor_licenses <- read_csv("PHL_PLCB_geocoded.csv")
# 1. 使用 drop_na() 清除在 "lon" 或 "lat" 列中包含 NA 值的行
Liquor_licenses_cleaned <- Liquor_licenses %>%
drop_na(lon, lat)
# 2. 对清理后的数据框进行转换
liquor_sf <- st_as_sf(
x = Liquor_licenses_cleaned,
coords = c("lon", "lat"),
crs = 4326
)
# 检查转换后的对象
print(liquor_sf)
# 3. **将 sf 对象保存为 Shapefile**
# 设定输出的文件路径和文件名。建议放在一个容易找到的目录。
output_file_path <- "liquor_licenses_points.shp"
# 使用 st_write() 函数进行保存
st_write(
obj = liquor_sf,
dsn = output_file_path,
# driver = "ESRI Shapefile" 是默认的，但显式指定更安全
driver = "ESRI Shapefile",
# delete_layer = TRUE 会在写入前删除已存在的同名文件，防止报错
delete_layer = TRUE
)
# 4. 打印一个确认信息
cat("Shapefile 已成功保存到:", output_file_path, "\n")
