---
title: "Spatial Predictive Analysis"
subtitle: "Building a spatial predictive model using a 311 service request type"
author: "Christine"
date: today
format:
  html:
    code-fold: true
    code-tools: true
    toc: true
    toc-depth: 3
    toc-location: left
    theme: cosmo
    embed-resources: true
editor: visual
execute:
  warning: false
  message: false
---

## About This Exercise

In this exercise, I will build a spatial predictive model for burglaries using count regression and spatial features.

# Setup

```{r setup, include=FALSE}
#| message: false
#| warning: false

# Load required packages
library(tidyverse)      # Data manipulation
library(sf)             # Spatial operations
library(here)           # Relative file paths
library(viridis)        # Color scales
library(terra)          # Raster operations (replaces 'raster')
library(spdep)          # Spatial dependence
library(FNN)            # Fast nearest neighbors
library(MASS)           # Negative binomial regression
library(patchwork)      # Plot composition (replaces grid/gridExtra)
library(knitr)          # Tables
library(kableExtra)     # Table formatting
library(classInt)       # Classification intervals
library(here)
library(tidycensus)

# Spatstat split into sub-packages
library(spatstat.geom)    # Spatial geometries
library(spatstat.explore) # Spatial exploration/KDE

# Set options
options(scipen = 999)  # No scientific notation
set.seed(5080)         # Reproducibility

# Create consistent theme for visualizations
theme_crime <- function(base_size = 11) {
  theme_minimal(base_size = base_size) +
    theme(
      plot.title = element_text(face = "bold", size = base_size + 1),
      plot.subtitle = element_text(color = "gray30", size = base_size - 1),
      legend.position = "right",
      panel.grid.minor = element_blank(),
      axis.text = element_blank(),
      axis.title = element_blank()
    )
}

# Set as default
theme_set(theme_crime())

cat("✓ All packages loaded successfully!\n")
cat("✓ Working directory:", getwd(), "\n")
```

# Part 1: **Data Loading & Exploration**

This initial phase establishes the foundational dataset for our spatial analysis of burglary patterns in Chicago. The process systematically assembles and prepares multiple data sources: administrative boundaries (police districts, beats, and city limits) provide the spatial framework; burglary incidents serve as the primary outcome variable; and street light outage reports function as a key environmental predictor. Each dataset undergoes rigorous processing including coordinate system standardization, temporal filtering to 2017, and geometric validation.

## 1.1: Load Chicago Spatial Data

```{r load-boundaries}
#| message: false

# Load police districts (used for spatial cross-validation)
policeDistricts <- 
  st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(District = dist_num)

# Load police beats (smaller administrative units)
policeBeats <- 
  st_read("https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(Beat = beat_num)

# Load Chicago boundary
chicagoBoundary <- 
  st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson") %>%
  st_transform('ESRI:102271')

cat("✓ Loaded spatial boundaries\n")
cat("  - Police districts:", nrow(policeDistricts), "\n")
cat("  - Police beats:", nrow(policeBeats), "\n")
```

## 1.2: Load Burglary Data

```{r load-burglaries}
#| message: false

# Load from provided data file (downloaded from Chicago open data portal)
burglaries <- st_read("D:/MUSA5080PPA/portfolio-setup-ChristineCui12/labs/lab_4/data/burglaries.shp") %>% 
  st_transform('ESRI:102271')

# Check the data
cat("\n✓ Loaded burglary data\n")
cat("  - Number of burglaries:", nrow(burglaries), "\n")
cat("  - CRS:", st_crs(burglaries)$input, "\n")
cat("  - Date range:", min(burglaries$Date, na.rm = TRUE), "to", 
    max(burglaries$Date, na.rm = TRUE), "\n")
```

## 1.3: Load 311 Street Lights Out Calls

```{r load-lights-out}
#| message: false

lights_out <- read_csv("D:/MUSA5080PPA/portfolio-setup-ChristineCui12/labs/lab_4/data/Alley_Lights_Out_2025.csv")%>%
  mutate(date_received = mdy(`Creation Date`)) %>%
  filter(!is.na(Latitude), !is.na(Longitude), year(date_received) == 2017) %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>%
  st_transform('ESRI:102271')

cat("✓ Loaded lights out calls\n")
cat("  - Number of calls:", nrow(lights_out), "\n")
```

The selection of "Alley Lights Out" service requests as our predictor variable is grounded in both theoretical foundations and practical analytical considerations. From an environmental criminology perspective, inadequate alley illumination directly impacts crime opportunities by reducing natural surveillance and creating concealment spaces, aligning with CPTED principles. This violation type differs significantly from the classroom example of abandoned vehicles, representing a distinct category of infrastructure issues with unique spatial characteristics. Furthermore, alley lighting problems typically exhibit temporal stability due to municipal maintenance cycles, providing reliable spatial patterns for predictive modeling.

For our analytical framework, this dataset offers several advantages. The persistent nature of lighting infrastructure ensures strong spatial autocorrelation, making it a robust predictor at the grid-cell level. The geographic specificity of alley locations enables precise spatial analysis, while the substantial data volume with quality coordinates supports reliable modeling. Importantly, findings from this analysis can directly inform evidence-based policies for infrastructure maintenance and targeted crime prevention, bridging academic research with practical urban governance applications.

## 1.4: Visualize the spatial distribution

```{r visualize-points}
#| fig-width: 15
#| fig-height: 10

# Simple point map
p1 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_sf(data = lights_out, color = "#d62828", size = 0.1, alpha = 0.4) +
  labs(
    title = "Lights out Locations",
    subtitle = paste0("Chicago 2017, n = ", nrow(lights_out))
  )

# Density surface using modern syntax
p2 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_density_2d_filled(
    data = data.frame(st_coordinates(lights_out)),
    aes(X, Y),
    alpha = 0.7,
    bins = 8
  ) +
  scale_fill_viridis_d(
    option = "plasma",
    direction = -1,
    guide = "none"  # Modern ggplot2 syntax (not guide = FALSE)
  ) +
  labs(
    title = "Density Surface",
    subtitle = "Kernel density estimation"
  )

# Combine plots using patchwork (modern approach)
p1 + p2 + 
  plot_annotation(
    title = "Spatial Distribution of Lights out in Chicago",
    tag_levels = 'A'
  )
```

From the maps above, it can be found that the spatial distribution of alley light outages in Chicago demonstrates pronounced unevenness across the urban landscape. The visualization reveals distinct concentration patterns, with particularly dense clusters forming continuous corridors in western and southern sectors, while central and northern lakeshore areas remain notably sparse.

The kernel density analysis further validates this spatial disparity, highlighting persistent high-intensity zones in historically underserved neighborhoods. This geographical patterning reflects underlying infrastructure maintenance disparities and suggests potential correlations between illumination deficiencies and neighborhood socioeconomic characteristics. The stark contrast between high-frequency reporting areas and minimally affected regions underscores systemic inequities in public service distribution, while the concentrated nature of these outages may significantly influence local safety conditions and nocturnal activity patterns.

# Part 2: **Fishnet Grid Creation and Distribution**

This section establishes the analytical foundation by transforming raw spatial data into structured, comparable units that enable systematic examination of crime patterns and environmental factors. The process begins with creating a uniform 500m x 500m grid system spanning Chicago's urban landscape, which serves as our standardized spatial framework. This grid-based approach is crucial because it transcends traditional administrative boundaries (unequal sizes), allowing us to analyze urban phenomena in consistent, equally-sized, and no boundary bias units that capture local variations while maintaining city-wide comparability.

## 2.1: Fishnet Creation

```{r create-fishnet}
# Create 500m x 500m grid
fishnet <- st_make_grid(
  chicagoBoundary,
  cellsize = 500,  # 500 meters per cell
  square = TRUE
) %>%
  st_sf() %>%
  mutate(uniqueID = row_number())

# Keep only cells that intersect Chicago
fishnet <- fishnet[chicagoBoundary, ]

# View basic info
cat("✓ Created fishnet grid\n")
cat("  - Number of cells:", nrow(fishnet), "\n")
cat("  - Cell size:", 500, "x", 500, "meters\n")
cat("  - Cell area:", round(st_area(fishnet[1,])), "square meters\n")
```

## 2.2: Aggregate Burglaries to Grid

```{r aggregate-burglaries}
# Spatial join: which cell contains each burglary?
burglaries_fishnet <- st_join(burglaries, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(countBurglaries = n())

# Join back to fishnet (cells with 0 burglaries will be NA)
fishnet <- fishnet %>%
  left_join(burglaries_fishnet, by = "uniqueID") %>%
  mutate(countBurglaries = replace_na(countBurglaries, 0))

# Summary statistics
cat("\nBurglary count distribution:\n")
summary(fishnet$countBurglaries)
cat("\nCells with zero burglaries:", 
    sum(fishnet$countBurglaries == 0), 
    "/", nrow(fishnet),
    "(", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), "%)\n")
```

```{r visualize-fishnet-burglaries}
#| fig-width: 8
#| fig-height: 6

# Visualize aggregated counts
ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
  scale_fill_viridis_c(
    name = "Burglaries",
    option = "plasma",
    trans = "sqrt",  # Square root for better visualization of skewed data
    breaks = c(0, 1, 5, 10, 20, 40)
  ) +
  labs(
    title = "Burglary Counts by Grid Cell",
    subtitle = "500m x 500m cells, Chicago 2017"
  ) +
  theme_crime()
```

The aggregation of burglary incidents to these grid cells reveals striking spatial disparities in crime distribution. Rather than being evenly dispersed across the city, burglaries concentrate in specific hotspots while large portions of the grid remain crime-free. This clustering pattern underscores the non-random nature of criminal activity and highlights the limitations of traditional Poisson regression for modeling such overdispersed count data, pointing toward the need for more flexible statistical approaches like negative binomial regression.

## 2.3: Count of Streets Lights out per Cell

```{r count-lights-out}
# Aggregate lights out calls to fishnet
lightsout_fishnet <- st_join(lights_out, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(lights_out = n())

# Join to fishnet
fishnet <- fishnet %>%
  left_join(lightsout_fishnet, by = "uniqueID") %>%
  mutate(lights_out = replace_na(lights_out, 0))

cat("Abandoned car distribution:\n")
summary(fishnet$lights_out)
```

```{r visualize-fishnet-lightsout}
#| fig-width: 8
#| fig-height: 6

# Visualize aggregated counts
ggplot() +
  geom_sf(data = fishnet, aes(fill = lights_out), color = NA) +
  geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
  scale_fill_viridis_c(
    name = "Lights Out",
    option = "plasma",
    trans = "sqrt",  # Square root for better visualization of skewed data
    breaks = c(0, 1, 5, 10, 20, 40)
  ) +
  labs(
    title = "Alley Lights Out by Grid Cell",
    subtitle = "500m x 500m cells, Chicago 2017"
  ) +
  theme_crime()
```

When we map alley light outage reports onto the same grid system, we observe distinct spatial concentrations that visually correspond with burglary hotspots (to some extent), particularly in western and southern sectors of the city. This spatial correlation between infrastructure deficiencies and crime patterns provides preliminary support for environmental criminology theories, suggesting that inadequate lighting may create environmental conditions conducive to criminal activity by reducing natural surveillance and increasing perceived opportunities for offenders. The consistent spatial framework enables us to systematically explore these relationships in subsequent modeling phases.

# Part 3: Create **Spatial Features**

This section develops advanced spatial predictors to characterize the environmental context of lighting infrastructure and its potential relationship to crime patterns. The analysis progresses from measuring local lighting conditions through nearest-neighbor distances to identifying statistically significant spatial clusters of illumination deficiencies. These clusters are then operationalized as distance-based features, capturing proximity to areas of concentrated infrastructure problems. Finally, the framework is enriched with administrative boundaries, vacant buildings and demographic characteristics, creating a comprehensive feature set for subsequent burglary prediction modeling.

## 3.1: **K-**Nearest Neighbor Features

This step calculates the average distance from each grid cell centroid to its three nearest alley light outage reports. We first extract the centroid coordinates of all fishnet grid cells and the geographic coordinates of all light outage incidents, then use k-nearest neighbor algorithm (k=3) to compute the mean distance for each cell.

```{r nn-feature}
#| message: false

# Calculate mean distance to 3 nearest abandoned cars
# (Do this OUTSIDE of mutate to avoid sf conflicts)

# Get coordinates
fishnet_coords <- st_coordinates(st_centroid(fishnet))
lightsout_coords <- st_coordinates(lights_out)

# Calculate k nearest neighbors and distances
nn_result <- get.knnx(lightsout_coords, fishnet_coords, k = 3)

# Add to fishnet
fishnet <- fishnet %>%
  mutate(
    lights_out.nn = rowMeans(nn_result$nn.dist)
  )

cat("✓ Calculated nearest neighbor distances\n")
summary(fishnet$lights_out.nn)
```

This spatial feature captures the local environmental context of lighting infrastructure conditions beyond simple count-based measures. This is crucial because concentrated lighting deficiencies may create extended dark corridors that facilitate criminal activity, whereas isolated outages might have less impact on overall neighborhood safety perception.

The distance statistics reveal substantial spatial variation in lighting infrastructure problems across Chicago. With distances ranging from 4.6 meters to over 2.3 kilometers, and a median distance of 172.7 meters, we observe that some areas experience very dense concentrations of lighting issues (low distance values) while others have relatively sparse problems (high distance values). The right-skewed distribution (mean of 295.5 meters exceeding the median) suggests that while many cells have relatively accessible lighting infrastructure, there are significant areas where residents must travel considerable distances to encounter multiple lighting problems, potentially indicating **systematic infrastructure neglect** in certain neighborhoods.

## 3.2: Local Moran's I analysis

（改）The code creates a k-nearest neighbor weights matrix on grid centroids (k = 5), runs Local Moran’s I on the abandoned_houses counts, and generates a categorical label (High-High, Low-Low, High-Low, Low-High, or Not Significant). This identifies locations where there are hotspots, coldspots, or outliers. This is useful in substantivbe interpretations to discover whether vacnact or abandoned households are in-step or spatially clustered with neighboring households. It actually supports our initial theory that house abandonment (foreclosure) is likely to occur in generally similar neighborhoods.

```{r local-morans-abandoned}
# Function to calculate Local Moran's I
calculate_local_morans <- function(data, variable, k = 5) {
  
  # Create spatial weights
  coords <- st_coordinates(st_centroid(data))
  neighbors <- knn2nb(knearneigh(coords, k = k))
  weights <- nb2listw(neighbors, style = "W", zero.policy = TRUE)
  
  # Calculate Local Moran's I
  local_moran <- localmoran(data[[variable]], weights)
  
  # Classify clusters
  mean_val <- mean(data[[variable]], na.rm = TRUE)
  
  data %>%
    mutate(
      local_i = local_moran[, 1],
      p_value = local_moran[, 5],
      is_significant = p_value < 0.05,
      
      moran_class = case_when(
        !is_significant ~ "Not Significant",
        local_i > 0 & .data[[variable]] > mean_val ~ "High-High",
        local_i > 0 & .data[[variable]] <= mean_val ~ "Low-Low",
        local_i < 0 & .data[[variable]] > mean_val ~ "High-Low",
        local_i < 0 & .data[[variable]] <= mean_val ~ "Low-High",
        TRUE ~ "Not Significant"
      )
    )
}

# Apply to abandoned cars
fishnet <- calculate_local_morans(fishnet, "lights_out", k = 5)
```

## 3.3: **Identify Hot Spots and Cold Spots**

```{r visualize-morans}
#| fig-width: 8
#| fig-height: 6

# Visualize hot spots
ggplot() +
  geom_sf(
    data = fishnet, 
    aes(fill = moran_class), 
    color = NA
  ) +
  scale_fill_manual(
    values = c(
      "High-High" = "#d7191c",
      "High-Low" = "#fdae61",
      "Low-High" = "#abd9e9",
      "Low-Low" = "#2c7bb6",
      "Not Significant" = "gray90"
    ),
    name = "Cluster Type"
  ) +
  labs(
    title = "Local Moran's I: Lights out Clusters",
    subtitle = "High-High = Hot spots of disorder"
  ) +
  theme_crime()
```

（改）The LISA map shows vacancy hot spots concentrated on the South and Southeast sides, with additional pockets on the Southwest and a few small clusters on the West Side. Most cells are “Not Significant,” which means their vacancy levels are either average or lack strong similarity with neighbors at the k = 5 neighborhood scale. The few “Low-High” cells appear along the fringes of hot areas and indicate local dips embedded within otherwise high-vacancy neighborhoods, consistent with edge effects at cluster boundaries. Overall, the pattern reinforces our earlier findings: vacancy is highly clustered rather than uniform, with multi-cell cores in the south that are likely to be influential in burglary risk modeling and useful for distance-to-hotspot features.

## 3.4: Distance to Hot Spots

（改）The code extracts centroids of cells labeled High-High, unions them into a single geometry, and computes the distance from each grid centroid to the nearest hotspot. The resulting dist_to_hotspot feature captures how far a location is from the nearest vacancy hot area and defaults to zero when no significant hotspots are found.

```{r distance-to-hotspots}
# Get centroids of "High-High" cells (hot spots)
hotspots <- fishnet %>%
  filter(moran_class == "High-High") %>%
  st_centroid()

# Calculate distance from each cell to nearest hot spot
if (nrow(hotspots) > 0) {
  fishnet <- fishnet %>%
    mutate(
      dist_to_hotspot = as.numeric(
        st_distance(st_centroid(fishnet), hotspots %>% st_union())
      )
    )
  
  cat("✓ Calculated distance to lights out hot spots\n")
  cat("  - Number of hot spot cells:", nrow(hotspots), "\n")
} else {
  fishnet <- fishnet %>%
    mutate(dist_to_hotspot = 0)
  cat("⚠ No significant hot spots found\n")
}
```

**Question 4.3:** Why might distance to a cluster of abandoned cars be more informative than distance to a single abandoned car? What does Local Moran's I tell us?

*Your answer here:*

## 3.5: Join **Additional Contextual Data**

（改）The code assigns each grid cell to a police district, then downloads 2017 ACS tract data for Cook County, computes demographic and socioeconomic percentages and median income, and joins these attributes to the grid where cells intersect tracts. These controls (race or ethnicity shares, poverty, renter share, vacancy, age structure, and income) provide broader structural context for explaining burglary patterns beyond vacancy alone.

```{r join-districts}
# Join district information to fishnet
fishnet <- st_join(
  fishnet,
  policeDistricts,
  join = st_within,
  left = TRUE
) %>%
  filter(!is.na(District))  # Remove cells outside districts

cat("✓ Joined police districts\n")
cat("  - Districts:", length(unique(fishnet$District)), "\n")
cat("  - Cells:", nrow(fishnet), "\n")
```

```{r}
acs_vars <- c(
  total_pop = "B01003_001",
  med_income = "B19013_001",
  # Male 65 years and over (sum of 6 categories)
  male_65_66 = "B01001_020",
  male_67_69 = "B01001_021",
  male_70_74 = "B01001_022",
  male_75_79 = "B01001_023",
  male_80_84 = "B01001_024",
  male_85_up = "B01001_025",
  # Female 65 years and over (sum of 6 categories)
  female_65_66 = "B01001_044",
  female_67_69 = "B01001_045",
  female_70_74 = "B01001_046",
  female_75_79 = "B01001_047",
  female_80_84 = "B01001_048",
  female_85_up = "B01001_049"
)

tracts_all <- get_acs(
  geography = "tract",
  state = "IL",
  county = "Cook",
  variables = acs_vars,
  output = "wide",
  year = 2017, 
  geometry = TRUE 
) 
  # Calculate the combined 65+ population and rename variables

tracts_chi <- tracts_all %>%
  st_transform(st_crs(chicagoBoundary)) %>%
  st_filter(chicagoBoundary, .predicate = st_intersects) %>%
  dplyr::mutate(
    pop_65_over = (male_65_66E + male_67_69E + male_70_74E + male_75_79E + male_80_84E + male_85_upE +
                  female_65_66E + female_67_69E + female_70_74E + female_75_79E + female_80_84E + female_85_upE),
    pct_65_over = 100 * pop_65_over / pmax(total_popE, 1),
    total_pop = total_popE,
    med_income = med_incomeE
  ) %>%
  # Select and rename final columns for clarity and consistency
  dplyr::select(GEOID,total_pop,med_income,pop_65_over,pct_65_over)
# Join to fishnet
fishnet <- fishnet %>%
  st_join(tracts_chi, join = st_intersects, left = TRUE)
```

```{r}
# Load vacant buildings data
vacant <-
  st_read("data/Vacant_and_Abandoned_Buildings.geojson", quiet = TRUE) %>%
  st_transform("ESRI:102271")
# Spatial join.
vacant_fishnet <- st_join(vacant, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(count_vacant = n())
# Join to fishnet
fishnet <- fishnet %>%
  left_join(vacant_fishnet, by = "uniqueID") %>%
  mutate(count_vacant = replace_na(count_vacant, 0))
```

# Part 4: Model Fitting

（改）This section prepares the modeling frame, fits a Poisson and a Negative Binomial count model for burglaries using vacancy features and ACS controls, checks for overdispersion, and selects the preferred specification based on AIC for downstream prediction and validation.

## 4.1: Poisson Regression

（改）The code builds fishnet_model by dropping geometry, selecting the outcome burglaries, adding vacancy predictors (abandoned_houses, nearest vacancy distance, distance to vacancy hotspots), and including demographic and socioeconomic controls. Percentage variables are rescaled to proportions and median income is log transformed to reduce skew. A Poisson GLM with log link is then estimated, and a summary is printed to inspect coefficients and standard errors.

```{r prepare-data}
# Create clean modeling dataset
fishnet_model <- fishnet %>%
  st_drop_geometry() %>%
  dplyr::select(
    uniqueID,
    District,
    countBurglaries,
    lights_out,
    lights_out.nn,
    dist_to_hotspot,
    med_income,
    pct_65_over,
    count_vacant
  ) %>%
  na.omit()  # Remove any remaining NAs

cat("✓ Prepared modeling data\n")
cat("  - Observations:", nrow(fishnet_model), "\n")
cat("  - Variables:", ncol(fishnet_model), "\n")
```

```{r fit-poisson}
# Fit Poisson regression
model_poisson <- glm(
  countBurglaries ~ lights_out + lights_out.nn + 
    dist_to_hotspot + med_income + pct_65_over + count_vacant,
  data = fishnet_model,
  family = "poisson"
)

# Summary
summary(model_poisson)
```

**Question:** Interpret the coefficients. Which variables are significant? What do the signs (positive/negative) tell you?

（改）Burglary counts increase with more vacant houses in a cell and with closer proximity to vacancy. Each additional vacant house is associated with about 4.5% higher expected burglaries (IRR ≈ exp(0.0439) = 1.045). Greater distance to the nearest vacant house lowers risk: a 100 m increase corresponds to ≈ 6.7% lower expected burglaries (IRR ≈ exp(−0.0006906×100) = 0.933). Being farther from vacancy hotspots also reduces risk slightly: 1 km farther gives ≈ 2.7% lower rates (IRR ≈ exp(−0.0000279×1000) = 0.973). Neighborhood structure matters: higher renter share is strongly positive (a 10 percentage-point increase → IRR ≈ exp(0.1×1.640) = 1.18, or +18%), higher log income is positive (IRR ≈ exp(0.394) = 1.48 per one-log increase), and percent Black is positive (10 percentage-points → \~2% higher, IRR ≈ 1.02). Percent Hispanic and ages 45–64 are not significant. Percent in poverty is negative and significant (10 percentage-points → \~10% lower, IRR ≈ exp(−0.1056) = 0.90), likely reflecting overlap with renter share and income. Percent 65+ is negative and significant (10 percentage-points → \~13% lower, IRR ≈ exp(−0.1473) = 0.86).

## 4.2: Check for Overdispersion

Poisson regression assumes mean = variance. Real count data often violates this (overdispersion).

```{r check-overdispersion}
# Calculate dispersion parameter
dispersion <- sum(residuals(model_poisson, type = "pearson")^2) / 
              model_poisson$df.residual

cat("Dispersion parameter:", round(dispersion, 2), "\n")
cat("Rule of thumb: >1.5 suggests overdispersion\n")

if (dispersion > 1.5) {
  cat("⚠ Overdispersion detected! Consider Negative Binomial model.\n")
} else {
  cat("✓ Dispersion looks okay for Poisson model.\n")
}
```

## 4.3: Negative Binomial Regression

(改)Overdispersion was substantial (dispersion ≈ 2.79), so moving to a Negative Binomial was appropriate

If overdispersed, we use **Negative Binomial regression** (more flexible).

```{r fit-negbin}
# Fit Negative Binomial model
model_nb <- glm.nb(
  countBurglaries ~ lights_out + lights_out.nn + 
    dist_to_hotspot + med_income + pct_65_over + count_vacant,
  data = fishnet_model
)

# Summary
summary(model_nb)

# Compare AIC (lower is better)
cat("\nModel Comparison:\n")
cat("Poisson AIC:", round(AIC(model_poisson), 1), "\n")
cat("Negative Binomial AIC:", round(AIC(model_nb), 1), "\n")
```

（改）it fits much better (AIC 19,659; residual deviance 4,837 on 4,193 df) and estimates an overdispersion parameter θ ≈ 2.21. Vacancy intensity and proximity both raise burglary risk; tenure structure (more renters) is a strong correlate; older population share is associated with lower risk. The Negative Binomial specification handles variance better than Poisson and is the recommended model for inference and prediction.

## 4.4: Model fit(AIC) Comparison

**Question 6.2:** Which model fits better (lower AIC)? What does this tell you about the data?

*Your answer here:*

# Part 6: Spatial Cross-Validation

（改）This section evaluates out-of-sample performance with Leave-One-Group-Out cross-validation where each police district is held out in turn. For every fold, the Negative Binomial model is trained on all other districts and then used to predict burglary counts in the held-out district. We then summarize prediction errors to assess how well the model generalizes across space.

## 6.1: Create a Kernel Density Baseline

```{r kde-baseline}
#| message: false

# Convert burglaries to ppp (point pattern) format for spatstat
burglaries_ppp <- as.ppp(
  st_coordinates(burglaries),
  W = as.owin(st_bbox(chicagoBoundary))
)

# Calculate KDE with 1km bandwidth
kde_burglaries <- density.ppp(
  burglaries_ppp,
  sigma = 1000,  # 1km bandwidth
  edge = TRUE    # Edge correction
)

# Convert to terra raster (modern approach, not raster::raster)
kde_raster <- rast(kde_burglaries)

# Extract KDE values to fishnet cells
fishnet <- fishnet %>%
  mutate(
    kde_value = terra::extract(
      kde_raster,
      vect(fishnet),
      fun = mean,
      na.rm = TRUE
    )[, 2]  # Extract just the values column
  )

cat("✓ Calculated KDE baseline\n")
```

```{r visualize-kde}
#| fig-width: 8
#| fig-height: 6

ggplot() +
  geom_sf(data = fishnet, aes(fill = kde_value), color = NA) +
  geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
  scale_fill_viridis_c(
    name = "KDE Value",
    option = "plasma"
  ) +
  labs(
    title = "Kernel Density Estimation Baseline",
    subtitle = "Simple spatial smoothing of burglary locations"
  ) +
  theme_crime()
```

Before building complex models, let's create a simple baseline using **Kernel Density Estimation (KDE)**.

**The KDE baseline asks:** "What if crime just happens where it happened before?" (simple spatial smoothing, no predictors)

**Question 3.1:** How does the KDE map compare to the count map? What does KDE capture well? What does it miss?

*Your answer here:* - Comparison to the count map: - KDE pros and cons:

::: callout-tip
## Why Start with KDE?

The KDE represents our **null hypothesis**: burglaries happen where they happened before, with no other information.

**Your complex model must outperform this simple baseline to justify its complexity.**

We'll compare back to this at the end!
:::

## 6.2: **Leave-One-Group-Out (LOGO) Cross-Validation**

```{r spatial-cv}
# Get unique districts
districts <- unique(fishnet_model$District)
cv_results <- tibble()

cat("Running LOGO Cross-Validation...\n")

for (i in seq_along(districts)) {
  
  test_district <- districts[i]
  
  # Split data
  train_data <- fishnet_model %>% filter(District != test_district)
  test_data <- fishnet_model %>% filter(District == test_district)
  
  # Fit model on training data
  model_cv <- glm.nb(
    countBurglaries ~ lights_out + lights_out.nn + 
      dist_to_hotspot + med_income + pct_65_over + count_vacant,
    data = train_data
  )
  
  # Predict on test data
  test_data <- test_data %>%
    mutate(
      prediction = predict(model_cv, test_data, type = "response")
    )
  
  # Calculate metrics
  mae <- mean(abs(test_data$countBurglaries - test_data$prediction))
  rmse <- sqrt(mean((test_data$countBurglaries - test_data$prediction)^2))
  
  # Store results
  cv_results <- bind_rows(
    cv_results,
    tibble(
      fold = i,
      test_district = test_district,
      n_test = nrow(test_data),
      mae = mae,
      rmse = rmse
    )
  )
  
  cat("  Fold", i, "/", length(districts), "- District", test_district, 
      "- MAE:", round(mae, 2), "\n")
}

# Overall results
cat("\n✓ Cross-Validation Complete\n")
cat("Mean MAE:", round(mean(cv_results$mae), 2), "\n")
cat("Mean RMSE:", round(mean(cv_results$rmse), 2), "\n")
```

## 6.3: **Error metrics calculation and report**

```{r cv-results-table}
# Show results
cv_results %>%
  arrange(desc(mae)) %>%
  kable(
    digits = 2,
    caption = "LOGO CV Results by District"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Question 7.1:** Why is spatial CV more appropriate than random CV for this problem? Which districts were hardest to predict?

*Your answer here:*

# Part 7: Model **Evaluation**

## 7.1: Generate Final Predictions

```{r final-predictions}
# Fit final model on all data
final_model <- glm.nb(
  countBurglaries ~ lights_out + lights_out.nn + 
    dist_to_hotspot + med_income + pct_65_over + count_vacant,
  data = fishnet_model
)

# Add predictions back to fishnet
fishnet <- fishnet %>%
  mutate(
    prediction_nb = predict(final_model, fishnet_model, type = "response")[match(uniqueID, fishnet_model$uniqueID)]
  )

# Also add KDE predictions (normalize to same scale as counts)
kde_sum <- sum(fishnet$kde_value, na.rm = TRUE)
count_sum <- sum(fishnet$countBurglaries, na.rm = TRUE)
fishnet <- fishnet %>%
  mutate(
    prediction_kde = (kde_value / kde_sum) * count_sum
  )
```

## 7.2: Compare Model vs. KDE Baseline

```{r compare-models}
#| fig-width: 15
#| fig-height: 6

# Create three maps
p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "plasma", limits = c(0, 15)) +
  labs(title = "Actual Burglaries") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "Model Predictions (Neg. Binomial)") +
  theme_crime()

p3 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "KDE Baseline Predictions") +
  theme_crime()

p1 + p2 + p3 +
  plot_annotation(
    title = "Actual vs. Predicted Burglaries",
    subtitle = "Does our complex model outperform simple KDE?"
  )
```

```{r model-comparison-metrics}
# Calculate performance metrics
comparison <- fishnet %>%
  st_drop_geometry() %>%
  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %>%
  summarize(
    model_mae = mean(abs(countBurglaries - prediction_nb)),
    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),
    kde_mae = mean(abs(countBurglaries - prediction_kde)),
    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))
  )

comparison %>%
  pivot_longer(everything(), names_to = "metric", values_to = "value") %>%
  separate(metric, into = c("approach", "metric"), sep = "_") %>%
  pivot_wider(names_from = metric, values_from = value) %>%
  kable(
    digits = 2,
    caption = "Model Performance Comparison"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Question 8.1:** Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?

*Your answer here:*

## 7.3: Where Does the Model Work Well?

```{r prediction-errors}
#| fig-width: 10
#| fig-height: 5

# Calculate errors
fishnet <- fishnet %>%
  mutate(
    error_nb = countBurglaries - prediction_nb,
    error_kde = countBurglaries - prediction_kde,
    abs_error_nb = abs(error_nb),
    abs_error_kde = abs(error_kde)
  )

# Map errors
p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +
  scale_fill_gradient2(
    name = "Error",
    low = "#2166ac", mid = "white", high = "#b2182b",
    midpoint = 0,
    limits = c(-10, 10)
  ) +
  labs(title = "Model Errors (Actual - Predicted)") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +
  scale_fill_viridis_c(name = "Abs. Error", option = "plasma") +
  labs(title = "Absolute Model Errors") +
  theme_crime()

p1 + p2
```

**Question 9.2:** Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?

*Your answer here:*

# Part 8: Summary Statistics and Tables

## 8.1: Model Summary Table

```{r model-summary-table}
# Create nice summary table
model_summary <- broom::tidy(final_model, exponentiate = TRUE) %>%
  mutate(
    across(where(is.numeric), ~round(., 3))
  )

model_summary %>%
  kable(
    caption = "Final Negative Binomial Model Coefficients (Exponentiated)",
    col.names = c("Variable", "Rate Ratio", "Std. Error", "Z", "P-Value")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  footnote(
    general = "Rate ratios > 1 indicate positive association with burglary counts."
  )
```

## 8.2: Key Findings Summary

Based on your analysis, complete this summary:

**Technical Performance:**

-   Cross-validation MAE: `r round(mean(cv_results$mae), 2)`
-   Model vs. KDE: \[Which performed better?\]
-   Most predictive variable: \[Which had largest effect?\]

**Spatial Patterns:**

-   Burglaries are \[evenly distributed / clustered\]
-   Hot spots are located in \[describe\]
-   Model errors show \[random / systematic\] patterns

**Model Limitations:**

-   Overdispersion: \[Yes/No\]
-   Spatial autocorrelation in residuals: \[Test this!\]
-   Cells with zero counts: \[What % of data?\]
