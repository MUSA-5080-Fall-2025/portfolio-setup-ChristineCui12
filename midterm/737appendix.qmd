---
title: "Philadelphia Housing Model - Technical Appendix"
author: "Jinyang Xu, Xinyuan Cui, Yuqing Yang"
format: 
  html:
    code-fold: show
    toc: true
    toc-location: left
    theme: cosmo
execute:
  warning: false
  message: false
---

## Phase 1: Data Preparation
### Complete data cleaning code

***Load necessary libraries***
```{r, eval=FALSE}
library(tidyverse)
library(sf)
library(tidycensus)
library(tigris)
options(tigris_use_cache = TRUE, tigris_class = "sf")
library(MASS)
library(dplyr)
library(scales)
library(ggplot2)
library(caret)
library(nngeo)
library(car)
library(knitr)
library(readr)
# Add this near the top of your .qmd after loading libraries
options(tigris_use_cache = TRUE)
options(tigris_progress = FALSE)  
```

**1.1 Load and clean Philadelphia sales data:**

- 1.1.1 Load data
```{r}
opa <- read_csv("opa_properties_public1.csv")
```

- 1.1.2 Filter to residential properties, 2023-2024 sales
```{r}
# data in 2022 will be used as predictor, so keep them as well.
opa_clean <- opa %>%
  mutate(sale_date = as.Date(sale_date)) %>%
  filter(sale_date >= "2022-01-01" & sale_date <= "2024-12-31",
  category_code == "1"  # 1 indicates single family
  )

# record the amount of data we will focus on
opa_clean2 <- opa %>%
  mutate(sale_date = as.Date(sale_date)) %>%
  filter(sale_date >= "2023-01-01" & sale_date <= "2024-12-31",
  category_code == "1"  # 1 indicates single family
  )

# Select relevant variables 
opa_var <- opa_clean %>%
  dplyr::select(
    sale_date, sale_price, market_value, building_code_description,
    total_livable_area, number_of_bedrooms, number_of_bathrooms,
    number_stories, garage_spaces, central_air, quality_grade,
    interior_condition, exterior_condition, year_built, 
    off_street_open, zip_code, census_tract, zoning, owner_1,
    category_code_description, shape, fireplaces
  )

```

- 1.1.3 Remove obvious errors & Handle missing values
```{r}
# ! check before remove NA value

cat("<small>
💡 Data Cleaning Notes:<br>
- Remove NA only if missing count is small.<br>
- If many are missing, retain them and transform NA into a meaningful category.<br>
- Always record how missing values were handled.
</small>")

#remove errors and drop rows with small NA counts in specific columns
opa_var <- opa_var %>% 
  distinct() %>%  #Remove duplicate lines
  filter(
    !is.na(total_livable_area) & total_livable_area > 0,
    !is.na(year_built) & year_built > 0 & year_built < 2025,
    !is.na(number_of_bathrooms),
    !is.na(fireplaces),
    !is.na(interior_condition),
    garage_spaces<30
  )   
```

- 1.1.4 Other cleaning decisions
```{r}
#numeric quality_grade
valid_grades <- c("A+", "A", "A-", 
                  "B+", "B", "B-", 
                  "C+", "C", "C-", 
                  "D+", "D", "D-", 
                  "E+", "E", "E-")

opa_var <- opa_var %>%
  filter(quality_grade %in% valid_grades) %>%
  mutate(
    quality_grade = factor(quality_grade, levels = valid_grades, ordered = TRUE),
    quality_grade_num = rev(seq_along(valid_grades))[as.numeric(quality_grade)]
  )

#central_air (keep and transform the large NA values)
opa_var <- opa_var %>%
  mutate(
    central_air_dummy = case_when(
      central_air %in% c(1, "Y", "y") ~ 1,
      central_air %in% c(0, "N", "n") ~ 0,
      TRUE ~ NA_real_
    ),
    central_air_missing = if_else(is.na(central_air), 1, 0),
    central_air_dummy = if_else(is.na(central_air_dummy), 0, central_air_dummy)
  )

#house age
opa_var <- opa_var %>%
  mutate(
    house_age = 2025 - year_built
  )

```

- 1.1.5 Explore structural data biases and identify non-market transactions
```{r}
# check the relation of Sale Price ~ Market Value
options(scipen = 999)
plot(opa_var$sale_price, opa_var$market_value,
     xlab = "Sale Price", ylab = "Market Price",
     main = " Market Value (Predicted)  vs  Sale Price (Actual)",
     pch = 19, col = rgb(0.2,0.4,0.6,0.4))
abline(0,1,col="red",lwd=2)
```
```{r}
# create a new column to record the identified non-market transactions
opa_var <- opa_var %>%
  mutate(
    non_market = 
      (((sale_price < 0.10*market_value) | sale_price < 2000) | (sale_price> 5*market_value))
  )
```

**Key Findings:**  
-


- 1.1.6 enhance the sales data

We have 8000+ non market transactions, that is 1/4 of the total sales data!
That's too big to let go, The model that we want to generate will become much more stable if we can make use of them.
```{r}
# filter the REAL MARKET data in the time period we need
opa_selected <- opa_var %>% 
  filter(
    non_market==0,
    sale_date >= "2023-01-01" & sale_date <= "2024-12-31"
  )   

#filter the NON MARKET data in the time period we need
opa_non_market <- opa_var %>%
  filter(
    non_market ==1,
    sale_date >= "2023-01-01" & sale_date <= "2024-12-31"
    )

opa_selected2 <- opa_var
opa_bonus <- opa_var 
```

```{r}
#try to find the relationship between market_value and sale_price
options(scipen = 999)
plot(opa_selected$sale_price, opa_selected$market_value,
     xlab = "Sale Price", ylab = "Market Price",
     main = " Market Value vs  Sale Price (cleaned)",
     pch = 19, col = rgb(0.2,0.4,0.6,0.4))
abline(0,1,col="red",lwd=2)

```
```{r}
# That's quite linear, let's try to build a simple OLS model
opa_mdata <- opa_bonus %>%
  filter(
    non_market == 0
    )

model_non <- lm(sale_price ~ market_value, data = opa_mdata)
summary(model_non)

pred <- predict(model_non, newdata = opa_mdata)
resid <- opa_mdata$sale_price - pred

# RMSE
rmse_value <- sqrt(mean(resid^2, na.rm = TRUE))
rmse_value

```
The results demonstrate a very strong linear relationship between sale_price and market_value.
The market_value variable in the dataset effectively predicts the sale_price.
Therefore, we can leverage this relationship to estimate the normal market prices for non-market transactions.
We record these estimated values as sale_price_predicted.
By doing this, we enhance our data!

```{r}
#bring data back
opa_non_market$sale_price_predicted <- predict(model_non, newdata = opa_non_market)

#join back to the main data
opa_selected <- opa_selected %>%
  mutate(sale_price_predicted= sale_price)

set.seed(123)
opa_bind <- bind_rows(opa_selected, opa_non_market) %>%
  slice_sample(prop = 1)
```


**1.2 Load and clean secondary dataset:**

- 1.2.1 Census data (tidycensus):
```{r}
# Transform to sf object 
opa_bind <- st_as_sf(opa_bind, wkt = "shape", crs = 2272) %>%
  st_transform(4326)
opa_sf<- opa_bind

```

```{r}
# Load Census data for Philadelphia tracts
philly_census <- get_acs(
  geography = "tract",
  variables = c(
    total_pop = "B01003_001",
    
    ba_degree = "B15003_022",
    total_edu = "B15003_001",
    
    median_income = "B19013_001",
   
    labor_force = "B23025_003",
    unemployed = "B23025_005",
    
    total_housing = "B25002_001",
    vacant_housing = "B25002_003"
  ),
  year = 2023,
  state = "PA",
  county = "Philadelphia",
  geometry = TRUE
) %>%
  dplyr::select(GEOID, variable, estimate, geometry) %>%   
  tidyr::pivot_wider(names_from = variable, values_from = estimate) %>%
  dplyr::mutate(
    ba_rate = 100 * ba_degree / total_edu,
    unemployment_rate = 100 * unemployed / labor_force,
    vacancy_rate = 100 * vacant_housing / total_housing
  ) %>%
  st_transform(st_crs(opa_sf))

# Spatial join of OPA data with Census data
opa_census <- st_join(opa_sf, philly_census, join = st_within) %>%
  filter(!is.na(median_income))
```

- 1.2.2 Spatial amenities (OpenDataPhilly)
```{r}
#load crime,poi,transit,hospital
opa_census <- st_transform(opa_census, 3857)

crime <- read_csv("crime_sel.csv") %>% 
  filter(!is.na(lat) & !is.na(lng)) 
crime_sf <- st_as_sf(crime, coords = c("lng", "lat"), crs = 4326) %>%
  st_transform(st_crs(opa_census)) 

poi_sf <- st_read("data/gis_osm_pois_a_free_1.shp", quiet = TRUE) %>%
  st_transform(st_crs(opa_census)) 

Transit <- read_csv("Transit.csv")
transit_sf <- st_as_sf(Transit, coords = c("Lon", "Lat"), crs = 4326) %>%
  st_transform(st_crs(opa_census))

hospital_sf <- st_read("hospitals.geojson", quiet = TRUE) %>%
  st_transform(st_crs(opa_census))

```

**1.3 Summary tables showing before/after dimensions**

```{r}
# Original data dimensions
opa_dims <- tibble(
  dataset = "raw CSV",
  rows = nrow(opa),
  columns = ncol(opa)
)

# cleaned data dimensions
opa_filter_dims <- tibble(
  dataset = "after fixed criteria",
  rows = nrow(opa_clean2),
  columns = ncol(opa_clean2)
)

opa_selected_dims <- tibble(
  dataset = "after cleaned",
  rows = nrow(opa_bind),
  columns = ncol(opa_bind)
)
# data dimensions (within census tracts)
opa_census_dims <- tibble(
  dataset = "after census joined",
  rows = nrow(opa_census),
  columns = ncol(opa_census)
)

# create summary table
summary_table <- bind_rows(opa_dims, opa_filter_dims,opa_selected_dims, opa_census_dims)
library(knitr)
summary_table %>%
  kable(caption = "Summary of OPA data before and after cleaning")

```


**Principles of Data Processing**

- opa Data  
  - Filter `sale_date` between `2023-01-01` and `2024-12-31`.  
  - Keep only residential properties (`category_code == 1`).  
  - Remove records with missing values in `total_livable_area`, `sale_price`, or `number_of_bathrooms`.  
  - Filter records `year_built > 0` .
  
- Census data  
  - Load data including `total_pop`, `ba_degree`, `total_edu`, `median_income`, `labor_force`, `unemployed`, `total_housing`, `vacant_housing`.
  - Transform to spatial format and remove records with missing values.
  
- Spatial amenities
  - Load datasets *Transit*, *crime*, *POIs*, *Hospitals*.
  - Transform to spatial format and remove records with missing values.


---

## Phase 2: Feature Engineering 

**2.1 Buffer-based features**:

- 2.1.1 neighborhood avg sale price in the past year
```{r}
#filter the past sales data

opa_census <- opa_census %>%
  mutate(sale_id = row_number())



opa_past <- opa_var %>% 
  filter(
    non_market==0,
    sale_date >= "2022-01-01" & sale_date <= "2022-12-31"
  ) 

opa_past <- opa_past %>%
  mutate(sale_id2 = row_number())

opa_past <- st_as_sf(opa_past, wkt = "shape", crs = 2272) %>%
  st_transform(3857)

opa_census <- st_transform(opa_census, 3857)
opa_past   <- st_transform(opa_past, 3857)

opa_census_buffer <- st_buffer(opa_census, 300)
drop_cols <- names(opa_census_buffer) %in% c("sale_price", "total_livable_area",
                                             "sale_price.y", "total_livable_area.y")
opa_census_buffer <- opa_census_buffer[ , !drop_cols, drop = FALSE]

join_result <- st_join(
  opa_census_buffer,
  opa_past,
  join = st_intersects,
  left = TRUE
)


join_result <- st_join(
  opa_census_buffer,
  opa_past,
  join = st_intersects,
  left = TRUE
)
join_dedup <- join_result %>%
  st_drop_geometry() %>%
  distinct(sale_id, sale_id2, .keep_all = TRUE)

opa_summary <- join_dedup %>%
  group_by(sale_id) %>%
  summarise(
    past_count = sum(!is.na(sale_id2)),
    avg_past_price_density = ifelse(
      sum(!is.na(total_livable_area)) == 0, NA_real_,
      sum(sale_price, na.rm = TRUE) / sum(total_livable_area, na.rm = TRUE)
    ),
    .groups = "drop"
  )
opa_census <- opa_census %>%
  left_join(opa_summary, by = "sale_id")
```

- 2.1.2 crime numbers 
```{r}
radius_cri <- 250 

opa_census$crime_count <- lengths(st_is_within_distance(opa_census, crime_sf, dist = radius_cri)) 
```

- 2.1.3 POI numbers 
```{r}
opa_census_m <- st_transform(opa_census, 3857)
poi_sf_m     <- st_transform(poi_sf, 3857)


radius_poi <- 400
opa_census_buffer <- st_buffer(opa_census_m, radius_poi)

join_result <- st_join(opa_census_buffer, poi_sf_m, join = st_intersects, left = TRUE)

poi_summary <- join_result %>%
  st_drop_geometry() %>%
  group_by(sale_id) %>%
  summarise(poi_count = sum(!is.na(osm_id)))  

opa_census <- opa_census_m %>%
  left_join(poi_summary, by = "sale_id")

```

- 2.1.4 Transit numbers
```{r}

opa_census_m <- st_transform(opa_census, 3857)
transit_sf_m <- st_transform(transit_sf, 3857)

radius_ts <- 400
opa_census_buffer <- st_buffer(opa_census_m, radius_ts)

join_result <- st_join(opa_census_buffer, transit_sf_m, join = st_intersects, left = TRUE)

transit_summary <- join_result %>%
  st_drop_geometry() %>%
  group_by(sale_id) %>%
  summarise(transit_count = sum(!is.na(FID)))  

opa_census <- opa_census_m %>%
  left_join(transit_summary, by = "sale_id")

```


**2.2 k-Nearest Neighbor features**:

- 2.2.1 Hospitals (KNN-3)
```{r}
nearest_hospital_index <- st_nn(
  opa_census,
  hospital_sf,
  k = 3,            
  returnDist = TRUE 
)

opa_census$nearest_hospital_knn3 <- sapply(nearest_hospital_index$dist, mean)
```

**2.3 Interaction terms**:
```{r}

```

**2.4 Weights**:
```{r}
# add different weights to actual and non market transactions

opa_census_all <- opa_census

non_market_share<- mean(opa_census_all$non_market == 1, na.rm = TRUE)
non_market_share
opa_census_all <- opa_census %>%
  mutate(weight_mix = ifelse(non_market == 1, non_market_share, 1))
```
**2.5 Transformation**:
```{r}
#standardize houseage and median income
mean_age_all <- mean(opa_census_all$house_age, na.rm = TRUE)
mean_income_log <- mean(log(opa_census_all$median_income), na.rm = TRUE)

# centralize
opa_census_all <- opa_census_all %>%
  mutate(
    house_age_c  = house_age - mean_age_all,
    house_age_c2 = house_age_c^2,
    income_log   = log(median_income),
    income_scaled = income_log - mean_income_log
  )

opa_census_2<- opa_census_all %>%
  filter(
    non_market==0
  )

```



```{r 2.2.1 quality grade num }

```

```{r 2.2.2 fireplaces}

```

```{r 2.2.3 central air dummy +central air missing}

```

```{r 2.2.4 1og(total_livable area)/number of bathrooms/house_age/interior _condition/garage_spaces}
# 感觉这些好像没有额外的处理，如果这里没代码就删掉这个chunk就好了，记得改编号
```


### 2.3 Census tract

```{r 2.3.1 ba rate}

```

```{r 2.3.2 unemployment rate}

```

```{r 2.3.3 1og(median income) }

```


### 2.4 Spatial amenities

```{r 2.4.1 sqrt(crime_count)}

```

```{r 2.4.2 1og(nearest hospital knn3)}

```

```{r 2.4.3 city dist_m}

```


### 2.5 Other factors

```{r 2.5.1 (interior_condition * 10g(median_income))}
# 依旧不需要代码的话可以删
```

```{r 2.5.2 factor(zip _code)}
# 依旧不需要代码的话可以删
```


---


## Phase 3: Exploratory Data Analysis

### 3.1 Distribution of sale prices (histogram)

```{r 3.1 sale prices histogram}

ggplot(opa_census_all, aes(x = sale_price_predicted)) +
  geom_histogram(
    bins = 50,                 # 调整分箱数量
    fill = "#6A1B9A",          # 柱子颜色
    color = "white",           # 边框颜色
    alpha = 0.8                # 透明度
  ) +
  scale_x_continuous(labels = scales::dollar_format()) +
  theme_minimal(base_size = 12) +
  labs(
    title = "Distribution of  Sale Prices",
    x = "Predicted Sale Price (USD)",
    y = "Count"
  )
```
```{r}
ggplot(opa_census_all, aes(x = log(sale_price_predicted))) +
  geom_histogram(
    bins = 50,                 # 调整分箱数量
    fill = "#6A1B9A",          # 柱子颜色
    color = "white",           # 边框颜色
    alpha = 0.8                # 透明度
  ) +
  scale_x_continuous() +
  theme_minimal(base_size = 12) +
  labs(
    title = "Distribution of  Log(Sale Prices)",
    x = "Log(Predicted Sale Price) ",
    y = "Count"
  )
```

**Key Findings:** 
- **Highly Right-Skewed:** The bulk of the distribution is concentrated on the left side (low price range), while the right tail is very long, extending towards higher prices. This means that the majority of houses have lower prices, and very expensive houses are very few in number.  
- **Concentration and Outliers:** The count of houses in each bin drops rapidly as the price increases. The number of houses above $2,500,000 is very small, indicating the presence of extreme high-price outliers.  
- **Preprocessing Requirement:** This distribution strongly suggests that before building a house price prediction model, the Sale Price variable will need a transformation, most commonly a log transformation, to make the distribution more closely resemble a normal distribution.

### 3.2 Spatial distribution of sale prices (map)

```{r 3.2 sale prices map}
opa_census_all <- opa_census_all %>%
  mutate(price_quartile = ntile(sale_price_predicted, 4))

ggplot() +
  geom_sf(data = philly_census, fill = "lightgrey", color = "white") +
  geom_sf(data = opa_census_all, aes(color = factor(price_quartile)), size = 0.5, alpha = 0.7) +
  scale_color_viridis_d(
    option = "plasma",
    direction = -1,
    labels = c("0%-25%", "25%-50%", "50%-75%", "75%-100%")
  ) +
  theme_minimal() +
  labs(
    title = "Housing Sales Price in Philadelphia (2023–2024)",
    color = "Sale Price Quartile"
  )  +
  guides(color = guide_legend(override.aes = list(size = 3)))
```
```

**Key Findings:**  
- **Spatial Concentration of Housing Prices:** Highest Prices are clearly concentrated in the Center City/Downtown area of Philadelphia and its immediate surroundings, which indicates that the most expensive property transactions occur in the high-value areas of and around the city center.  
- 


### 3.3 Price vs. structural features (scatter plots)


```{r 3.3.1 log(sale_price_predicted) vs total_livable_area}

```

```{r 3.3.2 log(sale_price_predicted) vs number of bathrooms}

```

```{r 3.3.3 log(sale_price_predicted) vs house_age}

```

```{r 3.3.4 log(sale_price_predicted) vs interior condition}

```

```{r 3.3.5 log(sale_price_predicted) vs quality grade num}

```

```{r 3.3.6 log(sale_price_predicted) vs fireplaces }

```

```{r 3.3.7 log(sale_price_predicted) vs garage spaces}

```

```{r 3.3.8 log(sale_price_predicted) vs central air}
#这个真能画吗 我不知道
```
quarto check
**Key Findings:**  


### 3.4 Price vs. spatial & Social features (scatter plots)

```{r 3.4.1 log(sale_price_predicted) vs crime_count}


```

```{r 3.4.2 log(sale_price_predicted) vs median income}

```

```{r 3.4.3  log(sale_price_predicted) vs ba rate}

```

```{r 3.4.4 log(sale_price_predicted) vs unemployment rate}

```

```{r 3.4.5 log(sale_price_predicted) vs nearest hospital knn3}

```

```{r 3.4.6 log(sale_price_predicted) vs city dist_m}

```

**Key Findings:**  


### Other visualization

```{r 2.5 Neighborhood mean sale price}

```

**Key Findings:**  


---


## Phase 4: Model Building
### Build models progressively

**4.1 Structural features only:**

```{r Model_1+Summary}

model_1 <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  +  #Structural
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing, 
              
              data = opa_census_all,
              weight=weight_mix
               )
summary(model_1)
```

**Coefficient Interpretation**:  
1. xxx  
2. xxx

**4.2 Census variables:**
```{r Model_2+Summary}
model_2 <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  +   #Structural
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                
                  income_scaled +              #Census
                  ba_rate +
                  unemployment_rate,
  
              data = opa_census_all,
              weight=weight_mix
               )
summary(model_2)
```

**Coefficient Interpretation**:  
1. xxx  
2. xxx

**4.3 Spatial features:**
```{r Model_3+Summary}
model_3 <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  +   #Structural
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                
                  income_scaled +              #Census 
                  ba_rate +
                  unemployment_rate +
                
                  transit_count+
                  avg_past_price_density+      #Spatial 
                  sqrt(crime_count) +
                  log(nearest_hospital_knn3),
              
              data = opa_census_all,
              weight=weight_mix
               )
summary(model_3)
```

**Coefficient Interpretation**:  
1. xxx  
2. xxx

**4.4 Interactions and fixed effects:**
```{r Model_4+Summary}
model_4 <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  +   #Structural
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                
                  income_scaled +              #Census 
                  ba_rate +
                  unemployment_rate +
                
                  transit_count+
                  avg_past_price_density+      #Spatial 
                  sqrt(crime_count) +
                  log(nearest_hospital_knn3)+
                
                  (interior_condition * income_scaled)+  #FE & Interaction
                  factor(zip_code),
              
              data = opa_census_all,
              weight=weight_mix
               )
summary(model_4)
```

```{r 4.4.3 VIF Validation}
vif(model_4)
```

```{r 4.4.4 Final Model}

```

**Coefficient Interpretation**:  
1. xxx  
2. xxx

## Phase 5: Model Validation
### 10-fold cross-validation

**5.1 Compare all 4 models:**

# Create predicted vs. actual plot
```{r}
opa_census_2_clean <- opa_census_2

models <- list(model_1, model_2, model_3, model_4)
model_names <- c("Model 1", "Model 2", "Model 3", "Model 4")


options(scipen = 999)

all_pred_usd <- c()
for (m in models) {
  pred_log_tmp <- predict(m, newdata = opa_census_2_clean)
  smearing_tmp <- mean(exp(residuals(m)), na.rm = TRUE)
  pred_usd_tmp <- exp(pred_log_tmp) * smearing_tmp
  all_pred_usd <- c(all_pred_usd, pred_usd_tmp)
}
actual_usd <- opa_census_2_clean$sale_price_predicted

actual_k <- actual_usd / 1000
pred_all_k <- all_pred_usd / 1000

x_min <- min(actual_k, pred_all_k, na.rm = TRUE)
x_max <- 8000               # manually fix max x at 8000K
y_min <- min(actual_k, pred_all_k, na.rm = TRUE)
y_max <- max(actual_k, pred_all_k, na.rm = TRUE)

x_ticks <- pretty(c(x_min, x_max), n = 6)
y_ticks <- pretty(c(y_min, y_max), n = 6)

# Loop
for (i in seq_along(models)) {
  model <- models[[i]]
  model_name <- model_names[i]
  
  #  Predict on the same validation dataset 
  pred_log <- predict(model, newdata = opa_census_2_clean)
  
  # Smearing correction (to restore to USD scale) 
  smearing_factor <- mean(exp(residuals(model)), na.rm = TRUE)
  pred_usd <- exp(pred_log) * smearing_factor
  pred_k <- pred_usd / 1000   # Convert to thousand dollars
  

  rmse <- sqrt(mean((pred_usd - opa_census_2_clean$sale_price_predicted)^2, na.rm = TRUE))
  rmse_norm <- rmse / mean(opa_census_2_clean$sale_price_predicted, na.rm = TRUE)
  

  cat("\n============================\n")
  cat(model_name, "\n")
  cat("RMSE (USD):", round(rmse, 2), "\n")
  cat("Normalized RMSE:", round(rmse_norm, 4), "\n")
  cat("============================\n")
  
  
  plot(
    actual_k, pred_k,
    xlab = "Actual Price ($K)",
    ylab = "Predicted Price ($K)",
    main = paste(model_name, "- Predicted vs Actual Sale Price"),
    pch = 19,
    col = rgb(0.2, 0.4, 0.6, 0.4),
    xlim = c(x_min, x_max),
    ylim = c(y_min, y_max),
    axes = FALSE
  )

  axis(1, at = x_ticks, labels = paste0(x_ticks, "K"))
  axis(2, at = y_ticks, labels = paste0(y_ticks, "K"), las = 1)
  box()
  
  # Add 45-degree line
  abline(0, 1, col = "red", lwd = 2)
  
  #Save as PNG with same scale
  png_filename <- paste0("pred_actual_", gsub(" ", "_", tolower(model_name)), ".png")
  png(png_filename, width = 900, height = 800)
  par(mar = c(5, 5, 4, 2))
  
  plot(
    actual_k, pred_k,
    xlab = "Actual Price ($K)",
    ylab = "Predicted Price ($K)",
    main = paste(model_name, "- Predicted vs Actual Sale Price"),
    pch = 19,
    col = rgb(0.2, 0.4, 0.6, 0.4),
    xlim = c(x_min, x_max),
    ylim = c(y_min, y_max),
    axes = FALSE
  )
  axis(1, at = x_ticks, labels = paste0(x_ticks),cex.axis = 0.8)
  axis(2, at = y_ticks, labels = paste0(y_ticks), las = 1,cex.axis = 0.8)
  box()
  abline(0, 1, col = "red", lwd = 2)
  dev.off()
}

```

# Report and Compare RMSE, MAE, R²
```{r 5.1.1 Model_1 Validation}
#Since data have different weights, we need to define a new 10-fold cross-validation model.

set.seed(123)   
k <- 10

# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
  slice_sample(prop = 1) %>%  # randomly reorder all rows
  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs

#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)

#Perform k-fold cross-validation
for (i in 1:k) {
  
  # Split training / validation sets
  train <- opa_census_all %>% filter(fold_id != i)
  test_raw <- opa_census_all %>% filter(fold_id == i)
  
  # ✅ Very important! Ensure the 1/10 test data only include real market transactions

  test <- test_raw %>% filter(non_market != 1)  
  
  if (nrow(test) < 10) {
    cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
    next
  }
  
  # Weighted linear regression
  model_i <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  + 
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing, 
    data = train,
    weights = train$weight_mix
  )
  
  # Predict in log scale
  test$pred_log <- predict(model_i, newdata = test)
  
  # Compute R² 
  actual_log <- log(test$sale_price_predicted)
  ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
  ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
  r2_log_vec[i] <- 1 - ss_res / ss_tot
  
  # RMSE (log)
  rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
  
  # Compute RMSE in USD (Duan smearing correction)
  smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
  test$pred_usd <- exp(test$pred_log) * smearing_factor
  rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
}
```

```{r, echo=FALSE}
# Summarize results
cat("\n====================================\n")
cat("MODEL_1\n")
cat("💵 Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average R²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("====================================\n")

# Optional: view per-fold results
cv_results <- data.frame(
  Fold = 1:k,
  RMSE = round(rmse_log_vec, 2),
  RMSE_USD = round(rmse_usd_vec, 2),
  R2 = round(r2_log_vec, 4)
)
print(cv_results)
```

```{r 5.1.2 Model_2 Validation}
#Since data have different weights, we need to define a new 10-fold cross-validation model.

set.seed(234)   
k <- 10

# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
  slice_sample(prop = 1) %>%  # randomly reorder all rows
  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs

#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)

#Perform k-fold cross-validation
for (i in 1:k) {
  
  # Split training / validation sets
  train <- opa_census_all %>% filter(fold_id != i)
  test_raw <- opa_census_all %>% filter(fold_id == i)
  
  # ✅ Very important! Ensure the 1/10 test data only include real market transactions

  test <- test_raw %>% filter(non_market != 1)  
  
  if (nrow(test) < 10) {
    cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
    next
  }
  
  # Weighted linear regression
  model_i <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  + 
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                  
                  income_scaled +             
                  ba_rate +
                  unemployment_rate,
    data = train,
    weights = train$weight_mix
  )
  
  # Predict in log scale
  test$pred_log <- predict(model_i, newdata = test)
  
  # Compute R² 
  actual_log <- log(test$sale_price_predicted)
  ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
  ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
  r2_log_vec[i] <- 1 - ss_res / ss_tot
  
  # RMSE (log)
  rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
  
  # Compute RMSE in USD (Duan smearing correction)
  smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
  test$pred_usd <- exp(test$pred_log) * smearing_factor
  rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
}

```

```{r, echo=FALSE}
# Summarize results
cat("\n====================================\n")
cat("MODEL_2\n")
cat("💵 Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average R²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("====================================\n")

# Optional: view per-fold results
cv_results <- data.frame(
  Fold = 1:k,
  RMSE = round(rmse_log_vec, 2),
  RMSE_USD = round(rmse_usd_vec, 2),
  R2 = round(r2_log_vec, 4)
)
print(cv_results)
```

```{r 5.1.3 Model_3 Validation}
#Since data have different weights, we need to define a new 10-fold cross-validation model.

set.seed(345)   
k <- 10

# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
  slice_sample(prop = 1) %>%  # randomly reorder all rows
  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs

#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)

#Perform k-fold cross-validation
for (i in 1:k) {
  
  # Split training / validation sets
  train <- opa_census_all %>% filter(fold_id != i)
  test_raw <- opa_census_all %>% filter(fold_id == i)
  
  # ✅ Very important! Ensure the 1/10 test data only include real market transactions

  test <- test_raw %>% filter(non_market != 1)  
  
  if (nrow(test) < 10) {
    cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
    next
  }
  
  # Weighted linear regression
  model_i <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  + 
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                  
                  income_scaled +             
                  ba_rate +
                  unemployment_rate+
                  
                  transit_count+
                  avg_past_price_density+ 
                  sqrt(crime_count) +
                  log(nearest_hospital_knn3),
    data = train,
    weights = train$weight_mix
  )
  
  # Predict in log scale
  test$pred_log <- predict(model_i, newdata = test)
  
  # Compute R² 
  actual_log <- log(test$sale_price_predicted)
  ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
  ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
  r2_log_vec[i] <- 1 - ss_res / ss_tot
  
  # RMSE (log)
  rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
  
  # Compute RMSE in USD (Duan smearing correction)
  smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
  test$pred_usd <- exp(test$pred_log) * smearing_factor
  rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
}

```

```{r, echo=FALSE}
# Summarize results
cat("\n====================================\n")
cat("MODEL_3\n")
cat("💵 Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average R²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("====================================\n")

# Optional: view per-fold results
cv_results <- data.frame(
  Fold = 1:k,
  RMSE = round(rmse_log_vec, 2),
  RMSE_USD = round(rmse_usd_vec, 2),
  R2 = round(r2_log_vec, 4)
)
print(cv_results)
```

```{r 5.1.4 Model_4 Validation}
#Since data have different weights, we need to define a new 10-fold cross-validation model.

set.seed(456)   
k <- 10

# Shuffle dataset rows
opa_census_all <- opa_census_all %>%
  slice_sample(prop = 1) %>%  # randomly reorder all rows
  mutate(fold_id = sample(rep(1:k, length.out = n())))  # randomly assign fold IDs

#Initialize result vectors
rmse_usd_vec <- numeric(k)
r2_log_vec <- numeric(k)
rmse_log_vec <- numeric(k)

#Perform k-fold cross-validation
for (i in 1:k) {
  
  # Split training / validation sets
  train <- opa_census_all %>% filter(fold_id != i)
  test_raw <- opa_census_all %>% filter(fold_id == i)
  
  # ✅ Very important! Ensure the 1/10 test data only include real market transactions

  test <- test_raw %>% filter(non_market != 1)  
  
  if (nrow(test) < 10) {
    cat("Skipping fold", i, ": too few validation samples after cleaning.\n")
    next
  }
  
  # Weighted linear regression
  model_i <- lm(log(sale_price_predicted) ~ 
                  log(total_livable_area)  + 
                  number_of_bathrooms + 
                  house_age_c +
                  house_age_c2 +
                  interior_condition +
                  quality_grade_num + 
                  fireplaces +
                  garage_spaces +
                  central_air_dummy + 
                  central_air_missing+
                  
                  income_scaled +             
                  ba_rate +
                  unemployment_rate+
                  
                  transit_count+
                  avg_past_price_density+ 
                  sqrt(crime_count) +
                  log(nearest_hospital_knn3)+
                  
                  (interior_condition * income_scaled)+
                  factor(zip_code),
    data = train,
    weights = train$weight_mix
  )
  
  # Predict in log scale
  test$pred_log <- predict(model_i, newdata = test)
  
  # Compute R² 
  actual_log <- log(test$sale_price_predicted)
  ss_res <- sum((actual_log - test$pred_log)^2, na.rm = TRUE)
  ss_tot <- sum((actual_log - mean(actual_log, na.rm = TRUE))^2, na.rm = TRUE)
  r2_log_vec[i] <- 1 - ss_res / ss_tot
  # RMSE (log)
  rmse_log_vec[i] <- sqrt(mean((actual_log - test$pred_log)^2, na.rm = TRUE))
  
  # Compute RMSE in USD (Duan smearing correction)
  smearing_factor <- mean(exp(residuals(model_i)), na.rm = TRUE)
  test$pred_usd <- exp(test$pred_log) * smearing_factor
  rmse_usd_vec[i] <- sqrt(mean((test$sale_price_predicted - test$pred_usd)^2, na.rm = TRUE))
}

```

```{r, echo=FALSE}
# Summarize results
cat("\n====================================\n")
cat("MODEL_4\n")
cat("💵 Weighted 10-Fold Cross Validation (Shuffled + Cleaned Validation)\n")
cat("Average RMSE (log):", round(mean(rmse_log_vec, na.rm = TRUE), 4), "\n")
cat("Average RMSE (USD):", round(mean(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("RMSE Std. Dev (USD):", round(sd(rmse_usd_vec, na.rm = TRUE), 2), "\n")
cat("Average R²:", round(mean(r2_log_vec, na.rm = TRUE), 4), "\n")
cat("====================================\n")

# Optional: view per-fold results
cv_results <- data.frame(
  Fold = 1:k,
  RMSE = round(rmse_log_vec, 2),
  RMSE_USD = round(rmse_usd_vec, 2),
  R2 = round(r2_log_vec, 4)
)
print(cv_results)
```

**Discussion of the most mattered features**:  
1. xxx  
2. xxx


## Phase 6: Model Diagnostics
### Check assumptions for best model:

**6.1 Residual plot:**
```{r}
model_data <- data.frame(
  Fitted = fitted(model_4),
  Residuals = resid(model_4)
)

p_resid_fitted <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
  geom_point(alpha = 0.5, color = "#6A1B9A", size = 2) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  geom_smooth(method = "loess", color = "black", se = FALSE, linewidth = 0.8) +
  labs(
    title = "Residuals vs Fitted Values",
    subtitle = "Checking linearity and homoscedasticity for Model 4",
    x = "Fitted Values (Log(Sale Price))",
    y = "Residuals"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 13, color = "gray40"),
    axis.title = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )
p_resid_fitted
```

**Interpretation**:  
1. xxx  
2. xxx

**6.2 Q-Q plot:**
```{r}
p_qq <- ggplot(model_data, aes(sample = Residuals)) +
  stat_qq(color = "#6A1B9A", size = 2, alpha = 0.6) +
  stat_qq_line(color = "red",linetype = "dashed", linewidth = 1) +
  labs(
    title = "Normal Q-Q Plot",
    subtitle = "Checking normality of residuals for Model 4",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 13, color = "gray40"),
    axis.title = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )
p_qq
```

**Interpretation**:  
1. xxx  
2. xxx

**6.3 Cook’s distance:**
```{r}
cooks_d <- cooks.distance(model_4)
model_data <- data.frame(
  Index = 1:length(cooks_d),
  CooksD = cooks_d
)
threshold <- 4 / nrow(model_4$model)

p_cook <- ggplot(model_data, aes(x = Index, y = CooksD)) +
  geom_segment(aes(xend = Index, yend = 0), color = "#6A1B9A", alpha = 0.7) +  # vertical lines
  geom_point(color = "#6A1B9A", size = 0.15) +
  geom_hline(yintercept = threshold, linetype = "dashed", color = "red", linewidth = 1) +
  labs(
    title = "Cook's Distance",
    subtitle = "Identifying influential observations for Model 4",
    x = "Observation Index",
    y = "Cook's Distance"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 13, color = "gray40"),
    axis.title = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )
p_cook
```

**Interpretation**:  
1. xxx  
2. xxx

## Phase 7: Conclusions & Recommendations
### Conclusion:
- Our final model’s accuracy is __________. __________features matter most for Philadelphia prices.

### Recommendations:
- **Equity concerns**
1. 哪些社区最难预测
2. 数据偏见
- **Limitations and nest steps**
1. xxx
2. xxx




